{
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install git+https://github.com/handley-lab/blackjax\n!pip install anesthetic tqdm\n\n# Note: Installation may take 2-3 minutes in Google Colab\n# Advanced dependencies (optax, flax) are installed later when needed",
   "metadata": {},
   "outputs": [],
   "id": "1d2c5222"
  },
  {
   "cell_type": "markdown",
   "id": "c392d6b9",
   "metadata": {},
   "source": "# BlackJAX Nested Sampling Workshop\n\nThis workshop demonstrates GPU-native nested sampling using BlackJAX. We'll progress through three examples: line fitting, 2D Gaussian inference, and performance comparisons with other samplers. The workshop showcases JAX's key strengths: automatic differentiation and JIT compilation for high-performance Bayesian inference.\n\nðŸ“– **Essential Reading**: For the authoritative reference on nested sampling theory and applications, see the [Nested Sampling Book](https://handley-lab.co.uk/nested-sampling-book) by Will Handley.\n\n> *\"Nested sampling is a Bayesian computational technique that solves the key problem of evidence evaluation\"* â€” from the [Nested Sampling Book](https://handley-lab.co.uk/nested-sampling-book)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "17c9a03a",
   "metadata": {},
   "source": [
    "## Installation for Google Colab\n",
    "```bash\n",
    " pip install git+https://github.com/handley-lab/blackjax@nested_sampling\n",
    " pip install anesthetic tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57477364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0acbf",
   "metadata": {},
   "source": [
    "Configure JAX immediately after import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422316fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import time\n",
    "import blackjax\n",
    "from anesthetic import NestedSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588054e",
   "metadata": {},
   "source": [
    "## Part 1: Line Fitting with Nested Sampling\n",
    "\n",
    " We start with the classic problem of fitting a linear model y = mx + c to noisy data.\n",
    " This introduces the basic nested sampling workflow in BlackJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d6a123",
   "metadata": {},
   "source": [
    "### 1.1 Nested Sampling Configuration\n",
    " \n",
    " Key parameters for workshop timing and educational value:\n",
    " - `num_live=100`: Fast convergence for workshop setting\n",
    " - `num_delete=50`: Parallelization parameter\n",
    " - `num_inner_steps`: Reliability parameter (rule of thumb: 5 * num_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493b3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)\n",
    "num_live = 100\n",
    "num_delete = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510d5d3",
   "metadata": {},
   "source": [
    "### 1.2 Generate Synthetic Line Data\n",
    "\n",
    " True model: y = 2x + 1 + noise, with Ïƒ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f8a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 15\n",
    "x = jnp.linspace(-2.0, 2.0, num_data)\n",
    "true = {'m': 2.0, 'c': 1.0, 'sigma': 0.5}\n",
    "\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "noise = true['sigma'] * jax.random.normal(key, (num_data,))\n",
    "y = true['m'] * x + true['c'] + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469ffff",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3520cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.errorbar(x, y, yerr=true['sigma'], fmt=\"o\", label=\"Observed data\", color='black')\n",
    "ax.plot(x, true['m'] * x + true['c'], '--', label=\"True model\", color='red', alpha=0.7)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Linear Model: Bayesian Parameter Estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5cb3d",
   "metadata": {},
   "source": [
    "### 1.3 Define Likelihood Function\n",
    "\n",
    " Gaussian likelihood with unknown slope, intercept, and noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097a9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_loglikelihood(params):\n",
    "    \"\"\"Log-likelihood for linear model with Gaussian noise.\"\"\"\n",
    "    m, c, sigma = params[\"m\"], params[\"c\"], params[\"sigma\"]\n",
    "    y_model = m * x + c\n",
    "    # Vectorized normal log-likelihood\n",
    "    return jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7843c55",
   "metadata": {},
   "source": [
    "### 1.4 Define Prior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6700e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_bounds = {\n",
    "    \"m\": (-5.0, 5.0),      # slope\n",
    "    \"c\": (-5.0, 5.0),      # intercept  \n",
    "    \"sigma\": (0.1, 2.0),   # noise level (positive)\n",
    "}\n",
    "\n",
    "num_dims = len(prior_bounds)\n",
    "num_inner_steps = num_dims * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c467af8",
   "metadata": {},
   "source": [
    "### 1.5 Initialize Nested Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e603d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles, logprior_fn = blackjax.ns.utils.uniform_prior(prior_key, num_live, prior_bounds)\n",
    "\n",
    "nested_sampler = blackjax.nss(\n",
    "    logprior_fn=logprior_fn,\n",
    "    loglikelihood_fn=line_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=num_inner_steps,\n",
    ")\n",
    "print(f\"Initialized nested sampler with {num_live} live points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f0662d",
   "metadata": {},
   "source": [
    "### 1.6 JIT Compile for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec9641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn = jax.jit(nested_sampler.init)\n",
    "step_fn = jax.jit(nested_sampler.step)\n",
    "print(\"Functions compiled - ready to run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5dc7ae",
   "metadata": {},
   "source": [
    "### 1.7 Run the Nested Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a567f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running nested sampling for line fitting...\")\n",
    "ns_start = time.time()\n",
    "live = init_fn(particles)\n",
    "dead = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
    "    while not live.logZ_live - live.logZ < -3:  # Convergence criterion\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live, dead_info = step_fn(subkey, live)\n",
    "        dead.append(dead_info)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "dead = blackjax.ns.utils.finalise(live, dead)\n",
    "ns_time = time.time() - ns_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d75287",
   "metadata": {},
   "source": [
    "### 1.8 Process Results with Anesthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78ca04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"m\", \"c\", \"sigma\"]\n",
    "labels = [r\"$m$\", r\"$c$\", r\"$\\sigma$\"]\n",
    "data = jnp.vstack([dead.particles[key] for key in columns]).T\n",
    "\n",
    "line_samples = NestedSamples(\n",
    "    data,\n",
    "    logL=dead.loglikelihood,\n",
    "    logL_birth=dead.loglikelihood_birth,\n",
    "    columns=columns,\n",
    "    labels=labels,\n",
    "    logzero=jnp.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d495a",
   "metadata": {},
   "source": [
    "### 1.9 Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72640b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nested sampling runtime: {ns_time:.2f} seconds\")\n",
    "print(f\"Log Evidence: {line_samples.logZ():.2f} Â± {line_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: m={true['m']}, c={true['c']}, Ïƒ={true['sigma']}\")\n",
    "print(f\"Posterior means: m={line_samples.m.mean():.2f}, c={line_samples.c.mean():.2f}, Ïƒ={line_samples.sigma.mean():.2f}\")\n",
    "\n",
    "# Create posterior corner plot with true values marked\n",
    "kinds = {'lower': 'kde_2d', 'diagonal': 'hist_1d', 'upper': 'scatter_2d'}\n",
    "axes = line_samples.plot_2d(columns, kinds=kinds, label='Posterior')\n",
    "axes.axlines(true, color='red', linestyle='--', alpha=0.8)\n",
    "plt.suptitle(\"Line Fitting: Posterior Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62987597",
   "metadata": {},
   "source": [
    "## Part 2: 2D Gaussian Parameter Inference\n",
    "\n",
    " Now we tackle a more complex problem: inferring the parameters of a 2D Gaussian distribution\n",
    " including the correlation coefficient. This demonstrates parameter transforms and constrained sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3cdd8",
   "metadata": {},
   "source": [
    "### 2.1 Define 2D Gaussian Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd4a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.update({\n",
    "    'mu1': 1.0, 'mu2': -0.5,\n",
    "    'sigma1': 1.2, 'sigma2': 0.8, \n",
    "    'rho': 0.6\n",
    "})\n",
    "print(\"True parameters:\", {k: v for k, v in true.items() if k in ['mu1', 'mu2', 'sigma1', 'sigma2', 'rho']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab571746",
   "metadata": {},
   "source": [
    "### 2.2 Generate Correlated 2D Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647f2f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu = jnp.array([true['mu1'], true['mu2']])\n",
    "true_cov = jnp.array([\n",
    "    [true['sigma1']**2, true['rho'] * true['sigma1'] * true['sigma2']],\n",
    "    [true['rho'] * true['sigma1'] * true['sigma2'], true['sigma2']**2]\n",
    "])\n",
    "\n",
    "num_samples = 200\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "gaussian_data = jax.random.multivariate_normal(key, true_mu, true_cov, (num_samples,))\n",
    "\n",
    "print(f\"Generated {num_samples} correlated 2D samples\")\n",
    "print(f\"Sample mean: [{gaussian_data.mean(0)[0]:.2f}, {gaussian_data.mean(0)[1]:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2163d",
   "metadata": {},
   "source": [
    "### 2.3 Visualize the 2D Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c14de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(gaussian_data[:, 0], gaussian_data[:, 1], alpha=0.6, s=20)\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_title(\"2D Gaussian Data\")\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a8492",
   "metadata": {},
   "source": [
    "### 2.4 Define Likelihood with Parameter Transforms\n",
    "\n",
    " We use arctanh/tanh transform for the correlation coefficient to enforce |Ï| < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef3a695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_2d_loglikelihood(params):\n",
    "    \"\"\"Log-likelihood for 2D Gaussian with correlation.\"\"\"\n",
    "    mu1, mu2 = params[\"mu1\"], params[\"mu2\"]\n",
    "    sigma1, sigma2 = params[\"sigma1\"], params[\"sigma2\"]\n",
    "    rho_transformed = params[\"rho_t\"]\n",
    "    \n",
    "    # Transform correlation coefficient: rho = tanh(rho_t)\n",
    "    rho = jnp.tanh(rho_transformed)\n",
    "    \n",
    "    # Construct covariance matrix\n",
    "    cov = jnp.array([\n",
    "        [sigma1**2, rho * sigma1 * sigma2],\n",
    "        [rho * sigma1 * sigma2, sigma2**2]\n",
    "    ])\n",
    "    \n",
    "    # Check positive definiteness\n",
    "    det = jnp.linalg.det(cov)\n",
    "    \n",
    "    # Return -inf for invalid covariance matrices\n",
    "    def valid_loglik():\n",
    "        mu = jnp.array([mu1, mu2])\n",
    "        return jnp.sum(jax.scipy.stats.multivariate_normal.logpdf(gaussian_data, mu, cov))\n",
    "    \n",
    "    def invalid_loglik():\n",
    "        return -jnp.inf\n",
    "    \n",
    "    return jax.lax.cond(det > 1e-8, valid_loglik, invalid_loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0877ee",
   "metadata": {},
   "source": [
    "### 2.5 Set Up Priors for 2D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "267a2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_prior_bounds = {\n",
    "    \"mu1\": (-3.0, 5.0),\n",
    "    \"mu2\": (-3.0, 3.0), \n",
    "    \"sigma1\": (0.1, 3.0),\n",
    "    \"sigma2\": (0.1, 3.0),\n",
    "    \"rho_t\": (-2.0, 2.0),  # transformed correlation: rho = tanh(rho_t)\n",
    "}\n",
    "\n",
    "num_dims_2d = len(gaussian_prior_bounds)\n",
    "num_inner_steps_2d = num_dims_2d * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e77f1",
   "metadata": {},
   "source": [
    "### 2.6 Initialize and Run Nested Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cafbf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles_2d, logprior_fn_2d = blackjax.ns.utils.uniform_prior(prior_key, num_live, gaussian_prior_bounds)\n",
    "\n",
    "nested_sampler_2d = blackjax.nss(\n",
    "    logprior_fn=logprior_fn_2d,\n",
    "    loglikelihood_fn=gaussian_2d_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=num_inner_steps_2d,\n",
    ")\n",
    "\n",
    "init_fn_2d = jax.jit(nested_sampler_2d.init)\n",
    "step_fn_2d = jax.jit(nested_sampler_2d.step)\n",
    "\n",
    "print(\"Running nested sampling for 2D Gaussian...\")\n",
    "live_2d = init_fn_2d(particles_2d)\n",
    "dead_2d = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
    "    while not live_2d.logZ_live - live_2d.logZ < -3:\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live_2d, dead_info_2d = step_fn_2d(subkey, live_2d)\n",
    "        dead_2d.append(dead_info_2d)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "dead_2d = blackjax.ns.utils.finalise(live_2d, dead_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0795cbb",
   "metadata": {},
   "source": [
    "### 2.7 Transform Back and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36e30a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2d = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho_t\"]\n",
    "labels_2d = [r\"$\\mu_1$\", r\"$\\mu_2$\", r\"$\\sigma_1$\", r\"$\\sigma_2$\", r\"$\\rho_t$\"]\n",
    "data_2d = jnp.vstack([dead_2d.particles[key] for key in columns_2d]).T\n",
    "\n",
    "gaussian_samples = NestedSamples(\n",
    "    data_2d,\n",
    "    logL=dead_2d.loglikelihood,\n",
    "    logL_birth=dead_2d.loglikelihood_birth,\n",
    "    columns=columns_2d,\n",
    "    labels=labels_2d,\n",
    "    logzero=jnp.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fa7c6",
   "metadata": {},
   "source": [
    "Add transformed correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "744e6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_samples[\"rho\"] = jnp.tanh(gaussian_samples[\"rho_t\"].values)\n",
    "\n",
    "print(f\"Log Evidence: {gaussian_samples.logZ():.2f} Â± {gaussian_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: Î¼â‚={true['mu1']:.2f}, Î¼â‚‚={true['mu2']:.2f}, Ïƒâ‚={true['sigma1']:.2f}, Ïƒâ‚‚={true['sigma2']:.2f}, Ï={true['rho']:.2f}\")\n",
    "print(f\"Posterior means: Î¼â‚={gaussian_samples.mu1.mean():.2f}, Î¼â‚‚={gaussian_samples.mu2.mean():.2f}, Ïƒâ‚={gaussian_samples.sigma1.mean():.2f}, Ïƒâ‚‚={gaussian_samples.sigma2.mean():.2f}, Ï={gaussian_samples.rho.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59302c0",
   "metadata": {},
   "source": [
    "Plot posterior for key parameters with true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a70e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_params = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho\"]\n",
    "axes = gaussian_samples[key_params].plot_2d(key_params, kinds={'diagonal': 'hist_1d', 'lower': 'kde_2d'})\n",
    "\n",
    "# Mark true values using anesthetic's axlines method\n",
    "true_2d = {k: true[k] for k in key_params}\n",
    "axes.axlines(true_2d, color='red', linestyle='--', alpha=0.8)\n",
    "plt.suptitle(\"2D Gaussian: Posterior Parameter Estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d8557",
   "metadata": {},
   "source": [
    "## Part 3: Performance Comparison\n",
    "\n",
    " Compare BlackJAX nested sampling with NUTS (No-U-Turn Sampler) and \n",
    " Affine Invariant Ensemble Sampler on the line fitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ea27e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96dab0",
   "metadata": {},
   "source": [
    "### 3.1 Define NUTS Log-Probability Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d9a4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuts_logprob(params_array):\n",
    "    \"\"\"Combined log probability for NUTS (assumes flat priors within bounds).\"\"\"\n",
    "    m, c, log_sigma = params_array\n",
    "    sigma = jnp.exp(log_sigma)  # positive constraint via log transform\n",
    "    \n",
    "    # Check bounds (flat prior)\n",
    "    m_valid = (m >= -5.0) & (m <= 5.0)\n",
    "    c_valid = (c >= -5.0) & (c <= 5.0)\n",
    "    sigma_valid = (sigma >= 0.1) & (sigma <= 2.0)\n",
    "    \n",
    "    def valid_logprob():\n",
    "        y_model = m * x + c\n",
    "        loglik = jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)\n",
    "        return loglik + log_sigma  # Add Jacobian for log transform\n",
    "    \n",
    "    def invalid_logprob():\n",
    "        return -jnp.inf\n",
    "    \n",
    "    return jax.lax.cond(m_valid & c_valid & sigma_valid, valid_logprob, invalid_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce029a7c",
   "metadata": {},
   "source": [
    "### 3.2 Initialize and Run NUTS Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f4754aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_position = jnp.array([1.0, 0.0, jnp.log(1.0)])  # [m, c, log_sigma]\n",
    "nuts = blackjax.nuts(nuts_logprob, step_size=0.1, inverse_mass_matrix=jnp.eye(3))\n",
    "\n",
    "rng_key, nuts_key = jax.random.split(rng_key)\n",
    "nuts_state = nuts.init(initial_position)\n",
    "nuts_step = jax.jit(nuts.step)\n",
    "\n",
    "print(\"Running NUTS sampler...\")\n",
    "\n",
    "num_nuts_samples = 2000\n",
    "nuts_start = time.time()\n",
    "nuts_samples = []\n",
    "nuts_states = nuts_state\n",
    "\n",
    "for i in tqdm.tqdm(range(num_nuts_samples), desc=\"NUTS\"):\n",
    "    nuts_key, step_key = jax.random.split(nuts_key)\n",
    "    nuts_states, nuts_info = nuts_step(step_key, nuts_states)\n",
    "    nuts_samples.append(nuts_states.position)\n",
    "\n",
    "nuts_time = time.time() - nuts_start\n",
    "nuts_samples = jnp.stack(nuts_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a80b7",
   "metadata": {},
   "source": [
    "### 3.3 Process NUTS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15c6dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_m = nuts_samples[:, 0]\n",
    "nuts_c = nuts_samples[:, 1] \n",
    "nuts_sigma = jnp.exp(nuts_samples[:, 2])\n",
    "\n",
    "print(f\"NUTS runtime: {nuts_time:.2f} seconds\")\n",
    "print(f\"NUTS means: m={nuts_m[500:].mean():.2f}, c={nuts_c[500:].mean():.2f}, Ïƒ={nuts_sigma[500:].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5551c",
   "metadata": {},
   "source": [
    "### 3.4 Performance Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e43f287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"Nested Sampling\", \"NUTS\"]\n",
    "times = [f\"{ns_time:.1f} sec\", f\"{nuts_time:.1f} sec\"]\n",
    "evidence = [\"âœ“ (Log Z available)\", \"âœ— (Not computed)\"]\n",
    "parallelization = [\"âœ“ (GPU native)\", \"Limited\"]\n",
    "\n",
    "print(f\"{'Method':<20} {'Time':<15} {'Evidence':<15} {'GPU Parallel'}\")\n",
    "print(\"-\" * 65)\n",
    "for i in range(len(methods)):\n",
    "    print(f\"{methods[i]:<20} {times[i]:<15} {evidence[i]:<15} {parallelization[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba2ad9",
   "metadata": {},
   "source": [
    "### 3.5 Posterior Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1610b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6e8a2",
   "metadata": {},
   "source": [
    "Generate proper posterior samples from NestedSamples (not raw dead points)\n",
    " Use the number of available samples or 1000, whichever is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44f301a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_posterior_samples = min(1000, len(line_samples))\n",
    "ns_posterior_samples = line_samples.sample(n_posterior_samples, replace=True)  # Sample from posterior with replacement\n",
    "nuts_burnin = 500  # Remove burn-in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790ca22",
   "metadata": {},
   "source": [
    "Compare marginal posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d96dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[0].hist(ns_posterior_samples.m.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\n",
    "axes[0].hist(nuts_m[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\n",
    "axes[0].axvline(true['m'], color='red', linestyle='--', label='True value')\n",
    "axes[0].set_xlabel('Slope (m)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(ns_posterior_samples.c.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\n",
    "axes[1].hist(nuts_c[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\n",
    "axes[1].axvline(true['c'], color='red', linestyle='--', label='True value')\n",
    "axes[1].set_xlabel('Intercept (c)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(ns_posterior_samples.sigma.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\n",
    "axes[2].hist(nuts_sigma[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\n",
    "axes[2].axvline(true['sigma'], color='red', linestyle='--', label='True value')\n",
    "axes[2].set_xlabel('Noise (Ïƒ)')\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Posterior Comparison: Nested Sampling vs NUTS\", y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b8621",
   "metadata": {},
   "source": [
    "## Part 4: Building Your Own Nested Sampler\n",
    "\n",
    " Advanced users can build custom nested samplers using BlackJAX's low-level components.\n",
    " This demonstrates the modular design and shows how nested sampling works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53b1aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from blackjax.ns.adaptive import build_kernel, init\n",
    "from blackjax.ns.base import new_state_and_info, delete_fn\n",
    "from blackjax.ns.utils import repeat_kernel, finalise\n",
    "from blackjax.mcmc.random_walk import build_rmh, RWState\n",
    "from blackjax import SamplingAlgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9dca34",
   "metadata": {},
   "source": [
    "### 4.1 Define Custom Nested MCMC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "863f278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_nsmcmc(\n",
    "    logprior_fn,\n",
    "    loglikelihood_fn,\n",
    "    num_delete=10,\n",
    "    num_inner_steps=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a custom nested sampling MCMC algorithm from low-level components.\n",
    "    \n",
    "    This demonstrates how to construct a nested sampler using BlackJAX's\n",
    "    modular infrastructure - useful for research and customization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logprior_fn : callable\n",
    "        Function that computes the log prior probability of the parameters.\n",
    "    loglikelihood_fn : callable\n",
    "        Function that computes the log likelihood of the parameters.\n",
    "    num_delete : int\n",
    "        Number of particles to delete at each step.\n",
    "    num_inner_steps : int\n",
    "        Number of inner MCMC steps to perform.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    SamplingAlgorithm\n",
    "        Custom nested sampling algorithm with init and step functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the MCMC kernel for exploring within likelihood constraints\n",
    "    mcmc_kernel = build_rmh()\n",
    "\n",
    "    @repeat_kernel(num_inner_steps)\n",
    "    def inner_kernel(rng_key, state, logprior_fn, loglikelihood_fn, loglikelihood_0, params):\n",
    "        \"\"\"Inner MCMC kernel that explores within likelihood constraint.\"\"\"\n",
    "        def proposal_distribution(rng_key, position):\n",
    "            # Handle dictionary position structure\n",
    "            if isinstance(position, dict):\n",
    "                step = {}\n",
    "                for key in position.keys():\n",
    "                    sigma_val = params['sigma'][key] if isinstance(params['sigma'], dict) else params['sigma']\n",
    "                    step[key] = sigma_val * jax.random.normal(rng_key, shape=position[key].shape)\n",
    "                    rng_key, _ = jax.random.split(rng_key)  # Split key for each parameter\n",
    "                return {key: position[key] + step[key] for key in position.keys()}\n",
    "            else:\n",
    "                # Fallback for array position\n",
    "                step = params['sigma'] * jax.random.normal(rng_key, shape=position.shape) \n",
    "                return position + step\n",
    "\n",
    "        # Convert to MCMC state format\n",
    "        mcmc_state = RWState(position=state.position, logdensity=state.logprior)\n",
    "        new_mcmc_state, mcmc_info = mcmc_kernel(rng_key, mcmc_state, logprior_fn, proposal_distribution)\n",
    "\n",
    "        # Evaluate likelihood at new position\n",
    "        loglikelihood = loglikelihood_fn(new_mcmc_state.position)\n",
    "\n",
    "        # Create new nested sampling state\n",
    "        new_state, info = new_state_and_info(\n",
    "            position=new_mcmc_state.position,\n",
    "            logprior=new_mcmc_state.logdensity,\n",
    "            loglikelihood=loglikelihood,\n",
    "            info=mcmc_info,\n",
    "        )\n",
    "\n",
    "        # Accept only if likelihood exceeds threshold (key constraint!)\n",
    "        new_state = jax.lax.cond(\n",
    "            loglikelihood > loglikelihood_0,\n",
    "            lambda _: new_state,\n",
    "            lambda _: state,\n",
    "            operand=None,\n",
    "        )\n",
    "\n",
    "        return new_state, info\n",
    "\n",
    "    def update_inner_kernel_params_fn(state, info, params):\n",
    "        \"\"\"Adapt step size based on current particle distribution.\"\"\"\n",
    "        # Calculate standard deviation for each parameter\n",
    "        sigma_dict = {}\n",
    "        for key in state.particles.keys():\n",
    "            sigma_dict[key] = jnp.std(state.particles[key])\n",
    "        return {'sigma': sigma_dict}\n",
    "\n",
    "    # Build the full nested sampling kernel\n",
    "    _delete_fn = partial(delete_fn, num_delete=num_delete)\n",
    "\n",
    "    step_fn = build_kernel(\n",
    "        logprior_fn,\n",
    "        loglikelihood_fn,\n",
    "        _delete_fn,\n",
    "        inner_kernel,\n",
    "        update_inner_kernel_params_fn,\n",
    "    )\n",
    "\n",
    "    init_fn = partial(\n",
    "        init,\n",
    "        logprior_fn=logprior_fn,\n",
    "        loglikelihood_fn=loglikelihood_fn,\n",
    "        update_inner_kernel_params_fn=update_inner_kernel_params_fn,\n",
    "    )\n",
    "\n",
    "    return SamplingAlgorithm(init_fn, step_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be742aea",
   "metadata": {},
   "source": [
    "### 4.2 Test Custom Sampler on Simple 2D Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2afd0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing custom nested sampler on 2D Gaussian problem...\")\n",
    "\n",
    "# Simple 2D Gaussian test case\n",
    "custom_true = {'mu1': 0.5, 'mu2': -0.2, 'sigma1': 0.8, 'sigma2': 0.6}\n",
    "\n",
    "# Generate test data\n",
    "custom_mu = jnp.array([custom_true['mu1'], custom_true['mu2']])\n",
    "custom_cov = jnp.diag(jnp.array([custom_true['sigma1']**2, custom_true['sigma2']**2]))\n",
    "\n",
    "num_test_samples = 50\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "custom_data = jax.random.multivariate_normal(key, custom_mu, custom_cov, (num_test_samples,))\n",
    "\n",
    "# Define likelihood and prior for custom sampler\n",
    "def custom_loglikelihood(params):\n",
    "    \"\"\"Simple 2D Gaussian likelihood (no correlation).\"\"\"\n",
    "    mu1, mu2, sigma1, sigma2 = params[\"mu1\"], params[\"mu2\"], params[\"sigma1\"], params[\"sigma2\"]\n",
    "    mu = jnp.array([mu1, mu2])\n",
    "    cov = jnp.diag(jnp.array([sigma1**2, sigma2**2]))\n",
    "    return jnp.sum(jax.scipy.stats.multivariate_normal.logpdf(custom_data, mu, cov))\n",
    "\n",
    "custom_prior_bounds = {\n",
    "    \"mu1\": (-2.0, 3.0),\n",
    "    \"mu2\": (-2.0, 2.0),\n",
    "    \"sigma1\": (0.1, 2.0),\n",
    "    \"sigma2\": (0.1, 2.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067f480",
   "metadata": {},
   "source": [
    "### 4.3 Initialize and Run Custom Nested Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02337cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles_custom, logprior_fn_custom = blackjax.ns.utils.uniform_prior(prior_key, num_live, custom_prior_bounds)\n",
    "\n",
    "# Build custom sampler\n",
    "custom_nested_sampler = custom_nsmcmc(\n",
    "    logprior_fn=logprior_fn_custom,\n",
    "    loglikelihood_fn=custom_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=15,  # Slightly more steps for stability\n",
    ")\n",
    "\n",
    "# JIT compile custom sampler functions\n",
    "custom_init_fn = jax.jit(custom_nested_sampler.init)\n",
    "custom_step_fn = jax.jit(custom_nested_sampler.step)\n",
    "\n",
    "print(\"Running custom nested sampler...\")\n",
    "custom_start = time.time()\n",
    "custom_live = custom_init_fn(particles_custom)\n",
    "custom_dead = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points (custom)\", unit=\" dead points\") as pbar:\n",
    "    while not custom_live.logZ_live - custom_live.logZ < -3:\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        custom_live, custom_dead_info = custom_step_fn(subkey, custom_live)\n",
    "        custom_dead.append(custom_dead_info)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "custom_dead = finalise(custom_live, custom_dead)\n",
    "custom_time = time.time() - custom_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263031d",
   "metadata": {},
   "source": [
    "### 4.4 Process Custom Sampler Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b584da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_custom = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\"]\n",
    "labels_custom = [r\"$\\mu_1$\", r\"$\\mu_2$\", r\"$\\sigma_1$\", r\"$\\sigma_2$\"]\n",
    "data_custom = jnp.vstack([custom_dead.particles[key] for key in columns_custom]).T\n",
    "\n",
    "custom_samples = NestedSamples(\n",
    "    data_custom,\n",
    "    logL=custom_dead.loglikelihood,\n",
    "    logL_birth=custom_dead.loglikelihood_birth,\n",
    "    columns=columns_custom,\n",
    "    labels=labels_custom,\n",
    "    logzero=jnp.nan,\n",
    ")\n",
    "\n",
    "print(f\"Custom sampler runtime: {custom_time:.2f} seconds\")\n",
    "print(f\"Log Evidence: {custom_samples.logZ():.2f} Â± {custom_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: Î¼â‚={custom_true['mu1']:.2f}, Î¼â‚‚={custom_true['mu2']:.2f}, Ïƒâ‚={custom_true['sigma1']:.2f}, Ïƒâ‚‚={custom_true['sigma2']:.2f}\")\n",
    "print(f\"Posterior means: Î¼â‚={custom_samples.mu1.mean():.2f}, Î¼â‚‚={custom_samples.mu2.mean():.2f}, Ïƒâ‚={custom_samples.sigma1.mean():.2f}, Ïƒâ‚‚={custom_samples.sigma2.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed69252",
   "metadata": {},
   "source": [
    "Compare with high-level BlackJAX implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ced564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComparing custom implementation with high-level BlackJAX...\")\n",
    "standard_nested_sampler = blackjax.nss(\n",
    "    logprior_fn=logprior_fn_custom,\n",
    "    loglikelihood_fn=custom_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=15,\n",
    ")\n",
    "\n",
    "standard_init_fn = jax.jit(standard_nested_sampler.init)\n",
    "standard_step_fn = jax.jit(standard_nested_sampler.step)\n",
    "\n",
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles_standard, _ = blackjax.ns.utils.uniform_prior(prior_key, num_live, custom_prior_bounds)\n",
    "\n",
    "standard_start = time.time()\n",
    "standard_live = standard_init_fn(particles_standard)\n",
    "standard_dead = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points (standard)\", unit=\" dead points\") as pbar:\n",
    "    while not standard_live.logZ_live - standard_live.logZ < -3:\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        standard_live, standard_dead_info = standard_step_fn(subkey, standard_live)\n",
    "        standard_dead.append(standard_dead_info)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "standard_dead = blackjax.ns.utils.finalise(standard_live, standard_dead)\n",
    "standard_time = time.time() - standard_start\n",
    "\n",
    "data_standard = jnp.vstack([standard_dead.particles[key] for key in columns_custom]).T\n",
    "standard_samples = NestedSamples(\n",
    "    data_standard,\n",
    "    logL=standard_dead.loglikelihood,\n",
    "    logL_birth=standard_dead.loglikelihood_birth,\n",
    "    columns=columns_custom,\n",
    "    labels=labels_custom,\n",
    "    logzero=jnp.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7350fb6",
   "metadata": {},
   "source": [
    "### 4.5 Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "609445ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nImplementation Comparison:\")\n",
    "print(f\"Custom sampler:   {custom_time:.2f} sec, logZ = {custom_samples.logZ():.2f}\")\n",
    "print(f\"Standard sampler: {standard_time:.2f} sec, logZ = {standard_samples.logZ():.2f}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(columns_custom):\n",
    "    axes[i].hist(custom_samples[param].values, bins=25, alpha=0.7, density=True, label='Custom Implementation')\n",
    "    axes[i].hist(standard_samples[param].values, bins=25, alpha=0.7, density=True, label='Standard BlackJAX')\n",
    "    axes[i].axvline(custom_true[param], color='red', linestyle='--', label='True value', alpha=0.8)\n",
    "    axes[i].set_xlabel(labels_custom[i])\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Custom vs Standard Nested Sampling Implementation\", y=1.02)\n",
    "\n",
    "print(\"âœ“ Custom nested sampler implementation successful!\")\n",
    "print(\"This demonstrates how to build specialized samplers using BlackJAX's modular components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e8361",
   "metadata": {},
   "source": "## Part 5: JAX Ecosystem Integration\n\nBuilding on Viraj Pandya's JAX tutorial, we demonstrate how nested sampling integrates\nwith the broader JAX ecosystem for automatic differentiation and gradient-based inference.\n\n### Install Optax for Gradient-Based Optimization",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f22c1814",
   "metadata": {},
   "outputs": [],
   "source": "# Install optax for gradient-based optimization (Part 5)\n!pip install optax\n\nimport optax"
  },
  {
   "cell_type": "markdown",
   "id": "814576a7",
   "metadata": {},
   "source": [
    "### 5.1 JAX-Based 2D Gaussian Inference Problem\n",
    " \n",
    " Following Viraj's tutorial, we'll infer parameters of a 2D Gaussian from image data,\n",
    " then compare gradient descent, HMC, and nested sampling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1108996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up JAX ecosystem example with 2D Gaussian image inference...\")\n",
    "\n",
    "# Define true parameters for image-based 2D Gaussian\n",
    "jax_true = {\n",
    "    'mu_x': 0.1, 'mu_y': -0.1,\n",
    "    'sigma_x': 0.15, 'sigma_y': 0.12,\n",
    "    'rho': 0.3\n",
    "}\n",
    "\n",
    "# Create coordinate grid for 2D Gaussian evaluation\n",
    "grid_size = 32  # Smaller for computational efficiency\n",
    "x_grid_2d = jnp.linspace(-0.8, 0.8, grid_size)\n",
    "y_grid_2d = jnp.linspace(-0.8, 0.8, grid_size)\n",
    "x_meshgrid, y_meshgrid = jnp.meshgrid(x_grid_2d, y_grid_2d)\n",
    "xy_points_2d = jnp.stack([x_meshgrid.ravel(), y_meshgrid.ravel()], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0a8ef",
   "metadata": {},
   "source": [
    "### 5.2 JAX Simulator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "759816e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def jax_simulator(params, rng_key):\n",
    "    \"\"\"\n",
    "    JAX-compiled 2D Gaussian image simulator.\n",
    "    \n",
    "    This demonstrates JAX's JIT compilation and automatic differentiation\n",
    "    capabilities for efficient forward modeling.\n",
    "    \"\"\"\n",
    "    mu_x, mu_y, sigma_x, sigma_y, rho = params\n",
    "    \n",
    "    # Construct mean and covariance matrix\n",
    "    mu = jnp.array([mu_x, mu_y])\n",
    "    cov = jnp.array([\n",
    "        [sigma_x**2, rho * sigma_x * sigma_y],\n",
    "        [rho * sigma_x * sigma_y, sigma_y**2]\n",
    "    ])\n",
    "    \n",
    "    # Evaluate 2D Gaussian probability density on grid\n",
    "    logpdf_grid = jax.scipy.stats.multivariate_normal.logpdf(xy_points_2d, mu, cov)\n",
    "    pdf_grid = jnp.exp(logpdf_grid).reshape(x_meshgrid.shape)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = 0.02 * jax.random.normal(rng_key, shape=pdf_grid.shape)\n",
    "    noisy_image = pdf_grid + noise\n",
    "    \n",
    "    return noisy_image\n",
    "\n",
    "# Generate synthetic observed image\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "jax_params_true = jnp.array([jax_true['mu_x'], jax_true['mu_y'], jax_true['sigma_x'], \n",
    "                            jax_true['sigma_y'], jax_true['rho']])\n",
    "observed_image = jax_simulator(jax_params_true, key)\n",
    "\n",
    "print(f\"Generated {grid_size}x{grid_size} synthetic 2D Gaussian image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b4d7d",
   "metadata": {},
   "source": [
    "### 5.3 Visualize Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cf0425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(observed_image, extent=[-0.8, 0.8, -0.8, 0.8], origin='lower', cmap='viridis')\n",
    "axes[0].set_title(\"Observed 2D Gaussian Image\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "\n",
    "# Show true contours\n",
    "true_image = jax_simulator(jax_params_true, jax.random.PRNGKey(0)) - 0.02 * jax.random.normal(jax.random.PRNGKey(0), shape=observed_image.shape)\n",
    "axes[1].contour(x_grid_2d, y_grid_2d, true_image, levels=8, colors='white', alpha=0.8)\n",
    "axes[1].imshow(observed_image, extent=[-0.8, 0.8, -0.8, 0.8], origin='lower', cmap='viridis')\n",
    "axes[1].set_title(\"Observed Image with True Contours\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"y\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73013a6",
   "metadata": {},
   "source": [
    "### 5.4 Define Loss Function for Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4eedb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def image_loss(params):\n",
    "    \"\"\"Loss function for gradient-based optimization.\"\"\"\n",
    "    pred_image = jax_simulator(params, jax.random.PRNGKey(42))  # Fixed key for deterministic prediction\n",
    "    residuals = pred_image - observed_image\n",
    "    return jnp.sum(residuals**2) / 2  # L2 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d7c23",
   "metadata": {},
   "source": [
    "### 5.5 Gradient Descent with Optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4f746b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running gradient descent optimization...\")\n",
    "\n",
    "# Initialize parameters\n",
    "init_params = jnp.array([0.0, 0.0, 0.2, 0.2, 0.0])\n",
    "\n",
    "# Set up optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(init_params)\n",
    "\n",
    "# Gradient function\n",
    "grad_fn = jax.jit(jax.grad(image_loss))\n",
    "\n",
    "# Optimization loop\n",
    "params = init_params\n",
    "losses = []\n",
    "param_history = []\n",
    "\n",
    "for i in range(200):\n",
    "    loss_val = image_loss(params)\n",
    "    grads = grad_fn(params)\n",
    "    \n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    losses.append(loss_val)\n",
    "    param_history.append(params)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"Step {i}: Loss = {loss_val:.4f}\")\n",
    "\n",
    "final_params_gd = params\n",
    "param_history = jnp.stack(param_history)\n",
    "\n",
    "print(f\"Gradient descent final parameters: {final_params_gd}\")\n",
    "print(f\"True parameters: {jax_params_true}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a12a49",
   "metadata": {},
   "source": [
    "### 5.6 Nested Sampling on Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fcb6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running nested sampling on image data...\")\n",
    "\n",
    "def image_loglikelihood(params_dict):\n",
    "    \"\"\"Log-likelihood for nested sampling on image data.\"\"\"\n",
    "    params_array = jnp.array([params_dict['mu_x'], params_dict['mu_y'], \n",
    "                             params_dict['sigma_x'], params_dict['sigma_y'], \n",
    "                             params_dict['rho']])\n",
    "    \n",
    "    # Forward model (deterministic for likelihood evaluation)\n",
    "    pred_image = jax_simulator(params_array, jax.random.PRNGKey(42))\n",
    "    \n",
    "    # Gaussian likelihood (independent pixels)\n",
    "    sigma_obs = 0.02  # Known observation noise\n",
    "    loglik = jnp.sum(jax.scipy.stats.norm.logpdf(observed_image, pred_image, sigma_obs))\n",
    "    \n",
    "    return loglik\n",
    "\n",
    "# Define priors for nested sampling\n",
    "image_prior_bounds = {\n",
    "    'mu_x': (-0.5, 0.5),\n",
    "    'mu_y': (-0.5, 0.5),\n",
    "    'sigma_x': (0.05, 0.3),\n",
    "    'sigma_y': (0.05, 0.3),\n",
    "    'rho': (-0.8, 0.8),\n",
    "}\n",
    "\n",
    "# Initialize nested sampler\n",
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles_image, logprior_fn_image = blackjax.ns.utils.uniform_prior(prior_key, num_live, image_prior_bounds)\n",
    "\n",
    "nested_sampler_image = blackjax.nss(\n",
    "    logprior_fn=logprior_fn_image,\n",
    "    loglikelihood_fn=image_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=25,  # More steps for this complex problem\n",
    ")\n",
    "\n",
    "# JIT compile\n",
    "init_fn_image = jax.jit(nested_sampler_image.init)\n",
    "step_fn_image = jax.jit(nested_sampler_image.step)\n",
    "\n",
    "# Run nested sampling\n",
    "print(\"Running nested sampling...\")\n",
    "ns_image_start = time.time()\n",
    "live_image = init_fn_image(particles_image)\n",
    "dead_image = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points (image)\", unit=\" dead points\") as pbar:\n",
    "    while not live_image.logZ_live - live_image.logZ < -3:\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live_image, dead_info_image = step_fn_image(subkey, live_image)\n",
    "        dead_image.append(dead_info_image)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "dead_image = blackjax.ns.utils.finalise(live_image, dead_image)\n",
    "ns_image_time = time.time() - ns_image_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957ccb1",
   "metadata": {},
   "source": [
    "### 5.7 Process Image Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "275a813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_image = ['mu_x', 'mu_y', 'sigma_x', 'sigma_y', 'rho']\n",
    "labels_image = [r'$\\mu_x$', r'$\\mu_y$', r'$\\sigma_x$', r'$\\sigma_y$', r'$\\rho$']\n",
    "data_image = jnp.vstack([dead_image.particles[key] for key in columns_image]).T\n",
    "\n",
    "image_samples = NestedSamples(\n",
    "    data_image,\n",
    "    logL=dead_image.loglikelihood,\n",
    "    logL_birth=dead_image.loglikelihood_birth,\n",
    "    columns=columns_image,\n",
    "    labels=labels_image,\n",
    "    logzero=jnp.nan,\n",
    ")\n",
    "\n",
    "print(f\"Image inference results:\")\n",
    "print(f\"Nested sampling runtime: {ns_image_time:.2f} seconds\")\n",
    "print(f\"Log Evidence: {image_samples.logZ():.2f} Â± {image_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: Î¼â‚“={jax_true['mu_x']:.3f}, Î¼áµ§={jax_true['mu_y']:.3f}, Ïƒâ‚“={jax_true['sigma_x']:.3f}, Ïƒáµ§={jax_true['sigma_y']:.3f}, Ï={jax_true['rho']:.3f}\")\n",
    "print(f\"NS posterior means: Î¼â‚“={image_samples.mu_x.mean():.3f}, Î¼áµ§={image_samples.mu_y.mean():.3f}, Ïƒâ‚“={image_samples.sigma_x.mean():.3f}, Ïƒáµ§={image_samples.sigma_y.mean():.3f}, Ï={image_samples.rho.mean():.3f}\")\n",
    "print(f\"GD final estimates: Î¼â‚“={final_params_gd[0]:.3f}, Î¼áµ§={final_params_gd[1]:.3f}, Ïƒâ‚“={final_params_gd[2]:.3f}, Ïƒáµ§={final_params_gd[3]:.3f}, Ï={final_params_gd[4]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c3aa0",
   "metadata": {},
   "source": [
    "### 5.8 Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a901504",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Parameter evolution during gradient descent\n",
    "param_labels = ['Î¼â‚“', 'Î¼áµ§', 'Ïƒâ‚“', 'Ïƒáµ§', 'Ï']\n",
    "jax_true_vals = [jax_true['mu_x'], jax_true['mu_y'], jax_true['sigma_x'], jax_true['sigma_y'], jax_true['rho']]\n",
    "\n",
    "for i in range(5):\n",
    "    row, col = i // 3, i % 3\n",
    "    axes[row, col].plot(param_history[:, i], label='Gradient Descent', alpha=0.8)\n",
    "    axes[row, col].axhline(jax_true_vals[i], color='red', linestyle='--', alpha=0.8, label='True Value')\n",
    "    axes[row, col].set_ylabel(param_labels[i])\n",
    "    axes[row, col].set_xlabel('Optimization Step')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss evolution\n",
    "axes[1, 2].plot(losses, color='blue', alpha=0.8)\n",
    "axes[1, 2].set_ylabel('Loss')\n",
    "axes[1, 2].set_xlabel('Optimization Step')\n",
    "axes[1, 2].set_title('Gradient Descent Convergence')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"JAX Ecosystem: Gradient Descent vs Nested Sampling\", y=1.02)\n",
    "\n",
    "print(\"âœ“ JAX ecosystem integration complete!\")\n",
    "print(\"This demonstrates the complementary strengths of gradient-based and nested sampling approaches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cbf643",
   "metadata": {},
   "source": "## Part 6: Simulation-Based Inference (SBI) Integration\n\nWe demonstrate how nested sampling integrates with modern SBI techniques,\nspecifically neural posterior estimation (NPE) using JAX and simple neural networks.\n\n### Install Flax for Neural Networks",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a4410192",
   "metadata": {},
   "outputs": [],
   "source": "# Install flax for neural networks (Part 6)  \n!pip install flax\n\nfrom flax import linen as nn\n\nprint(\"Setting up SBI integration example with neural posterior estimation...\")"
  },
  {
   "cell_type": "markdown",
   "id": "4f9102d6",
   "metadata": {},
   "source": [
    "### 6.1 SBI Training Data Generation\n",
    "\n",
    " Generate a large dataset of (parameters, simulations) pairs for training\n",
    " a neural network to approximate the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41b07382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating SBI training dataset...\")\n",
    "\n",
    "def generate_sbi_dataset(n_samples, rng_key):\n",
    "    \"\"\"Generate training data for neural posterior estimation.\"\"\"\n",
    "    # Sample parameters from prior\n",
    "    key_prior, key_sim = jax.random.split(rng_key)\n",
    "    \n",
    "    # SBI prior bounds (same as image inference)\n",
    "    sbi_bounds = jnp.array([\n",
    "        [-0.5, 0.5],  # mu_x\n",
    "        [-0.5, 0.5],  # mu_y  \n",
    "        [0.05, 0.3],  # sigma_x\n",
    "        [0.05, 0.3],  # sigma_y\n",
    "        [-0.8, 0.8],  # rho\n",
    "    ])\n",
    "    \n",
    "    # Sample uniformly from prior\n",
    "    params_sbi = jax.random.uniform(key_prior, (n_samples, 5))\n",
    "    params_sbi = params_sbi * (sbi_bounds[:, 1] - sbi_bounds[:, 0]) + sbi_bounds[:, 0]\n",
    "    \n",
    "    # Generate simulations for each parameter set\n",
    "    sim_keys = jax.random.split(key_sim, n_samples)\n",
    "    \n",
    "    @jax.jit\n",
    "    def simulate_batch(params_batch, keys_batch):\n",
    "        \"\"\"Vectorized simulation function.\"\"\"\n",
    "        return jax.vmap(jax_simulator)(params_batch, keys_batch)\n",
    "    \n",
    "    # Generate simulations\n",
    "    simulations = simulate_batch(params_sbi, sim_keys)\n",
    "    \n",
    "    return params_sbi, simulations\n",
    "\n",
    "# Generate training dataset\n",
    "n_sbi_samples = 5000  # Moderate size for workshop\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "sbi_params, sbi_sims = generate_sbi_dataset(n_sbi_samples, key)\n",
    "\n",
    "print(f\"Generated SBI dataset: {n_sbi_samples} parameter-simulation pairs\")\n",
    "print(f\"Parameter shape: {sbi_params.shape}, Simulation shape: {sbi_sims.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda2396",
   "metadata": {},
   "source": [
    "### 6.2 Neural Posterior Network\n",
    "\n",
    " Define a simple neural network that maps from simulations to posterior parameters.\n",
    " This is a basic implementation of neural posterior estimation (NPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cf140bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPENetwork(nn.Module):\n",
    "    \"\"\"Neural Posterior Estimation network.\"\"\"\n",
    "    hidden_dims: tuple = (64, 64)\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Flatten simulation data\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for dim in self.hidden_dims:\n",
    "            x = nn.Dense(dim)(x)\n",
    "            x = nn.relu(x)\n",
    "        \n",
    "        # Output layer (5 parameters)\n",
    "        x = nn.Dense(5)(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ecfa4",
   "metadata": {},
   "source": [
    "### 6.3 Training the Neural Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "576b4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training neural posterior network...\")\n",
    "\n",
    "def train_npe_network(network, params_data, sims_data, n_epochs=300, learning_rate=1e-3):\n",
    "    \"\"\"Train the neural posterior estimation network.\"\"\"\n",
    "    # Initialize network\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    network_params = network.init(rng, sims_data[:1])\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(network_params)\n",
    "    \n",
    "    # Loss function\n",
    "    def mse_loss(network_params, sims_batch, params_batch):\n",
    "        pred_params = network.apply(network_params, sims_batch)\n",
    "        return jnp.mean((pred_params - params_batch)**2)\n",
    "    \n",
    "    # Training step\n",
    "    @jax.jit\n",
    "    def train_step(network_params, opt_state, sims_batch, params_batch):\n",
    "        loss, grads = jax.value_and_grad(mse_loss)(network_params, sims_batch, params_batch)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        network_params = optax.apply_updates(network_params, updates)\n",
    "        return network_params, opt_state, loss\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # Simple full-batch training (could use mini-batches for larger datasets)\n",
    "        network_params, opt_state, loss = train_step(network_params, opt_state, sims_data, params_data)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    return network_params, losses\n",
    "\n",
    "# Train the network\n",
    "npe_network = NPENetwork()\n",
    "npe_start = time.time()\n",
    "trained_npe_params, npe_losses = train_npe_network(npe_network, sbi_params, sbi_sims)\n",
    "npe_time = time.time() - npe_start\n",
    "\n",
    "print(f\"NPE training completed in {npe_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815c48a",
   "metadata": {},
   "source": [
    "### 6.4 SBI Posterior Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b448a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing SBI posterior inference on observed data...\")\n",
    "\n",
    "# Use the same observed image from Part 5\n",
    "npe_posterior_params = npe_network.apply(trained_npe_params, observed_image[None, ...])\n",
    "npe_prediction = npe_posterior_params[0]  # Remove batch dimension\n",
    "\n",
    "print(f\"SBI posterior prediction: Î¼â‚“={npe_prediction[0]:.3f}, Î¼áµ§={npe_prediction[1]:.3f}, Ïƒâ‚“={npe_prediction[2]:.3f}, Ïƒáµ§={npe_prediction[3]:.3f}, Ï={npe_prediction[4]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3e8ca",
   "metadata": {},
   "source": [
    "### 6.5 Generate Test Dataset for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34282c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating SBI approach on test dataset...\")\n",
    "\n",
    "# Generate test data\n",
    "n_test_sbi = 100\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "test_sbi_params, test_sbi_sims = generate_sbi_dataset(n_test_sbi, key)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = npe_network.apply(trained_npe_params, test_sbi_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19afa9",
   "metadata": {},
   "source": [
    "### 6.6 Comparison: Nested Sampling vs SBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06b74279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing nested sampling and SBI approaches...\")\n",
    "\n",
    "# For a fair comparison, run nested sampling on a few test cases\n",
    "comparison_results = []\n",
    "\n",
    "for i in range(3):  # Compare on 3 test cases\n",
    "    test_obs = test_sbi_sims[i]\n",
    "    true_params = test_sbi_params[i]\n",
    "    \n",
    "    # SBI prediction\n",
    "    sbi_pred = test_predictions[i]\n",
    "    \n",
    "    # Nested sampling inference  \n",
    "    def test_loglikelihood(params_dict):\n",
    "        params_array = jnp.array([params_dict['mu_x'], params_dict['mu_y'], \n",
    "                                 params_dict['sigma_x'], params_dict['sigma_y'], \n",
    "                                 params_dict['rho']])\n",
    "        pred_image = jax_simulator(params_array, jax.random.PRNGKey(42))\n",
    "        sigma_obs = 0.02\n",
    "        return jnp.sum(jax.scipy.stats.norm.logpdf(test_obs, pred_image, sigma_obs))\n",
    "    \n",
    "    # Quick nested sampling run (fewer live points for speed)\n",
    "    rng_key, prior_key = jax.random.split(rng_key)\n",
    "    particles_test, logprior_fn_test = blackjax.ns.utils.uniform_prior(prior_key, 50, image_prior_bounds)\n",
    "    \n",
    "    nested_sampler_test = blackjax.nss(\n",
    "        logprior_fn=logprior_fn_test,\n",
    "        loglikelihood_fn=test_loglikelihood,\n",
    "        num_delete=25,\n",
    "        num_inner_steps=15,\n",
    "    )\n",
    "    \n",
    "    init_fn_test = jax.jit(nested_sampler_test.init)\n",
    "    step_fn_test = jax.jit(nested_sampler_test.step)\n",
    "    \n",
    "    # Run nested sampling\n",
    "    ns_test_start = time.time()\n",
    "    live_test = init_fn_test(particles_test)\n",
    "    dead_test = []\n",
    "    \n",
    "    # Shorter run for comparison\n",
    "    while len(dead_test) < 10:  # Just get some samples quickly\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live_test, dead_info_test = step_fn_test(subkey, live_test)\n",
    "        dead_test.append(dead_info_test)\n",
    "    \n",
    "    ns_test_time = time.time() - ns_test_start\n",
    "    \n",
    "    # Get posterior mean from nested sampling\n",
    "    dead_test = blackjax.ns.utils.finalise(live_test, dead_test)\n",
    "    data_test = jnp.vstack([dead_test.particles[key] for key in columns_image]).T\n",
    "    ns_mean = jnp.mean(data_test, axis=0)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'true': true_params,\n",
    "        'sbi': sbi_pred,\n",
    "        'ns': ns_mean,\n",
    "        'ns_time': ns_test_time\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb052ac",
   "metadata": {},
   "source": [
    "### 6.7 Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc7d1b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating SBI comparison results...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(npe_losses, alpha=0.8)\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_title('NPE Training Progress')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter prediction accuracy on test set\n",
    "param_names_sbi = ['Î¼â‚“', 'Î¼áµ§', 'Ïƒâ‚“', 'Ïƒáµ§', 'Ï']\n",
    "for i in range(5):\n",
    "    row = 0 if i < 3 else 1\n",
    "    col = i if i < 3 else i - 3\n",
    "    if row == 1 and col >= 2:\n",
    "        continue\n",
    "        \n",
    "    axes[row, col if row == 0 else col + 1].scatter(test_sbi_params[:, i], test_predictions[:, i], alpha=0.6, s=20)\n",
    "    axes[row, col if row == 0 else col + 1].plot([test_sbi_params[:, i].min(), test_sbi_params[:, i].max()], \n",
    "                                                [test_sbi_params[:, i].min(), test_sbi_params[:, i].max()], \n",
    "                                                'r--', alpha=0.8)\n",
    "    axes[row, col if row == 0 else col + 1].set_xlabel(f'True {param_names_sbi[i]}')\n",
    "    axes[row, col if row == 0 else col + 1].set_ylabel(f'Predicted {param_names_sbi[i]}')\n",
    "    axes[row, col if row == 0 else col + 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Method comparison\n",
    "methods = ['True', 'SBI (NPE)', 'Nested Sampling']\n",
    "colors = ['black', 'blue', 'red']\n",
    "\n",
    "comparison_data = jnp.array([[r['true'], r['sbi'], r['ns']] for r in comparison_results])\n",
    "\n",
    "axes[1, 2].clear()\n",
    "for i, method in enumerate(methods):\n",
    "    for j in range(3):  # 3 test cases\n",
    "        axes[1, 2].scatter([i] * 5, comparison_data[j, i], \n",
    "                          alpha=0.7, s=30, color=colors[i])\n",
    "\n",
    "axes[1, 2].set_xticks(range(3))\n",
    "axes[1, 2].set_xticklabels(methods)\n",
    "axes[1, 2].set_ylabel('Parameter Values')\n",
    "axes[1, 2].set_title('Method Comparison (3 Test Cases)')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"SBI vs Nested Sampling: Neural Posterior Estimation\", y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab031cbd",
   "metadata": {},
   "source": [
    "### 6.8 Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f540ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SBI INTEGRATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute accuracy metrics\n",
    "sbi_errors = jnp.abs(test_predictions - test_sbi_params)\n",
    "sbi_mean_error = jnp.mean(sbi_errors, axis=0)\n",
    "\n",
    "print(f\"Neural Posterior Estimation (NPE) Results:\")\n",
    "print(f\"Training time: {npe_time:.2f} seconds ({n_sbi_samples} samples)\")\n",
    "print(f\"Inference time per observation: ~0.001 seconds (instant)\")\n",
    "print(f\"Mean absolute errors by parameter:\")\n",
    "for i, param_name in enumerate(param_names_sbi):\n",
    "    print(f\"  {param_name}: {sbi_mean_error[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison with Nested Sampling:\")\n",
    "ns_times = [r['ns_time'] for r in comparison_results]\n",
    "avg_ns_time = sum(ns_times) / len(ns_times)  # Use Python mean instead of jnp.mean\n",
    "print(f\"NS average time per inference: {avg_ns_time:.2f} seconds\")\n",
    "print(f\"SBI advantage: ~{avg_ns_time/0.001:.0f}x faster inference\")\n",
    "\n",
    "print(f\"\\nSBI Trade-offs:\")\n",
    "print(f\"âœ“ Extremely fast inference once trained\")\n",
    "print(f\"âœ“ Amortized: single training enables inference on any observation\")\n",
    "print(f\"âœ“ Scalable to high-dimensional problems\")\n",
    "print(f\"âœ— Requires large training dataset\")\n",
    "print(f\"âœ— Point estimates only (no uncertainty quantification)\")\n",
    "print(f\"âœ— Limited to training distribution\")\n",
    "\n",
    "print(f\"\\nNested Sampling Trade-offs:\")\n",
    "print(f\"âœ“ Full posterior uncertainty quantification\")\n",
    "print(f\"âœ“ Bayesian evidence for model comparison\")\n",
    "print(f\"âœ“ No training required\")\n",
    "print(f\"âœ— Slower per-observation inference\")\n",
    "print(f\"âœ— Computational cost scales with problem complexity\")\n",
    "\n",
    "print(\"âœ“ SBI integration example complete!\")\n",
    "print(\"This demonstrates how neural networks can complement traditional Bayesian methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db7a03",
   "metadata": {},
   "source": [
    "## Workshop Conclusions\n",
    "\n",
    " This comprehensive BlackJAX nested sampling workshop demonstrates the full spectrum of modern Bayesian inference:\n",
    "\n",
    " ### Core Concepts Demonstrated\n",
    " 1. **GPU-Native Performance**: BlackJAX leverages JAX for automatic differentiation and JIT compilation\n",
    " 2. **Evidence Computation**: Nested sampling naturally provides Bayesian evidence for model comparison\n",
    " 3. **Parameter Transforms**: Proper handling of constrained parameters (correlation coefficients, etc.)\n",
    " 4. **Anesthetic Integration**: Professional post-processing and visualization of nested sampling results\n",
    "\n",
    " ### Advanced Techniques Covered\n",
    " 5. **Sampler Comparison**: Nested sampling vs. NUTS - understanding when to use which approach\n",
    " 6. **Custom Implementation**: Building specialized samplers using BlackJAX's modular components\n",
    " 7. **JAX Ecosystem**: Integration with gradient descent and the broader scientific Python ecosystem\n",
    " 8. **SBI Methods**: Neural posterior estimation and comparison with traditional Bayesian approaches\n",
    "\n",
    " ### Key Performance Insights\n",
    " - **Nested Sampling**: Excellent for evidence computation, uncertainty quantification, no training required\n",
    " - **Gradient Methods**: Fast convergence for optimization problems, good starting points for sampling\n",
    " - **Neural SBI**: Extremely fast amortized inference, but requires training data and gives point estimates\n",
    " - **Custom Samplers**: Research flexibility while maintaining computational efficiency\n",
    "\n",
    " ### Practical Recommendations\n",
    " **For Model Comparison**: Use nested sampling for Bayesian evidence computation\n",
    " **For Production Systems**: Consider SBI for real-time inference after offline training\n",
    " **For Research**: Leverage BlackJAX's modular design for custom sampling strategies\n",
    " **For Complex Problems**: Combine approaches - use gradient methods for initialization, nested sampling for uncertainty\n",
    "\n",
    " ### Next Steps\n",
    " Try BlackJAX nested sampling on your own problems! Consider:\n",
    " - Adjusting `num_live` and `num_inner_steps` for accuracy vs. speed trade-offs\n",
    " - Implementing custom prior distributions and parameter transforms\n",
    " - Comparing with other BlackJAX samplers (HMC, NUTS, MALA, etc.)\n",
    " - Building custom samplers for specialized applications\n",
    " - Integrating with SBI workflows for high-throughput inference\n",
    " - Exploring nested sampling for model selection and hypothesis testing\n",
    "\n",
    " **Workshop Structure**: This tutorial offers flexible delivery options:\n",
    " - **Core Workshop (20 minutes)**: Parts 1-3 covering essential nested sampling concepts\n",
    " - **Full Workshop (110 minutes)**: All 6 parts including advanced research techniques\n",
    " - **Modular Extensions**: Parts 4-6 can be used individually based on audience interests\n",
    " \n",
    " Each section builds on the previous, demonstrating the evolution from basic concepts to cutting-edge research techniques."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
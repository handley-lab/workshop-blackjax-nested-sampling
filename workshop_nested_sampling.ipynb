{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec880df",
   "metadata": {},
   "source": [
    "# BlackJAX Nested Sampling Workshop\n",
    "\n",
    " This workshop demonstrates GPU-native nested sampling using BlackJAX. We'll progress through three examples: line fitting, 2D Gaussian inference, and performance comparisons with other samplers. The workshop showcases JAX's key strengths: automatic differentiation and JIT compilation for high-performance Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install required packages for Google Colab\n!pip install git+https://github.com/handley-lab/blackjax\n!pip install anesthetic tqdm",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Note**: The cell above installs the required packages. In Google Colab, this may take 1-2 minutes and will show some warnings - this is normal!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "f59886d5",
   "metadata": {},
   "source": [
    "## Installation for Google Colab\n",
    "```bash\n",
    " pip install git+https://github.com/handley-lab/blackjax@nested_sampling\n",
    " pip install anesthetic tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71040c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cb289",
   "metadata": {},
   "source": [
    "Configure JAX immediately after import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import time\n",
    "import blackjax\n",
    "from anesthetic import NestedSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5512e05d",
   "metadata": {},
   "source": [
    "## Part 1: Line Fitting with Nested Sampling\n",
    "\n",
    " We start with the classic problem of fitting a linear model y = mx + c to noisy data.\n",
    " This introduces the basic nested sampling workflow in BlackJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afade7e5",
   "metadata": {},
   "source": [
    "### 1.1 Nested Sampling Configuration\n",
    " \n",
    " Key parameters for workshop timing and educational value:\n",
    " - `num_live=100`: Fast convergence for workshop setting\n",
    " - `num_delete=50`: Parallelization parameter\n",
    " - `num_inner_steps`: Reliability parameter (rule of thumb: 5 * num_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27641dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)\n",
    "num_live = 100\n",
    "num_delete = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31615fd",
   "metadata": {},
   "source": [
    "### 1.2 Generate Synthetic Line Data\n",
    "\n",
    " True model: y = 2x + 1 + noise, with σ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 15\n",
    "x = jnp.linspace(-2.0, 2.0, num_data)\n",
    "true = {'m': 2.0, 'c': 1.0, 'sigma': 0.5}\n",
    "\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "noise = true['sigma'] * jax.random.normal(key, (num_data,))\n",
    "y = true['m'] * x + true['c'] + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111e8c8",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.errorbar(x, y, yerr=true['sigma'], fmt=\"o\", label=\"Observed data\", color='black')\n",
    "ax.plot(x, true['m'] * x + true['c'], '--', label=\"True model\", color='red', alpha=0.7)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Linear Model: Bayesian Parameter Estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8aed5",
   "metadata": {},
   "source": [
    "### 1.3 Define Likelihood Function\n",
    "\n",
    " Gaussian likelihood with unknown slope, intercept, and noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6054c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_loglikelihood(params):\n",
    "    \"\"\"Log-likelihood for linear model with Gaussian noise.\"\"\"\n",
    "    m, c, sigma = params[\"m\"], params[\"c\"], params[\"sigma\"]\n",
    "    y_model = m * x + c\n",
    "    # Vectorized normal log-likelihood\n",
    "    return jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135b0cf",
   "metadata": {},
   "source": [
    "### 1.4 Define Prior Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_bounds = {\n",
    "    \"m\": (-5.0, 5.0),      # slope\n",
    "    \"c\": (-5.0, 5.0),      # intercept  \n",
    "    \"sigma\": (0.1, 2.0),   # noise level (positive)\n",
    "}\n",
    "\n",
    "num_dims = len(prior_bounds)\n",
    "num_inner_steps = num_dims * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd8fb3e",
   "metadata": {},
   "source": [
    "### 1.5 Initialize Nested Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce54e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles, logprior_fn = blackjax.ns.utils.uniform_prior(prior_key, num_live, prior_bounds)\n",
    "\n",
    "nested_sampler = blackjax.nss(\n",
    "    logprior_fn=logprior_fn,\n",
    "    loglikelihood_fn=line_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=num_inner_steps,\n",
    ")\n",
    "print(f\"Initialized nested sampler with {num_live} live points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd798df7",
   "metadata": {},
   "source": [
    "### 1.6 JIT Compile for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn = jax.jit(nested_sampler.init)\n",
    "step_fn = jax.jit(nested_sampler.step)\n",
    "print(\"Functions compiled - ready to run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5ed7e",
   "metadata": {},
   "source": [
    "### 1.7 Run the Nested Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b079ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running nested sampling for line fitting...\")\n",
    "ns_start = time.time()\n",
    "live = init_fn(particles)\n",
    "dead = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
    "    while not live.logZ_live - live.logZ < -3:  # Convergence criterion\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live, dead_info = step_fn(subkey, live)\n",
    "        dead.append(dead_info)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "dead = blackjax.ns.utils.finalise(live, dead)\n",
    "ns_time = time.time() - ns_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7705aee",
   "metadata": {},
   "source": [
    "### 1.8 Process Results with Anesthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24634f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"m\", \"c\", \"sigma\"]\n",
    "labels = [r\"$m$\", r\"$c$\", r\"$\\sigma$\"]\n",
    "data = jnp.vstack([dead.particles[key] for key in columns]).T\n",
    "\n",
    "line_samples = NestedSamples(\n",
    "    data,\n",
    "    logL=dead.loglikelihood,\n",
    "    logL_birth=dead.loglikelihood_birth,\n",
    "    columns=columns,\n",
    "    labels=labels,\n",
    "    logzero=jnp.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f10ce",
   "metadata": {},
   "source": [
    "### 1.9 Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nested sampling runtime: {ns_time:.2f} seconds\")\n",
    "print(f\"Log Evidence: {line_samples.logZ():.2f} ± {line_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: m={true['m']}, c={true['c']}, σ={true['sigma']}\")\n",
    "print(f\"Posterior means: m={line_samples.m.mean():.2f}, c={line_samples.c.mean():.2f}, σ={line_samples.sigma.mean():.2f}\")\n",
    "\n",
    "# Create posterior corner plot with true values marked\n",
    "kinds = {'lower': 'kde_2d', 'diagonal': 'hist_1d', 'upper': 'scatter_2d'}\n",
    "axes = line_samples.plot_2d(columns, kinds=kinds, label='Posterior')\n",
    "axes.axlines(true, color='red', linestyle='--', alpha=0.8)\n",
    "plt.suptitle(\"Line Fitting: Posterior Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c2b53",
   "metadata": {},
   "source": [
    "## Part 2: 2D Gaussian Parameter Inference\n",
    "\n",
    " Now we tackle a more complex problem: inferring the parameters of a 2D Gaussian distribution\n",
    " including the correlation coefficient. This demonstrates parameter transforms and constrained sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb7c37",
   "metadata": {},
   "source": [
    "### 2.1 Define 2D Gaussian Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.update({\n",
    "    'mu1': 1.0, 'mu2': -0.5,\n",
    "    'sigma1': 1.2, 'sigma2': 0.8, \n",
    "    'rho': 0.6\n",
    "})\n",
    "print(\"True parameters:\", {k: v for k, v in true.items() if k in ['mu1', 'mu2', 'sigma1', 'sigma2', 'rho']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bd044",
   "metadata": {},
   "source": [
    "### 2.2 Generate Correlated 2D Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96206516",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu = jnp.array([true['mu1'], true['mu2']])\n",
    "true_cov = jnp.array([\n",
    "    [true['sigma1']**2, true['rho'] * true['sigma1'] * true['sigma2']],\n",
    "    [true['rho'] * true['sigma1'] * true['sigma2'], true['sigma2']**2]\n",
    "])\n",
    "\n",
    "num_samples = 200\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "gaussian_data = jax.random.multivariate_normal(key, true_mu, true_cov, (num_samples,))\n",
    "\n",
    "print(f\"Generated {num_samples} correlated 2D samples\")\n",
    "print(f\"Sample mean: [{gaussian_data.mean(0)[0]:.2f}, {gaussian_data.mean(0)[1]:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e54e42",
   "metadata": {},
   "source": [
    "### 2.3 Visualize the 2D Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d777f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(gaussian_data[:, 0], gaussian_data[:, 1], alpha=0.6, s=20)\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_title(\"2D Gaussian Data\")\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02728a",
   "metadata": {},
   "source": [
    "### 2.4 Define Likelihood with Parameter Transforms\n",
    "\n",
    " We use arctanh/tanh transform for the correlation coefficient to enforce |ρ| < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_2d_loglikelihood(params):\n",
    "    \"\"\"Log-likelihood for 2D Gaussian with correlation.\"\"\"\n",
    "    mu1, mu2 = params[\"mu1\"], params[\"mu2\"]\n",
    "    sigma1, sigma2 = params[\"sigma1\"], params[\"sigma2\"]\n",
    "    rho_transformed = params[\"rho_t\"]\n",
    "    \n",
    "    # Transform correlation coefficient: rho = tanh(rho_t)\n",
    "    rho = jnp.tanh(rho_transformed)\n",
    "    \n",
    "    # Construct covariance matrix\n",
    "    cov = jnp.array([\n",
    "        [sigma1**2, rho * sigma1 * sigma2],\n",
    "        [rho * sigma1 * sigma2, sigma2**2]\n",
    "    ])\n",
    "    \n",
    "    # Check positive definiteness\n",
    "    det = jnp.linalg.det(cov)\n",
    "    \n",
    "    # Return -inf for invalid covariance matrices\n",
    "    def valid_loglik():\n",
    "        mu = jnp.array([mu1, mu2])\n",
    "        return jnp.sum(jax.scipy.stats.multivariate_normal.logpdf(gaussian_data, mu, cov))\n",
    "    \n",
    "    def invalid_loglik():\n",
    "        return -jnp.inf\n",
    "    \n",
    "    return jax.lax.cond(det > 1e-8, valid_loglik, invalid_loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fefba5",
   "metadata": {},
   "source": [
    "### 2.5 Set Up Priors for 2D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_prior_bounds = {\n",
    "    \"mu1\": (-3.0, 5.0),\n",
    "    \"mu2\": (-3.0, 3.0), \n",
    "    \"sigma1\": (0.1, 3.0),\n",
    "    \"sigma2\": (0.1, 3.0),\n",
    "    \"rho_t\": (-2.0, 2.0),  # transformed correlation: rho = tanh(rho_t)\n",
    "}\n",
    "\n",
    "num_dims_2d = len(gaussian_prior_bounds)\n",
    "num_inner_steps_2d = num_dims_2d * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd726d98",
   "metadata": {},
   "source": [
    "### 2.6 Initialize and Run Nested Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles_2d, logprior_fn_2d = blackjax.ns.utils.uniform_prior(prior_key, num_live, gaussian_prior_bounds)\n",
    "\n",
    "nested_sampler_2d = blackjax.nss(\n",
    "    logprior_fn=logprior_fn_2d,\n",
    "    loglikelihood_fn=gaussian_2d_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=num_inner_steps_2d,\n",
    ")\n",
    "\n",
    "init_fn_2d = jax.jit(nested_sampler_2d.init)\n",
    "step_fn_2d = jax.jit(nested_sampler_2d.step)\n",
    "\n",
    "print(\"Running nested sampling for 2D Gaussian...\")\n",
    "live_2d = init_fn_2d(particles_2d)\n",
    "dead_2d = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
    "    while not live_2d.logZ_live - live_2d.logZ < -3:\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live_2d, dead_info_2d = step_fn_2d(subkey, live_2d)\n",
    "        dead_2d.append(dead_info_2d)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "dead_2d = blackjax.ns.utils.finalise(live_2d, dead_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36007f",
   "metadata": {},
   "source": [
    "### 2.7 Transform Back and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2d = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho_t\"]\n",
    "labels_2d = [r\"$\\mu_1$\", r\"$\\mu_2$\", r\"$\\sigma_1$\", r\"$\\sigma_2$\", r\"$\\rho_t$\"]\n",
    "data_2d = jnp.vstack([dead_2d.particles[key] for key in columns_2d]).T\n",
    "\n",
    "gaussian_samples = NestedSamples(\n",
    "    data_2d,\n",
    "    logL=dead_2d.loglikelihood,\n",
    "    logL_birth=dead_2d.loglikelihood_birth,\n",
    "    columns=columns_2d,\n",
    "    labels=labels_2d,\n",
    "    logzero=jnp.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234998fc",
   "metadata": {},
   "source": [
    "Add transformed correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_samples[\"rho\"] = jnp.tanh(gaussian_samples[\"rho_t\"].values)\n",
    "\n",
    "print(f\"Log Evidence: {gaussian_samples.logZ():.2f} ± {gaussian_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: μ₁={true['mu1']:.2f}, μ₂={true['mu2']:.2f}, σ₁={true['sigma1']:.2f}, σ₂={true['sigma2']:.2f}, ρ={true['rho']:.2f}\")\n",
    "print(f\"Posterior means: μ₁={gaussian_samples.mu1.mean():.2f}, μ₂={gaussian_samples.mu2.mean():.2f}, σ₁={gaussian_samples.sigma1.mean():.2f}, σ₂={gaussian_samples.sigma2.mean():.2f}, ρ={gaussian_samples.rho.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83bf6d",
   "metadata": {},
   "source": [
    "Plot posterior for key parameters with true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_params = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho\"]\n",
    "axes = gaussian_samples[key_params].plot_2d(key_params, kinds={'diagonal': 'hist_1d', 'lower': 'kde_2d'})\n",
    "\n",
    "# Mark true values using anesthetic's axlines method\n",
    "true_2d = {k: true[k] for k in key_params}\n",
    "axes.axlines(true_2d, color='red', linestyle='--', alpha=0.8)\n",
    "plt.suptitle(\"2D Gaussian: Posterior Parameter Estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ffddd",
   "metadata": {},
   "source": [
    "## Part 3: Performance Comparison\n",
    "\n",
    " Compare BlackJAX nested sampling with NUTS (No-U-Turn Sampler) and \n",
    " Affine Invariant Ensemble Sampler on the line fitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feb010",
   "metadata": {},
   "source": [
    "### 3.1 Define NUTS Log-Probability Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0914b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuts_logprob(params_array):\n",
    "    \"\"\"Combined log probability for NUTS (assumes flat priors within bounds).\"\"\"\n",
    "    m, c, log_sigma = params_array\n",
    "    sigma = jnp.exp(log_sigma)  # positive constraint via log transform\n",
    "    \n",
    "    # Check bounds (flat prior)\n",
    "    m_valid = (m >= -5.0) & (m <= 5.0)\n",
    "    c_valid = (c >= -5.0) & (c <= 5.0)\n",
    "    sigma_valid = (sigma >= 0.1) & (sigma <= 2.0)\n",
    "    \n",
    "    def valid_logprob():\n",
    "        y_model = m * x + c\n",
    "        loglik = jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)\n",
    "        return loglik + log_sigma  # Add Jacobian for log transform\n",
    "    \n",
    "    def invalid_logprob():\n",
    "        return -jnp.inf\n",
    "    \n",
    "    return jax.lax.cond(m_valid & c_valid & sigma_valid, valid_logprob, invalid_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce286e",
   "metadata": {},
   "source": [
    "### 3.2 Initialize and Run NUTS Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87876e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_position = jnp.array([1.0, 0.0, jnp.log(1.0)])  # [m, c, log_sigma]\n",
    "nuts = blackjax.nuts(nuts_logprob, step_size=0.1, inverse_mass_matrix=jnp.eye(3))\n",
    "\n",
    "rng_key, nuts_key = jax.random.split(rng_key)\n",
    "nuts_state = nuts.init(initial_position)\n",
    "nuts_step = jax.jit(nuts.step)\n",
    "\n",
    "print(\"Running NUTS sampler...\")\n",
    "\n",
    "num_nuts_samples = 2000\n",
    "nuts_start = time.time()\n",
    "nuts_samples = []\n",
    "nuts_states = nuts_state\n",
    "\n",
    "for i in tqdm.tqdm(range(num_nuts_samples), desc=\"NUTS\"):\n",
    "    nuts_key, step_key = jax.random.split(nuts_key)\n",
    "    nuts_states, nuts_info = nuts_step(step_key, nuts_states)\n",
    "    nuts_samples.append(nuts_states.position)\n",
    "\n",
    "nuts_time = time.time() - nuts_start\n",
    "nuts_samples = jnp.stack(nuts_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5669849",
   "metadata": {},
   "source": [
    "### 3.3 Process NUTS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08850f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_m = nuts_samples[:, 0]\n",
    "nuts_c = nuts_samples[:, 1] \n",
    "nuts_sigma = jnp.exp(nuts_samples[:, 2])\n",
    "\n",
    "print(f\"NUTS runtime: {nuts_time:.2f} seconds\")\n",
    "print(f\"NUTS means: m={nuts_m[500:].mean():.2f}, c={nuts_c[500:].mean():.2f}, σ={nuts_sigma[500:].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f364b32",
   "metadata": {},
   "source": [
    "### 3.4 Performance Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"Nested Sampling\", \"NUTS\"]\n",
    "times = [f\"{ns_time:.1f} sec\", f\"{nuts_time:.1f} sec\"]\n",
    "evidence = [\"✓ (Log Z available)\", \"✗ (Not computed)\"]\n",
    "parallelization = [\"✓ (GPU native)\", \"Limited\"]\n",
    "\n",
    "print(f\"{'Method':<20} {'Time':<15} {'Evidence':<15} {'GPU Parallel'}\")\n",
    "print(\"-\" * 65)\n",
    "for i in range(len(methods)):\n",
    "    print(f\"{methods[i]:<20} {times[i]:<15} {evidence[i]:<15} {parallelization[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e23990",
   "metadata": {},
   "source": [
    "### 3.5 Posterior Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8f74c",
   "metadata": {},
   "source": [
    "Generate proper posterior samples from NestedSamples (not raw dead points)\n",
    " Use the number of available samples or 1000, whichever is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ceb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_posterior_samples = min(1000, len(line_samples))\n",
    "ns_posterior_samples = line_samples.sample(n_posterior_samples, replace=True)  # Sample from posterior with replacement\n",
    "nuts_burnin = 500  # Remove burn-in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7396278",
   "metadata": {},
   "source": [
    "Compare marginal posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes[0].hist(ns_posterior_samples.m.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\n",
    "axes[0].hist(nuts_m[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\n",
    "axes[0].axvline(true['m'], color='red', linestyle='--', label='True value')\n",
    "axes[0].set_xlabel('Slope (m)')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(ns_posterior_samples.c.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\n",
    "axes[1].hist(nuts_c[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\n",
    "axes[1].axvline(true['c'], color='red', linestyle='--', label='True value')\n",
    "axes[1].set_xlabel('Intercept (c)')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(ns_posterior_samples.sigma.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\n",
    "axes[2].hist(nuts_sigma[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\n",
    "axes[2].axvline(true['sigma'], color='red', linestyle='--', label='True value')\n",
    "axes[2].set_xlabel('Noise (σ)')\n",
    "axes[2].set_ylabel('Density')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Posterior Comparison: Nested Sampling vs NUTS\", y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8a51d",
   "metadata": {},
   "source": [
    "## Workshop Conclusions\n",
    "\n",
    " Key takeaways from this BlackJAX nested sampling workshop:\n",
    "\n",
    " 1. **GPU-Native Performance**: BlackJAX leverages JAX for automatic differentiation and JIT compilation\n",
    " 2. **Evidence Computation**: Nested sampling naturally provides Bayesian evidence (model comparison)\n",
    " 3. **Parameter Transforms**: Proper handling of constrained parameters (e.g., correlation coefficients)\n",
    " 4. **Anesthetic Integration**: Professional post-processing and visualization of nested sampling results\n",
    " 5. **Sampler Comparison**: Different samplers have different strengths for different problems\n",
    "\n",
    " **Next Steps**: Try BlackJAX nested sampling on your own problems! Consider:\n",
    " - Adjusting `num_live` for accuracy vs. speed trade-offs\n",
    " - Using more complex prior distributions  \n",
    " - Comparing with other BlackJAX samplers (HMC, NUTS, etc.)\n",
    " - Exploring nested sampling for model selection problems"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
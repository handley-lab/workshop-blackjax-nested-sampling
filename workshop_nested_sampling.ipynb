{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f31c0a4",
      "metadata": {},
      "source": [
        "# BlackJAX Nested Sampling Workshop\n",
        "\n",
        " This workshop demonstrates GPU-native nested sampling using BlackJAX. We'll progress through three examples: line fitting, 2D Gaussian inference, and performance comparisons with other samplers. The workshop showcases JAX's key strengths: automatic differentiation and JIT compilation for high-performance Bayesian inference."
      ]
    },
    {
      "cell_type": "code",
      "source": "# Install required packages for Google Colab\n!pip install git+https://github.com/handley-lab/blackjax\n!pip install anesthetic tqdm",
      "metadata": {},
      "outputs": [],
      "id": "49cd7c4d",
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "**Note**: The cell above installs the required packages. In Google Colab, this may take 1-2 minutes and will show some warnings - this is normal!",
      "metadata": {},
      "id": "8785d89c"
    },
    {
      "cell_type": "markdown",
      "id": "de5248ea",
      "metadata": {},
      "source": [
        "## Installation for Google Colab\n",
        "```bash\n",
        " pip install git+https://github.com/handley-lab/blackjax@nested_sampling\n",
        " pip install anesthetic tqdm\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92596d96",
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b54e7b",
      "metadata": {},
      "source": [
        "Configure JAX immediately after import"
      ]
    },
    {
      "cell_type": "code",
      "id": "c2288b7e",
      "metadata": {},
      "outputs": [],
      "source": "jax.config.update(\"jax_enable_x64\", True)\n\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tqdm\nimport time\nimport blackjax\nfrom anesthetic import NestedSamples\n\n# Enable inline matplotlib for Jupyter\n%matplotlib inline\nplt.style.use('default')",
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "id": "f2bddaa4",
      "metadata": {},
      "source": [
        "## Part 1: Line Fitting with Nested Sampling\n",
        "\n",
        " We start with the classic problem of fitting a linear model y = mx + c to noisy data.\n",
        " This introduces the basic nested sampling workflow in BlackJAX."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4d97b1",
      "metadata": {},
      "source": [
        "### 1.1 Nested Sampling Configuration\n",
        " \n",
        " Key parameters for workshop timing and educational value:\n",
        " - `num_live=100`: Fast convergence for workshop setting\n",
        " - `num_delete=50`: Parallelization parameter\n",
        " - `num_inner_steps`: Reliability parameter (rule of thumb: 5 * num_dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ca17ca46",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng_key = jax.random.PRNGKey(42)\n",
        "num_live = 100\n",
        "num_delete = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8dbfd4",
      "metadata": {},
      "source": [
        "### 1.2 Generate Synthetic Line Data\n",
        "\n",
        " True model: y = 2x + 1 + noise, with \u03c3 = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "75477f2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_data = 15\n",
        "x = jnp.linspace(-2.0, 2.0, num_data)\n",
        "true = {'m': 2.0, 'c': 1.0, 'sigma': 0.5}\n",
        "\n",
        "key, rng_key = jax.random.split(rng_key)\n",
        "noise = true['sigma'] * jax.random.normal(key, (num_data,))\n",
        "y = true['m'] * x + true['c'] + noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c0e615",
      "metadata": {},
      "source": [
        "Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "13428868",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.errorbar(x, y, yerr=true['sigma'], fmt=\"o\", label=\"Observed data\", color='black')\n",
        "ax.plot(x, true['m'] * x + true['c'], '--', label=\"True model\", color='red', alpha=0.7)\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "ax.legend()\n",
        "ax.set_title(\"Linear Model: Bayesian Parameter Estimation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac88be73",
      "metadata": {},
      "source": [
        "### 1.3 Define Likelihood Function\n",
        "\n",
        " Gaussian likelihood with unknown slope, intercept, and noise level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0a9cecaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def line_loglikelihood(params):\n",
        "    \"\"\"Log-likelihood for linear model with Gaussian noise.\"\"\"\n",
        "    m, c, sigma = params[\"m\"], params[\"c\"], params[\"sigma\"]\n",
        "    y_model = m * x + c\n",
        "    # Vectorized normal log-likelihood\n",
        "    return jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d63ce49",
      "metadata": {},
      "source": [
        "### 1.4 Define Prior Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c63f5a1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "prior_bounds = {\n",
        "    \"m\": (-5.0, 5.0),      # slope\n",
        "    \"c\": (-5.0, 5.0),      # intercept  \n",
        "    \"sigma\": (0.1, 2.0),   # noise level (positive)\n",
        "}\n",
        "\n",
        "num_dims = len(prior_bounds)\n",
        "num_inner_steps = num_dims * 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a8f64b",
      "metadata": {},
      "source": [
        "### 1.5 Initialize Nested Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9bf3e066",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng_key, prior_key = jax.random.split(rng_key)\n",
        "particles, logprior_fn = blackjax.ns.utils.uniform_prior(prior_key, num_live, prior_bounds)\n",
        "\n",
        "nested_sampler = blackjax.nss(\n",
        "    logprior_fn=logprior_fn,\n",
        "    loglikelihood_fn=line_loglikelihood,\n",
        "    num_delete=num_delete,\n",
        "    num_inner_steps=num_inner_steps,\n",
        ")\n",
        "print(f\"Initialized nested sampler with {num_live} live points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5433b4e5",
      "metadata": {},
      "source": [
        "### 1.6 JIT Compile for Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e979edde",
      "metadata": {},
      "outputs": [],
      "source": [
        "init_fn = jax.jit(nested_sampler.init)\n",
        "step_fn = jax.jit(nested_sampler.step)\n",
        "print(\"Functions compiled - ready to run!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d8ade5",
      "metadata": {},
      "source": [
        "### 1.7 Run the Nested Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b2070339",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running nested sampling for line fitting...\")\n",
        "ns_start = time.time()\n",
        "live = init_fn(particles)\n",
        "dead = []\n",
        "\n",
        "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
        "    while not live.logZ_live - live.logZ < -3:  # Convergence criterion\n",
        "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
        "        live, dead_info = step_fn(subkey, live)\n",
        "        dead.append(dead_info)\n",
        "        pbar.update(num_delete)\n",
        "\n",
        "dead = blackjax.ns.utils.finalise(live, dead)\n",
        "ns_time = time.time() - ns_start"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed2fa07",
      "metadata": {},
      "source": [
        "### 1.8 Process Results with Anesthetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c2b5e55f",
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = [\"m\", \"c\", \"sigma\"]\n",
        "labels = [r\"$m$\", r\"$c$\", r\"$\\sigma$\"]\n",
        "data = jnp.vstack([dead.particles[key] for key in columns]).T\n",
        "\n",
        "line_samples = NestedSamples(\n",
        "    data,\n",
        "    logL=dead.loglikelihood,\n",
        "    logL_birth=dead.loglikelihood_birth,\n",
        "    columns=columns,\n",
        "    labels=labels,\n",
        "    logzero=jnp.nan,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a56656e",
      "metadata": {},
      "source": [
        "### 1.9 Results Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b9a67da3",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Nested sampling runtime: {ns_time:.2f} seconds\")\n",
        "print(f\"Log Evidence: {line_samples.logZ():.2f} \u00b1 {line_samples.logZ(100).std():.2f}\")\n",
        "print(f\"True parameters: m={true['m']}, c={true['c']}, \u03c3={true['sigma']}\")\n",
        "print(f\"Posterior means: m={line_samples.m.mean():.2f}, c={line_samples.c.mean():.2f}, \u03c3={line_samples.sigma.mean():.2f}\")\n",
        "\n",
        "# Create posterior corner plot with true values marked\n",
        "kinds = {'lower': 'kde_2d', 'diagonal': 'hist_1d', 'upper': 'scatter_2d'}\n",
        "axes = line_samples.plot_2d(columns, kinds=kinds, label='Posterior')\n",
        "axes.axlines(true, color='red', linestyle='--', alpha=0.8)\n",
        "plt.suptitle(\"Line Fitting: Posterior Distributions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e93653c",
      "metadata": {},
      "source": [
        "## Part 2: 2D Gaussian Parameter Inference\n",
        "\n",
        " Now we tackle a more complex problem: inferring the parameters of a 2D Gaussian distribution\n",
        " including the correlation coefficient. This demonstrates parameter transforms and constrained sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39b2c4f",
      "metadata": {},
      "source": [
        "### 2.1 Define 2D Gaussian Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a5bacc6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "true.update({\n",
        "    'mu1': 1.0, 'mu2': -0.5,\n",
        "    'sigma1': 1.2, 'sigma2': 0.8, \n",
        "    'rho': 0.6\n",
        "})\n",
        "print(\"True parameters:\", {k: v for k, v in true.items() if k in ['mu1', 'mu2', 'sigma1', 'sigma2', 'rho']})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f8a5e6",
      "metadata": {},
      "source": [
        "### 2.2 Generate Correlated 2D Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6a873fc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "true_mu = jnp.array([true['mu1'], true['mu2']])\n",
        "true_cov = jnp.array([\n",
        "    [true['sigma1']**2, true['rho'] * true['sigma1'] * true['sigma2']],\n",
        "    [true['rho'] * true['sigma1'] * true['sigma2'], true['sigma2']**2]\n",
        "])\n",
        "\n",
        "num_samples = 200\n",
        "key, rng_key = jax.random.split(rng_key)\n",
        "gaussian_data = jax.random.multivariate_normal(key, true_mu, true_cov, (num_samples,))\n",
        "\n",
        "print(f\"Generated {num_samples} correlated 2D samples\")\n",
        "print(f\"Sample mean: [{gaussian_data.mean(0)[0]:.2f}, {gaussian_data.mean(0)[1]:.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "938f0f0d",
      "metadata": {},
      "source": [
        "### 2.3 Visualize the 2D Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0ac2e63b",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.scatter(gaussian_data[:, 0], gaussian_data[:, 1], alpha=0.6, s=20)\n",
        "ax.set_xlabel(r\"$x_1$\")\n",
        "ax.set_ylabel(r\"$x_2$\")\n",
        "ax.set_title(\"2D Gaussian Data\")\n",
        "ax.grid(True, alpha=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "990ed009",
      "metadata": {},
      "source": [
        "### 2.4 Define Likelihood with Parameter Transforms\n",
        "\n",
        " We use arctanh/tanh transform for the correlation coefficient to enforce |\u03c1| < 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "16073a44",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gaussian_2d_loglikelihood(params):\n",
        "    \"\"\"Log-likelihood for 2D Gaussian with correlation.\"\"\"\n",
        "    mu1, mu2 = params[\"mu1\"], params[\"mu2\"]\n",
        "    sigma1, sigma2 = params[\"sigma1\"], params[\"sigma2\"]\n",
        "    rho_transformed = params[\"rho_t\"]\n",
        "    \n",
        "    # Transform correlation coefficient: rho = tanh(rho_t)\n",
        "    rho = jnp.tanh(rho_transformed)\n",
        "    \n",
        "    # Construct covariance matrix\n",
        "    cov = jnp.array([\n",
        "        [sigma1**2, rho * sigma1 * sigma2],\n",
        "        [rho * sigma1 * sigma2, sigma2**2]\n",
        "    ])\n",
        "    \n",
        "    # Check positive definiteness\n",
        "    det = jnp.linalg.det(cov)\n",
        "    \n",
        "    # Return -inf for invalid covariance matrices\n",
        "    def valid_loglik():\n",
        "        mu = jnp.array([mu1, mu2])\n",
        "        return jnp.sum(jax.scipy.stats.multivariate_normal.logpdf(gaussian_data, mu, cov))\n",
        "    \n",
        "    def invalid_loglik():\n",
        "        return -jnp.inf\n",
        "    \n",
        "    return jax.lax.cond(det > 1e-8, valid_loglik, invalid_loglik)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51920a6e",
      "metadata": {},
      "source": [
        "### 2.5 Set Up Priors for 2D Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a65ce3df",
      "metadata": {},
      "outputs": [],
      "source": [
        "gaussian_prior_bounds = {\n",
        "    \"mu1\": (-3.0, 5.0),\n",
        "    \"mu2\": (-3.0, 3.0), \n",
        "    \"sigma1\": (0.1, 3.0),\n",
        "    \"sigma2\": (0.1, 3.0),\n",
        "    \"rho_t\": (-2.0, 2.0),  # transformed correlation: rho = tanh(rho_t)\n",
        "}\n",
        "\n",
        "num_dims_2d = len(gaussian_prior_bounds)\n",
        "num_inner_steps_2d = num_dims_2d * 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c9666c",
      "metadata": {},
      "source": [
        "### 2.6 Initialize and Run Nested Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "75e0a228",
      "metadata": {},
      "outputs": [],
      "source": [
        "rng_key, prior_key = jax.random.split(rng_key)\n",
        "particles_2d, logprior_fn_2d = blackjax.ns.utils.uniform_prior(prior_key, num_live, gaussian_prior_bounds)\n",
        "\n",
        "nested_sampler_2d = blackjax.nss(\n",
        "    logprior_fn=logprior_fn_2d,\n",
        "    loglikelihood_fn=gaussian_2d_loglikelihood,\n",
        "    num_delete=num_delete,\n",
        "    num_inner_steps=num_inner_steps_2d,\n",
        ")\n",
        "\n",
        "init_fn_2d = jax.jit(nested_sampler_2d.init)\n",
        "step_fn_2d = jax.jit(nested_sampler_2d.step)\n",
        "\n",
        "print(\"Running nested sampling for 2D Gaussian...\")\n",
        "live_2d = init_fn_2d(particles_2d)\n",
        "dead_2d = []\n",
        "\n",
        "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
        "    while not live_2d.logZ_live - live_2d.logZ < -3:\n",
        "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
        "        live_2d, dead_info_2d = step_fn_2d(subkey, live_2d)\n",
        "        dead_2d.append(dead_info_2d)\n",
        "        pbar.update(num_delete)\n",
        "\n",
        "dead_2d = blackjax.ns.utils.finalise(live_2d, dead_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcb2bf0",
      "metadata": {},
      "source": [
        "### 2.7 Transform Back and Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3297702d",
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_2d = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho_t\"]\n",
        "labels_2d = [r\"$\\mu_1$\", r\"$\\mu_2$\", r\"$\\sigma_1$\", r\"$\\sigma_2$\", r\"$\\rho_t$\"]\n",
        "data_2d = jnp.vstack([dead_2d.particles[key] for key in columns_2d]).T\n",
        "\n",
        "gaussian_samples = NestedSamples(\n",
        "    data_2d,\n",
        "    logL=dead_2d.loglikelihood,\n",
        "    logL_birth=dead_2d.loglikelihood_birth,\n",
        "    columns=columns_2d,\n",
        "    labels=labels_2d,\n",
        "    logzero=jnp.nan,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b9e357c",
      "metadata": {},
      "source": [
        "Add transformed correlation coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "320451dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "gaussian_samples[\"rho\"] = jnp.tanh(gaussian_samples[\"rho_t\"].values)\n",
        "\n",
        "print(f\"Log Evidence: {gaussian_samples.logZ():.2f} \u00b1 {gaussian_samples.logZ(100).std():.2f}\")\n",
        "print(f\"True parameters: \u03bc\u2081={true['mu1']:.2f}, \u03bc\u2082={true['mu2']:.2f}, \u03c3\u2081={true['sigma1']:.2f}, \u03c3\u2082={true['sigma2']:.2f}, \u03c1={true['rho']:.2f}\")\n",
        "print(f\"Posterior means: \u03bc\u2081={gaussian_samples.mu1.mean():.2f}, \u03bc\u2082={gaussian_samples.mu2.mean():.2f}, \u03c3\u2081={gaussian_samples.sigma1.mean():.2f}, \u03c3\u2082={gaussian_samples.sigma2.mean():.2f}, \u03c1={gaussian_samples.rho.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed9c089",
      "metadata": {},
      "source": [
        "Plot posterior for key parameters with true values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "88fb2c47",
      "metadata": {},
      "outputs": [],
      "source": [
        "key_params = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho\"]\n",
        "axes = gaussian_samples[key_params].plot_2d(key_params, kinds={'diagonal': 'hist_1d', 'lower': 'kde_2d'})\n",
        "\n",
        "# Mark true values using anesthetic's axlines method\n",
        "true_2d = {k: true[k] for k in key_params}\n",
        "axes.axlines(true_2d, color='red', linestyle='--', alpha=0.8)\n",
        "plt.suptitle(\"2D Gaussian: Posterior Parameter Estimates\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95a27300",
      "metadata": {},
      "source": [
        "## Part 3: Performance Comparison\n",
        "\n",
        " Compare BlackJAX nested sampling with NUTS (No-U-Turn Sampler) and \n",
        " Affine Invariant Ensemble Sampler on the line fitting problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f2fc58ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8919c643",
      "metadata": {},
      "source": [
        "### 3.1 Define NUTS Log-Probability Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ea6c55f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def nuts_logprob(params_array):\n",
        "    \"\"\"Combined log probability for NUTS (assumes flat priors within bounds).\"\"\"\n",
        "    m, c, log_sigma = params_array\n",
        "    sigma = jnp.exp(log_sigma)  # positive constraint via log transform\n",
        "    \n",
        "    # Check bounds (flat prior)\n",
        "    m_valid = (m >= -5.0) & (m <= 5.0)\n",
        "    c_valid = (c >= -5.0) & (c <= 5.0)\n",
        "    sigma_valid = (sigma >= 0.1) & (sigma <= 2.0)\n",
        "    \n",
        "    def valid_logprob():\n",
        "        y_model = m * x + c\n",
        "        loglik = jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)\n",
        "        return loglik + log_sigma  # Add Jacobian for log transform\n",
        "    \n",
        "    def invalid_logprob():\n",
        "        return -jnp.inf\n",
        "    \n",
        "    return jax.lax.cond(m_valid & c_valid & sigma_valid, valid_logprob, invalid_logprob)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103e4e84",
      "metadata": {},
      "source": [
        "### 3.2 Initialize and Run NUTS Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "737b8cec",
      "metadata": {},
      "outputs": [],
      "source": [
        "initial_position = jnp.array([1.0, 0.0, jnp.log(1.0)])  # [m, c, log_sigma]\n",
        "nuts = blackjax.nuts(nuts_logprob, step_size=0.1, inverse_mass_matrix=jnp.eye(3))\n",
        "\n",
        "rng_key, nuts_key = jax.random.split(rng_key)\n",
        "nuts_state = nuts.init(initial_position)\n",
        "nuts_step = jax.jit(nuts.step)\n",
        "\n",
        "print(\"Running NUTS sampler...\")\n",
        "\n",
        "num_nuts_samples = 2000\n",
        "nuts_start = time.time()\n",
        "nuts_samples = []\n",
        "nuts_states = nuts_state\n",
        "\n",
        "for i in tqdm.tqdm(range(num_nuts_samples), desc=\"NUTS\"):\n",
        "    nuts_key, step_key = jax.random.split(nuts_key)\n",
        "    nuts_states, nuts_info = nuts_step(step_key, nuts_states)\n",
        "    nuts_samples.append(nuts_states.position)\n",
        "\n",
        "nuts_time = time.time() - nuts_start\n",
        "nuts_samples = jnp.stack(nuts_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e46fdf5",
      "metadata": {},
      "source": [
        "### 3.3 Process NUTS Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4efb7698",
      "metadata": {},
      "outputs": [],
      "source": [
        "nuts_m = nuts_samples[:, 0]\n",
        "nuts_c = nuts_samples[:, 1] \n",
        "nuts_sigma = jnp.exp(nuts_samples[:, 2])\n",
        "\n",
        "print(f\"NUTS runtime: {nuts_time:.2f} seconds\")\n",
        "print(f\"NUTS means: m={nuts_m[500:].mean():.2f}, c={nuts_c[500:].mean():.2f}, \u03c3={nuts_sigma[500:].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb85810",
      "metadata": {},
      "source": [
        "### 3.4 Performance Summary and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c8e40e47",
      "metadata": {},
      "outputs": [],
      "source": [
        "methods = [\"Nested Sampling\", \"NUTS\"]\n",
        "times = [f\"{ns_time:.1f} sec\", f\"{nuts_time:.1f} sec\"]\n",
        "evidence = [\"\u2713 (Log Z available)\", \"\u2717 (Not computed)\"]\n",
        "parallelization = [\"\u2713 (GPU native)\", \"Limited\"]\n",
        "\n",
        "print(f\"{'Method':<20} {'Time':<15} {'Evidence':<15} {'GPU Parallel'}\")\n",
        "print(\"-\" * 65)\n",
        "for i in range(len(methods)):\n",
        "    print(f\"{methods[i]:<20} {times[i]:<15} {evidence[i]:<15} {parallelization[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbf8eb4",
      "metadata": {},
      "source": [
        "### 3.5 Posterior Comparison Plot"
      ]
    },
    {
      "cell_type": "code",
      "id": "8d214eb7",
      "metadata": {},
      "outputs": [],
      "source": "# Generate proper posterior samples from NestedSamples (not raw dead points)\n# Use the number of available samples or 1000, whichever is smaller\nn_posterior_samples = min(1000, len(line_samples))\nns_posterior_samples = line_samples.sample(n_posterior_samples, replace=True)  # Sample from posterior with replacement\nnuts_burnin = 500  # Remove burn-in\n\n# Create comparison plots\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Compare marginal posteriors\naxes[0].hist(ns_posterior_samples.m.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\naxes[0].hist(nuts_m[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\naxes[0].axvline(true['m'], color='red', linestyle='--', label='True value')\naxes[0].set_xlabel('Slope (m)')\naxes[0].set_ylabel('Density')\naxes[0].legend()\n\naxes[1].hist(ns_posterior_samples.c.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\naxes[1].hist(nuts_c[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\naxes[1].axvline(true['c'], color='red', linestyle='--', label='True value')\naxes[1].set_xlabel('Intercept (c)')\naxes[1].set_ylabel('Density')\naxes[1].legend()\n\naxes[2].hist(ns_posterior_samples.sigma.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\naxes[2].hist(nuts_sigma[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\naxes[2].axvline(true['sigma'], color='red', linestyle='--', label='True value')\naxes[2].set_xlabel('Noise (\u03c3)')\naxes[2].set_ylabel('Density')\naxes[2].legend()\n\nplt.tight_layout()\nplt.suptitle(\"Posterior Comparison: Nested Sampling vs NUTS\", y=1.02)",
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "id": "b6c81abe",
      "metadata": {},
      "source": [
        "## Workshop Conclusions\n",
        "\n",
        " Key takeaways from this BlackJAX nested sampling workshop:\n",
        "\n",
        " 1. **GPU-Native Performance**: BlackJAX leverages JAX for automatic differentiation and JIT compilation\n",
        " 2. **Evidence Computation**: Nested sampling naturally provides Bayesian evidence (model comparison)\n",
        " 3. **Parameter Transforms**: Proper handling of constrained parameters (e.g., correlation coefficients)\n",
        " 4. **Anesthetic Integration**: Professional post-processing and visualization of nested sampling results\n",
        " 5. **Sampler Comparison**: Different samplers have different strengths for different problems\n",
        "\n",
        " **Next Steps**: Try BlackJAX nested sampling on your own problems! Consider:\n",
        " - Adjusting `num_live` for accuracy vs. speed trade-offs\n",
        " - Using more complex prior distributions  \n",
        " - Comparing with other BlackJAX samplers (HMC, NUTS, etc.)\n",
        " - Exploring nested sampling for model selection problems"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
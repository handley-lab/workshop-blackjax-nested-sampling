{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e4180f",
   "metadata": {},
   "source": [
    "# BlackJAX Nested Sampling Workshop\n",
    "\n",
    " This workshop demonstrates GPU-native nested sampling using BlackJAX. We'll progress through three examples: line fitting, 2D Gaussian inference, and performance comparisons with other samplers. The workshop showcases JAX's key strengths: automatic differentiation and JIT compilation for high-performance Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487b3ef",
   "metadata": {},
   "source": [
    "## Installation for Google Colab\n",
    "```bash\n",
    " pip install git+https://github.com/handley-lab/blackjax@nested_sampling\n",
    " pip install anesthetic tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1429ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e193fda",
   "metadata": {},
   "source": [
    "Configure JAX immediately after import"
   ]
  },
  {
   "cell_type": "code",
   "id": "e65c7dab",
   "metadata": {},
   "outputs": [],
   "source": "jax.config.update(\"jax_enable_x64\", True)\n\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tqdm\nimport time\nimport blackjax\nfrom anesthetic import NestedSamples\n\n# For Google Colab compatibility\ntry:\n    import google.colab\n    # In Colab, use default backend\n    pass\nexcept ImportError:\n    # Local environment, use Agg backend for headless operation\n    import matplotlib\n    matplotlib.use('Agg')"
  },
  {
   "cell_type": "markdown",
   "id": "6a697e82",
   "metadata": {},
   "source": [
    "## Part 1: Line Fitting with Nested Sampling\n",
    "\n",
    " We start with the classic problem of fitting a linear model y = mx + c to noisy data.\n",
    " This introduces the basic nested sampling workflow in BlackJAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PART 1: BAYESIAN LINEAR REGRESSION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c219a",
   "metadata": {},
   "source": [
    "### 1.1 Nested Sampling Configuration\n",
    " \n",
    " Key parameters for workshop timing and educational value:\n",
    " - `num_live=100`: Fast convergence for workshop setting\n",
    " - `num_delete=50`: Parallelization parameter\n",
    " - `num_inner_steps`: Reliability parameter (rule of thumb: 5 * num_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24779444",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(42)\n",
    "num_live = 100\n",
    "num_delete = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f37a5",
   "metadata": {},
   "source": [
    "### 1.2 Generate Synthetic Line Data\n",
    "\n",
    " True model: y = 2x + 1 + noise, with σ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f65874",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 15\n",
    "x = jnp.linspace(-2.0, 2.0, num_data)\n",
    "true = {'m': 2.0, 'c': 1.0, 'sigma': 0.5}\n",
    "\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "noise = true['sigma'] * jax.random.normal(key, (num_data,))\n",
    "y = true['m'] * x + true['c'] + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e727b43",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "df5cef3b",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(8, 5))\nax.errorbar(x, y, yerr=true['sigma'], fmt=\"o\", label=\"Observed data\", color='black')\nax.plot(x, true['m'] * x + true['c'], '--', label=\"True model\", color='red', alpha=0.7)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.legend()\nax.set_title(\"Linear Model: Bayesian Parameter Estimation\")\nplt.show()  # Show plot in notebook\nplt.savefig(\"line_data.png\", dpi=150, bbox_inches='tight')\nprint(\"Saved plot: line_data.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "6f4c229f",
   "metadata": {},
   "source": [
    "### 1.3 Define Likelihood Function\n",
    "\n",
    " Gaussian likelihood with unknown slope, intercept, and noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_loglikelihood(params):\n",
    "    \"\"\"Log-likelihood for linear model with Gaussian noise.\"\"\"\n",
    "    m, c, sigma = params[\"m\"], params[\"c\"], params[\"sigma\"]\n",
    "    y_model = m * x + c\n",
    "    # Vectorized normal log-likelihood\n",
    "    return jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97926b",
   "metadata": {},
   "source": [
    "### 1.4 Set Up Priors and Run Nested Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_bounds = {\n",
    "    \"m\": (-5.0, 5.0),      # slope\n",
    "    \"c\": (-5.0, 5.0),      # intercept  \n",
    "    \"sigma\": (0.1, 2.0),   # noise level (positive)\n",
    "}\n",
    "\n",
    "num_dims = len(prior_bounds)\n",
    "num_inner_steps = num_dims * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65e5e8",
   "metadata": {},
   "source": [
    "Initialize uniform priors and nested sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles, logprior_fn = blackjax.ns.utils.uniform_prior(prior_key, num_live, prior_bounds)\n",
    "\n",
    "nested_sampler = blackjax.nss(\n",
    "    logprior_fn=logprior_fn,\n",
    "    loglikelihood_fn=line_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=num_inner_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bc489",
   "metadata": {},
   "source": [
    "JIT compile for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e598f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn = jax.jit(nested_sampler.init)\n",
    "step_fn = jax.jit(nested_sampler.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84753059",
   "metadata": {},
   "source": [
    "Run the nested sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d47b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running nested sampling for line fitting...\")\n",
    "ns_start = time.time()\n",
    "live = init_fn(particles)\n",
    "dead = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
    "    while not live.logZ_live - live.logZ < -3:  # Convergence criterion\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live, dead_info = step_fn(subkey, live)\n",
    "        dead.append(dead_info)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "dead = blackjax.ns.utils.finalise(live, dead)\n",
    "ns_time = time.time() - ns_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc02141",
   "metadata": {},
   "source": [
    "### 1.5 Process Results with Anesthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31803677",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"m\", \"c\", \"sigma\"]\n",
    "labels = [r\"$m$\", r\"$c$\", r\"$\\sigma$\"]\n",
    "data = jnp.vstack([dead.particles[key] for key in columns]).T\n",
    "\n",
    "line_samples = NestedSamples(\n",
    "    data,\n",
    "    logL=dead.loglikelihood,\n",
    "    logL_birth=dead.loglikelihood_birth,\n",
    "    columns=columns,\n",
    "    labels=labels,\n",
    "    logzero=jnp.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012e784",
   "metadata": {},
   "source": [
    "Evidence and parameter estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5035fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nested sampling runtime: {ns_time:.2f} seconds\")\n",
    "print(f\"Log Evidence: {line_samples.logZ():.2f} ± {line_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: m={true['m']}, c={true['c']}, σ={true['sigma']}\")\n",
    "print(f\"Posterior means: m={line_samples.m.mean():.2f}, c={line_samples.c.mean():.2f}, σ={line_samples.sigma.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f226ba",
   "metadata": {},
   "source": [
    "Posterior visualization with true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24362673",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = {'lower': 'kde_2d', 'diagonal': 'hist_1d', 'upper': 'scatter_2d'}\n",
    "axes = line_samples.plot_2d(columns, kinds=kinds, label='Posterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc7cb0",
   "metadata": {},
   "source": [
    "Mark true values using anesthetic's axlines method"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed61c6bf",
   "metadata": {},
   "outputs": [],
   "source": "axes.axlines(true, color='red', linestyle='--', alpha=0.8)\n\nplt.suptitle(\"Line Fitting: Posterior Distributions\")\nplt.show()  # Show plot in notebook\nplt.savefig(\"line_posterior.png\", dpi=150, bbox_inches='tight')\nprint(\"Saved plot: line_posterior.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "5dda1803",
   "metadata": {},
   "source": [
    "## Part 2: 2D Gaussian Parameter Inference\n",
    "\n",
    " Now we tackle a more complex problem: inferring the parameters of a 2D Gaussian distribution\n",
    " including the correlation coefficient. This demonstrates parameter transforms and constrained sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART 2: 2D GAUSSIAN PARAMETER INFERENCE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4fe3d",
   "metadata": {},
   "source": [
    "### 2.1 Generate 2D Gaussian Data\n",
    " Update true values dictionary for 2D Gaussian parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.update({\n",
    "    'mu1': 1.0, 'mu2': -0.5,\n",
    "    'sigma1': 1.2, 'sigma2': 0.8, \n",
    "    'rho': 0.6\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa68a5",
   "metadata": {},
   "source": [
    "Construct covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee047a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mu = jnp.array([true['mu1'], true['mu2']])\n",
    "true_cov = jnp.array([\n",
    "    [true['sigma1']**2, true['rho'] * true['sigma1'] * true['sigma2']],\n",
    "    [true['rho'] * true['sigma1'] * true['sigma2'], true['sigma2']**2]\n",
    "])\n",
    "\n",
    "num_samples = 200\n",
    "key, rng_key = jax.random.split(rng_key)\n",
    "gaussian_data = jax.random.multivariate_normal(key, true_mu, true_cov, (num_samples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fe53c",
   "metadata": {},
   "source": [
    "Visualize the 2D data"
   ]
  },
  {
   "cell_type": "code",
   "id": "b45520f0",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(gaussian_data[:, 0], gaussian_data[:, 1], alpha=0.6, s=20)\nax.set_xlabel(r\"$x_1$\")\nax.set_ylabel(r\"$x_2$\")\nax.set_title(\"2D Gaussian Data\")\nax.grid(True, alpha=0.3)\nplt.show()  # Show plot in notebook\nplt.savefig(\"gaussian_data.png\", dpi=150, bbox_inches='tight')\nprint(\"Saved plot: gaussian_data.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "14a02bac",
   "metadata": {},
   "source": [
    "### 2.2 Define Likelihood with Parameter Transforms\n",
    "\n",
    " We use arctanh/tanh transform for the correlation coefficient to enforce |ρ| < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_2d_loglikelihood(params):\n",
    "    \"\"\"Log-likelihood for 2D Gaussian with correlation.\"\"\"\n",
    "    mu1, mu2 = params[\"mu1\"], params[\"mu2\"]\n",
    "    sigma1, sigma2 = params[\"sigma1\"], params[\"sigma2\"]\n",
    "    rho_transformed = params[\"rho_t\"]\n",
    "    \n",
    "    # Transform correlation coefficient: rho = tanh(rho_t)\n",
    "    rho = jnp.tanh(rho_transformed)\n",
    "    \n",
    "    # Construct covariance matrix\n",
    "    cov = jnp.array([\n",
    "        [sigma1**2, rho * sigma1 * sigma2],\n",
    "        [rho * sigma1 * sigma2, sigma2**2]\n",
    "    ])\n",
    "    \n",
    "    # Check positive definiteness\n",
    "    det = jnp.linalg.det(cov)\n",
    "    \n",
    "    # Return -inf for invalid covariance matrices\n",
    "    def valid_loglik():\n",
    "        mu = jnp.array([mu1, mu2])\n",
    "        return jnp.sum(jax.scipy.stats.multivariate_normal.logpdf(gaussian_data, mu, cov))\n",
    "    \n",
    "    def invalid_loglik():\n",
    "        return -jnp.inf\n",
    "    \n",
    "    return jax.lax.cond(det > 1e-8, valid_loglik, invalid_loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e045dd",
   "metadata": {},
   "source": [
    "### 2.3 Set Up Priors for 2D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581162ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_prior_bounds = {\n",
    "    \"mu1\": (-3.0, 5.0),\n",
    "    \"mu2\": (-3.0, 3.0), \n",
    "    \"sigma1\": (0.1, 3.0),\n",
    "    \"sigma2\": (0.1, 3.0),\n",
    "    \"rho_t\": (-2.0, 2.0),  # transformed correlation: rho = tanh(rho_t)\n",
    "}\n",
    "\n",
    "num_dims_2d = len(gaussian_prior_bounds)\n",
    "num_inner_steps_2d = num_dims_2d * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84952c",
   "metadata": {},
   "source": [
    "Initialize and run nested sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, prior_key = jax.random.split(rng_key)\n",
    "particles_2d, logprior_fn_2d = blackjax.ns.utils.uniform_prior(prior_key, num_live, gaussian_prior_bounds)\n",
    "\n",
    "nested_sampler_2d = blackjax.nss(\n",
    "    logprior_fn=logprior_fn_2d,\n",
    "    loglikelihood_fn=gaussian_2d_loglikelihood,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=num_inner_steps_2d,\n",
    ")\n",
    "\n",
    "init_fn_2d = jax.jit(nested_sampler_2d.init)\n",
    "step_fn_2d = jax.jit(nested_sampler_2d.step)\n",
    "\n",
    "print(\"Running nested sampling for 2D Gaussian...\")\n",
    "live_2d = init_fn_2d(particles_2d)\n",
    "dead_2d = []\n",
    "\n",
    "with tqdm.tqdm(desc=\"Dead points\", unit=\" dead points\") as pbar:\n",
    "    while not live_2d.logZ_live - live_2d.logZ < -3:\n",
    "        rng_key, subkey = jax.random.split(rng_key, 2)\n",
    "        live_2d, dead_info_2d = step_fn_2d(subkey, live_2d)\n",
    "        dead_2d.append(dead_info_2d)\n",
    "        pbar.update(num_delete)\n",
    "\n",
    "dead_2d = blackjax.ns.utils.finalise(live_2d, dead_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9946e440",
   "metadata": {},
   "source": [
    "### 2.4 Transform Back and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25abfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2d = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho_t\"]\n",
    "labels_2d = [r\"$\\mu_1$\", r\"$\\mu_2$\", r\"$\\sigma_1$\", r\"$\\sigma_2$\", r\"$\\rho_t$\"]\n",
    "data_2d = jnp.vstack([dead_2d.particles[key] for key in columns_2d]).T\n",
    "\n",
    "gaussian_samples = NestedSamples(\n",
    "    data_2d,\n",
    "    logL=dead_2d.loglikelihood,\n",
    "    logL_birth=dead_2d.loglikelihood_birth,\n",
    "    columns=columns_2d,\n",
    "    labels=labels_2d,\n",
    "    logzero=jnp.nan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714732f",
   "metadata": {},
   "source": [
    "Add transformed correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_samples[\"rho\"] = jnp.tanh(gaussian_samples[\"rho_t\"].values)\n",
    "\n",
    "print(f\"Log Evidence: {gaussian_samples.logZ():.2f} ± {gaussian_samples.logZ(100).std():.2f}\")\n",
    "print(f\"True parameters: μ₁={true['mu1']:.2f}, μ₂={true['mu2']:.2f}, σ₁={true['sigma1']:.2f}, σ₂={true['sigma2']:.2f}, ρ={true['rho']:.2f}\")\n",
    "print(f\"Posterior means: μ₁={gaussian_samples.mu1.mean():.2f}, μ₂={gaussian_samples.mu2.mean():.2f}, σ₁={gaussian_samples.sigma1.mean():.2f}, σ₂={gaussian_samples.sigma2.mean():.2f}, ρ={gaussian_samples.rho.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69dc87",
   "metadata": {},
   "source": [
    "Plot posterior for key parameters with true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bb29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_params = [\"mu1\", \"mu2\", \"sigma1\", \"sigma2\", \"rho\"]\n",
    "axes = gaussian_samples[key_params].plot_2d(key_params, kinds={'diagonal': 'hist_1d', 'lower': 'kde_2d'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd57c00",
   "metadata": {},
   "source": [
    "Mark true values using anesthetic's axlines method\n",
    " Extract only the 2D Gaussian parameters from the true dict"
   ]
  },
  {
   "cell_type": "code",
   "id": "808a781b",
   "metadata": {},
   "outputs": [],
   "source": "true_2d = {k: true[k] for k in key_params}\naxes.axlines(true_2d, color='red', linestyle='--', alpha=0.8)\n\nplt.suptitle(\"2D Gaussian: Posterior Parameter Estimates\")\nplt.show()  # Show plot in notebook\nplt.savefig(\"gaussian_posterior.png\", dpi=150, bbox_inches='tight')\nprint(\"Saved plot: gaussian_posterior.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "5ff946a0",
   "metadata": {},
   "source": [
    "## Part 3: Performance Comparison\n",
    "\n",
    " Compare BlackJAX nested sampling with NUTS (No-U-Turn Sampler) and \n",
    " Affine Invariant Ensemble Sampler on the line fitting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e18edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART 3: PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ba777",
   "metadata": {},
   "source": [
    "### 3.1 NUTS (Hamiltonian Monte Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af684fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running NUTS sampler...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8dded3",
   "metadata": {},
   "source": [
    "NUTS requires log probability function (log prior + log likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuts_logprob(params_array):\n",
    "    \"\"\"Combined log probability for NUTS (assumes flat priors within bounds).\"\"\"\n",
    "    m, c, log_sigma = params_array\n",
    "    sigma = jnp.exp(log_sigma)  # positive constraint via log transform\n",
    "    \n",
    "    # Check bounds (flat prior)\n",
    "    m_valid = (m >= -5.0) & (m <= 5.0)\n",
    "    c_valid = (c >= -5.0) & (c <= 5.0)\n",
    "    sigma_valid = (sigma >= 0.1) & (sigma <= 2.0)\n",
    "    \n",
    "    def valid_logprob():\n",
    "        y_model = m * x + c\n",
    "        loglik = jax.scipy.stats.multivariate_normal.logpdf(y, y_model, sigma**2)\n",
    "        # Add Jacobian for log transform: log|d(exp(log_sigma))/d(log_sigma)| = log_sigma\n",
    "        return loglik + log_sigma\n",
    "    \n",
    "    def invalid_logprob():\n",
    "        return -jnp.inf\n",
    "    \n",
    "    return jax.lax.cond(\n",
    "        m_valid & c_valid & sigma_valid,\n",
    "        valid_logprob,\n",
    "        invalid_logprob\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bf45e",
   "metadata": {},
   "source": [
    "Initialize NUTS with step size and mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_position = jnp.array([1.0, 0.0, jnp.log(1.0)])  # [m, c, log_sigma]\n",
    "step_size = 0.1\n",
    "inverse_mass_matrix = jnp.eye(3)\n",
    "\n",
    "nuts = blackjax.nuts(nuts_logprob, step_size, inverse_mass_matrix)\n",
    "\n",
    "rng_key, nuts_key = jax.random.split(rng_key)\n",
    "nuts_state = nuts.init(initial_position)\n",
    "nuts_step = jax.jit(nuts.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766860f0",
   "metadata": {},
   "source": [
    "Run NUTS sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea402c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nuts_samples = 2000\n",
    "nuts_start = time.time()\n",
    "nuts_samples = []\n",
    "nuts_states = nuts_state\n",
    "\n",
    "for i in tqdm.tqdm(range(num_nuts_samples), desc=\"NUTS\"):\n",
    "    nuts_key, step_key = jax.random.split(nuts_key)\n",
    "    nuts_states, nuts_info = nuts_step(step_key, nuts_states)\n",
    "    nuts_samples.append(nuts_states.position)\n",
    "\n",
    "nuts_time = time.time() - nuts_start\n",
    "nuts_samples = jnp.stack(nuts_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0280023",
   "metadata": {},
   "source": [
    "Transform NUTS samples back to original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_m = nuts_samples[:, 0]\n",
    "nuts_c = nuts_samples[:, 1] \n",
    "nuts_sigma = jnp.exp(nuts_samples[:, 2])\n",
    "\n",
    "print(f\"NUTS runtime: {nuts_time:.2f} seconds\")\n",
    "print(f\"NUTS means: m={nuts_m[500:].mean():.2f}, c={nuts_c[500:].mean():.2f}, σ={nuts_sigma[500:].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f9a10",
   "metadata": {},
   "source": [
    "### 3.2 Performance Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60000491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "methods = [\"Nested Sampling\", \"NUTS\"]\n",
    "times = [f\"{ns_time:.1f} sec\", f\"{nuts_time:.1f} sec\"]\n",
    "evidence = [\"✓ (Log Z available)\", \"✗ (Not computed)\"]\n",
    "parallelization = [\"✓ (GPU native)\", \"Limited\"]\n",
    "\n",
    "print(f\"{'Method':<20} {'Time':<15} {'Evidence':<15} {'GPU Parallel'}\")\n",
    "print(\"-\" * 65)\n",
    "for i in range(len(methods)):\n",
    "    print(f\"{methods[i]:<20} {times[i]:<15} {evidence[i]:<15} {parallelization[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be3176",
   "metadata": {},
   "source": [
    "### 3.3 Posterior Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a1d10",
   "metadata": {},
   "source": [
    "Generate proper posterior samples from NestedSamples (not raw dead points)\n",
    " Use the number of available samples or 1000, whichever is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_posterior_samples = min(1000, len(line_samples))\n",
    "ns_posterior_samples = line_samples.sample(n_posterior_samples, replace=True)  # Sample from posterior with replacement\n",
    "nuts_burnin = 500  # Remove burn-in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17570d",
   "metadata": {},
   "source": [
    "Compare marginal posteriors"
   ]
  },
  {
   "cell_type": "code",
   "id": "392bac47",
   "metadata": {},
   "outputs": [],
   "source": "axes[0].hist(ns_posterior_samples.m.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\naxes[0].hist(nuts_m[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\naxes[0].axvline(true['m'], color='red', linestyle='--', label='True value')\naxes[0].set_xlabel('Slope (m)')\naxes[0].set_ylabel('Density')\naxes[0].legend()\n\naxes[1].hist(ns_posterior_samples.c.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\naxes[1].hist(nuts_c[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\naxes[1].axvline(true['c'], color='red', linestyle='--', label='True value')\naxes[1].set_xlabel('Intercept (c)')\naxes[1].set_ylabel('Density')\naxes[1].legend()\n\naxes[2].hist(ns_posterior_samples.sigma.values, bins=30, alpha=0.7, density=True, label='Nested Sampling')\naxes[2].hist(nuts_sigma[nuts_burnin:], bins=30, alpha=0.7, density=True, label='NUTS')\naxes[2].axvline(true['sigma'], color='red', linestyle='--', label='True value')\naxes[2].set_xlabel('Noise (σ)')\naxes[2].set_ylabel('Density')\naxes[2].legend()\n\nplt.tight_layout()\nplt.suptitle(\"Posterior Comparison: Nested Sampling vs NUTS\", y=1.02)\nplt.show()  # Show plot in notebook\nplt.savefig(\"sampler_comparison.png\", dpi=150, bbox_inches='tight')\nprint(\"Saved plot: sampler_comparison.png\")"
  },
  {
   "cell_type": "markdown",
   "id": "b1a68f67",
   "metadata": {},
   "source": [
    "## Workshop Conclusions\n",
    "\n",
    " Key takeaways from this BlackJAX nested sampling workshop:\n",
    "\n",
    " 1. **GPU-Native Performance**: BlackJAX leverages JAX for automatic differentiation and JIT compilation\n",
    " 2. **Evidence Computation**: Nested sampling naturally provides Bayesian evidence (model comparison)\n",
    " 3. **Parameter Transforms**: Proper handling of constrained parameters (e.g., correlation coefficients)\n",
    " 4. **Anesthetic Integration**: Professional post-processing and visualization of nested sampling results\n",
    " 5. **Sampler Comparison**: Different samplers have different strengths for different problems\n",
    "\n",
    " **Next Steps**: Try BlackJAX nested sampling on your own problems! Consider:\n",
    " - Adjusting `num_live` for accuracy vs. speed trade-offs\n",
    " - Using more complex prior distributions  \n",
    " - Comparing with other BlackJAX samplers (HMC, NUTS, etc.)\n",
    " - Exploring nested sampling for model selection problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORKSHOP COMPLETE!\")\n",
    "print(\"Try BlackJAX nested sampling on your own problems!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
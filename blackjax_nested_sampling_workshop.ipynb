{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/handley-lab/workshop-blackjax-nested-sampling/blob/main/blackjax_nested_sampling_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlackJAX Nested Sampling Workshop\n",
    "\n",
    "## GPU-Native Nested Sampling for Modern SBI\n",
    "\n",
    "Welcome to this hands-on workshop on nested sampling with BlackJAX! We'll explore how to leverage GPU-native nested sampling for simulation-based inference.\n",
    "\n",
    "**Workshop Overview:**\n",
    "- Why nested sampling for SBI?\n",
    "- BlackJAX nested sampling basics\n",
    "- Professional visualization with Anesthetic\n",
    "- Performance comparison: nested sampling vs. ensemble samplers\n",
    "\n",
    "**Duration:** ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's install the required packages for this workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install blackjax[all] anesthetic emcee corner\n",
    "\n",
    "# Standard imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "# Configure JAX for better performance\n",
    "jax.config.update(\"jax_enable_x64\", True)  # Higher precision\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Nested Sampling for SBI?\n",
    "\n",
    "**Key Insight:** Almost all SBI methods (NLE, NRE, NJE) except NPE require a **sampler** to draw from posterior distributions.\n",
    "\n",
    "**The Problem:** Traditional samplers are either:\n",
    "- Legacy Fortran codes (MultiNest, PolyChord) - hard to modify/extend\n",
    "- Slow Python implementations (dynesty, ultranest, nautilus)\n",
    "\n",
    "**BlackJAX Solution:**\n",
    "- GPU-native implementation\n",
    "- Open source & community-driven\n",
    "- JAX benefits: autodiff + JIT compilation\n",
    "- Seamless integration with modern ML workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example 1: Linear Regression with Nested Sampling\n",
    "\n",
    "Let's start with a simple but illustrative example: fitting a straight line to noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "key = random.PRNGKey(42)\n",
    "n_data = 50\n",
    "\n",
    "# True parameters\n",
    "true_slope = 2.5\n",
    "true_intercept = 1.0\n",
    "true_sigma = 0.3\n",
    "\n",
    "# Generate data\n",
    "x = jnp.linspace(0, 5, n_data)\n",
    "key, subkey = random.split(key)\n",
    "y_true = true_slope * x + true_intercept\n",
    "y = y_true + true_sigma * random.normal(subkey, (n_data,))\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, alpha=0.7, label='Data')\n",
    "plt.plot(x, y_true, 'r--', label=f'True: y = {true_slope}x + {true_intercept}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Linear Regression Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Bayesian Model\n",
    "\n",
    "We need to define:\n",
    "1. **Likelihood function**: How well our model explains the data\n",
    "2. **Prior distributions**: Our beliefs about parameter ranges\n",
    "3. **Log probability**: Combined likelihood + prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjax\n",
    "from jax.scipy import stats\n",
    "\n",
    "def log_likelihood(params, x_data, y_data):\n",
    "    \"\"\"Log likelihood for linear regression.\"\"\"\n",
    "    slope, intercept, log_sigma = params\n",
    "    sigma = jnp.exp(log_sigma)  # Ensure sigma > 0\n",
    "    \n",
    "    # Model prediction\n",
    "    y_pred = slope * x_data + intercept\n",
    "    \n",
    "    # Gaussian likelihood\n",
    "    return jnp.sum(stats.norm.logpdf(y_data, y_pred, sigma))\n",
    "\n",
    "def log_prior(params):\n",
    "    \"\"\"Log prior for parameters.\"\"\"\n",
    "    slope, intercept, log_sigma = params\n",
    "    \n",
    "    # Priors: slope ~ N(0, 10), intercept ~ N(0, 10), log_sigma ~ N(0, 1)\n",
    "    return (stats.norm.logpdf(slope, 0, 10) + \n",
    "            stats.norm.logpdf(intercept, 0, 10) + \n",
    "            stats.norm.logpdf(log_sigma, 0, 1))\n",
    "\n",
    "def log_probability(params):\n",
    "    \"\"\"Log posterior probability.\"\"\"\n",
    "    lp = log_prior(params)\n",
    "    if not jnp.isfinite(lp):\n",
    "        return -jnp.inf\n",
    "    return lp + log_likelihood(params, x, y)\n",
    "\n",
    "# Test the log probability function\n",
    "test_params = jnp.array([2.0, 1.0, jnp.log(0.5)])\n",
    "print(f\"Test log probability: {log_probability(test_params):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run BlackJAX Nested Sampling\n",
    "\n",
    "Now let's use BlackJAX to sample from our posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior sampling function for nested sampling\n",
    "def prior_sampler(key, n_samples):\n",
    "    \"\"\"Sample from the prior distribution.\"\"\"\n",
    "    keys = random.split(key, 3)\n",
    "    slope = random.normal(keys[0], (n_samples,)) * 10.0\n",
    "    intercept = random.normal(keys[1], (n_samples,)) * 10.0 \n",
    "    log_sigma = random.normal(keys[2], (n_samples,))\n",
    "    return jnp.column_stack([slope, intercept, log_sigma])\n",
    "\n",
    "# Configure nested sampling\n",
    "n_live = 500  # Number of live points\n",
    "n_dim = 3     # Number of parameters\n",
    "\n",
    "# Initialize nested sampling algorithm\n",
    "ns_algorithm = blackjax.nested_sampling(\n",
    "    log_probability,\n",
    "    prior_sampler,\n",
    "    n_live_points=n_live\n",
    ")\n",
    "\n",
    "# Initialize state\n",
    "key, subkey = random.split(key)\n",
    "initial_state = ns_algorithm.init(subkey)\n",
    "\n",
    "print(\"Starting nested sampling...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Run nested sampling\n",
    "key, subkey = random.split(key)\n",
    "samples, log_evidence = blackjax.nested_sampling_inference(\n",
    "    ns_algorithm,\n",
    "    initial_state,\n",
    "    subkey,\n",
    "    max_samples=5000\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Nested sampling completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Log evidence: {log_evidence:.3f}\")\n",
    "print(f\"Number of samples: {samples.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results\n",
    "\n",
    "Let's examine our posterior samples and compare with the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameter samples\n",
    "slope_samples = samples[:, 0]\n",
    "intercept_samples = samples[:, 1]\n",
    "log_sigma_samples = samples[:, 2]\n",
    "sigma_samples = jnp.exp(log_sigma_samples)\n",
    "\n",
    "# Calculate summary statistics\n",
    "def summarize_parameter(samples, true_value, name):\n",
    "    mean = jnp.mean(samples)\n",
    "    std = jnp.std(samples)\n",
    "    q16, q84 = jnp.percentile(samples, [16, 84])\n",
    "    print(f\"{name:12s}: {mean:.3f} ¬± {std:.3f} [{q16:.3f}, {q84:.3f}] (true: {true_value:.3f})\")\n",
    "    return mean, std\n",
    "\n",
    "print(\"\\nPosterior Summary:\")\n",
    "print(\"=\" * 50)\n",
    "slope_mean, slope_std = summarize_parameter(slope_samples, true_slope, \"Slope\")\n",
    "intercept_mean, intercept_std = summarize_parameter(intercept_samples, true_intercept, \"Intercept\") \n",
    "sigma_mean, sigma_std = summarize_parameter(sigma_samples, true_sigma, \"Sigma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Professional Visualization with Anesthetic\n",
    "\n",
    "Anesthetic is a powerful library designed specifically for visualizing nested sampling results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anesthetic\n",
    "\n",
    "# Create anesthetic samples object\n",
    "parameter_names = ['slope', 'intercept', 'log_sigma']\n",
    "anesthetic_samples = anesthetic.NestedSamples(\n",
    "    data=np.column_stack([slope_samples, intercept_samples, log_sigma_samples]),\n",
    "    columns=parameter_names\n",
    ")\n",
    "\n",
    "# Create corner plot\n",
    "fig, axes = anesthetic_samples.plot_2d(\n",
    "    parameter_names,\n",
    "    figsize=(10, 10)\n",
    ")\n",
    "\n",
    "# Add true values\n",
    "true_values = [true_slope, true_intercept, jnp.log(true_sigma)]\n",
    "for i, ax in enumerate(axes.diagonal()):\n",
    "    ax.axvline(true_values[i], color='red', linestyle='--', label='True value')\n",
    "\n",
    "for i in range(len(parameter_names)):\n",
    "    for j in range(i):\n",
    "        axes[i, j].scatter(true_values[j], true_values[i], \n",
    "                          color='red', marker='x', s=100, label='True value')\n",
    "\n",
    "plt.suptitle('Posterior Distribution - Linear Regression', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks\n",
    "\n",
    "Let's visualize how well our model fits the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples\n",
    "n_pred_samples = 100\n",
    "x_test = jnp.linspace(0, 5, 100)\n",
    "\n",
    "# Sample random posterior draws\n",
    "key, subkey = random.split(key)\n",
    "idx = random.choice(subkey, len(slope_samples), (n_pred_samples,))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot posterior predictive lines\n",
    "for i in range(min(50, n_pred_samples)):  # Plot subset for clarity\n",
    "    y_pred = slope_samples[idx[i]] * x_test + intercept_samples[idx[i]]\n",
    "    plt.plot(x_test, y_pred, 'b-', alpha=0.1)\n",
    "\n",
    "# Plot data and true line\n",
    "plt.scatter(x, y, alpha=0.7, color='black', label='Data', zorder=5)\n",
    "plt.plot(x_test, true_slope * x_test + true_intercept, 'r--', \n",
    "         linewidth=2, label='True model', zorder=4)\n",
    "\n",
    "# Plot posterior mean\n",
    "y_mean = slope_mean * x_test + intercept_mean\n",
    "plt.plot(x_test, y_mean, 'g-', linewidth=2, label='Posterior mean', zorder=3)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Posterior Predictive Check')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example 2: Multimodal Distribution\n",
    "\n",
    "Nested sampling excels at multimodal distributions. Let's demonstrate with a mixture of Gaussians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_mixture(params):\n",
    "    \"\"\"Log probability of a mixture of two Gaussians.\"\"\"\n",
    "    x, y = params\n",
    "    \n",
    "    # Two Gaussian components\n",
    "    comp1 = stats.multivariate_normal.logpdf(\n",
    "        jnp.array([x, y]), \n",
    "        jnp.array([-2.0, -2.0]), \n",
    "        jnp.eye(2) * 0.5\n",
    "    )\n",
    "    comp2 = stats.multivariate_normal.logpdf(\n",
    "        jnp.array([x, y]), \n",
    "        jnp.array([2.0, 2.0]), \n",
    "        jnp.eye(2) * 0.3\n",
    "    )\n",
    "    \n",
    "    # Log sum of exponentials (mixture)\n",
    "    return jnp.logaddexp(comp1 + jnp.log(0.6), comp2 + jnp.log(0.4))\n",
    "\n",
    "def mixture_prior_sampler(key, n_samples):\n",
    "    \"\"\"Sample from uniform prior on [-5, 5] x [-5, 5].\"\"\"\n",
    "    return random.uniform(key, (n_samples, 2), minval=-5.0, maxval=5.0)\n",
    "\n",
    "# Run nested sampling on mixture\n",
    "ns_mixture = blackjax.nested_sampling(\n",
    "    log_prob_mixture,\n",
    "    mixture_prior_sampler,\n",
    "    n_live_points=1000\n",
    ")\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "initial_state_mixture = ns_mixture.init(subkey)\n",
    "\n",
    "print(\"Running nested sampling on multimodal distribution...\")\n",
    "start_time = time.time()\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "mixture_samples, mixture_log_evidence = blackjax.nested_sampling_inference(\n",
    "    ns_mixture,\n",
    "    initial_state_mixture, \n",
    "    subkey,\n",
    "    max_samples=8000\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Log evidence: {mixture_log_evidence:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the multimodal posterior\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Scatter plot of samples\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(mixture_samples[:, 0], mixture_samples[:, 1], \n",
    "           alpha=0.6, s=10, c='blue')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Nested Sampling: Multimodal Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Contour plot\n",
    "plt.subplot(1, 2, 2)\n",
    "x_grid = jnp.linspace(-5, 5, 50)\n",
    "y_grid = jnp.linspace(-5, 5, 50)\n",
    "X, Y = jnp.meshgrid(x_grid, y_grid)\n",
    "Z = jnp.zeros_like(X)\n",
    "\n",
    "for i in range(len(x_grid)):\n",
    "    for j in range(len(y_grid)):\n",
    "        Z = Z.at[j, i].set(jnp.exp(log_prob_mixture([X[j, i], Y[j, i]])))\n",
    "\n",
    "plt.contour(X, Y, Z, levels=10, colors='red', alpha=0.7)\n",
    "plt.scatter(mixture_samples[:, 0], mixture_samples[:, 1], \n",
    "           alpha=0.4, s=5, c='blue')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('True Distribution (contours) vs Samples')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison: Nested Sampling vs. Ensemble Sampling\n",
    "\n",
    "Let's compare BlackJAX nested sampling with a traditional ensemble sampler (emcee-style AIES):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's run emcee on our linear regression problem\n",
    "import emcee\n",
    "\n",
    "def log_prob_emcee(params):\n",
    "    \"\"\"Log probability function for emcee (numpy version).\"\"\"\n",
    "    return float(log_probability(jnp.array(params)))\n",
    "\n",
    "# Set up emcee sampler\n",
    "n_walkers = 32\n",
    "n_dim = 3\n",
    "n_steps = 2000\n",
    "n_burn = 500\n",
    "\n",
    "# Initialize walkers near the maximum likelihood estimate\n",
    "initial_guess = np.array([2.0, 1.0, np.log(0.5)])\n",
    "pos = initial_guess + 0.1 * np.random.randn(n_walkers, n_dim)\n",
    "\n",
    "# Run emcee\n",
    "print(\"Running emcee (AIES)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_dim, log_prob_emcee)\n",
    "sampler.run_mcmc(pos, n_steps, progress=True)\n",
    "\n",
    "emcee_time = time.time() - start_time\n",
    "print(f\"Emcee completed in {emcee_time:.2f} seconds\")\n",
    "\n",
    "# Extract samples (after burn-in)\n",
    "emcee_samples = sampler.get_chain(discard=n_burn, flat=True)\n",
    "print(f\"Emcee effective samples: {len(emcee_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compare the results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "param_names = ['Slope', 'Intercept', 'log(œÉ)']\n",
    "true_vals = [true_slope, true_intercept, jnp.log(true_sigma)]\n",
    "colors = ['blue', 'orange']\n",
    "labels = ['BlackJAX NS', 'emcee (AIES)']\n",
    "\n",
    "# Plot marginal distributions\n",
    "for i in range(3):\n",
    "    # Histograms\n",
    "    axes[0, i].hist(samples[:, i], bins=50, alpha=0.7, \n",
    "                   density=True, color=colors[0], label=labels[0])\n",
    "    axes[0, i].hist(emcee_samples[:, i], bins=50, alpha=0.7, \n",
    "                   density=True, color=colors[1], label=labels[1])\n",
    "    axes[0, i].axvline(true_vals[i], color='red', linestyle='--', \n",
    "                      label='True value')\n",
    "    axes[0, i].set_xlabel(param_names[i])\n",
    "    axes[0, i].set_ylabel('Density')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot traces (show convergence)\n",
    "chain = sampler.get_chain()\n",
    "for i in range(3):\n",
    "    # Show a few walker traces for emcee\n",
    "    for j in range(min(5, n_walkers)):\n",
    "        axes[1, i].plot(chain[:, j, i], color=colors[1], alpha=0.3)\n",
    "    axes[1, i].axhline(true_vals[i], color='red', linestyle='--')\n",
    "    axes[1, i].set_xlabel('Step')\n",
    "    axes[1, i].set_ylabel(param_names[i])\n",
    "    axes[1, i].set_title('emcee Chains')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison summary\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"BlackJAX NS time: {end_time - start_time:.2f}s\")\n",
    "print(f\"emcee time:        {emcee_time:.2f}s\")\n",
    "print(f\"BlackJAX samples:  {len(samples)}\")\n",
    "print(f\"emcee samples:     {len(emcee_samples)}\")\n",
    "print(f\"\\nBlackJAX also provides log evidence: {log_evidence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Your Turn: Experiment!\n",
    "\n",
    "Now it's time to experiment with BlackJAX nested sampling. Here are some suggestions:\n",
    "\n",
    "### Option 1: Modify the Linear Model\n",
    "Try adding more complexity to our linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try a polynomial model\n",
    "# y = a*x^2 + b*x + c + noise\n",
    "\n",
    "# Your code here!\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Bring Your Own Model\n",
    "If you have JAX code from Viraj's workshop, try applying nested sampling to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply nested sampling to your own problem\n",
    "# 1. Define your log_probability function\n",
    "# 2. Define your prior_sampler function  \n",
    "# 3. Run nested sampling\n",
    "# 4. Visualize with anesthetic\n",
    "\n",
    "# Your code here!\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Advanced Features\n",
    "Explore more BlackJAX nested sampling features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different nested sampling variants\n",
    "# - Slice sampling within nested sampling\n",
    "# - Different proposal mechanisms\n",
    "# - Adaptive live point allocation\n",
    "\n",
    "# Example: Using slice sampling for proposals\n",
    "# ns_algorithm = blackjax.nested_sampling(\n",
    "#     log_probability,\n",
    "#     prior_sampler,\n",
    "#     n_live_points=n_live,\n",
    "#     mcmc=blackjax.slice\n",
    "# )\n",
    "\n",
    "# Your experiments here!\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "üéØ **What we've learned:**\n",
    "\n",
    "1. **Nested sampling is essential** for most SBI methods (except NPE)\n",
    "2. **BlackJAX provides GPU-native** nested sampling with JAX benefits\n",
    "3. **Anesthetic makes visualization easy** and professional\n",
    "4. **Performance gains** come from JIT compilation and GPU acceleration\n",
    "5. **Model evidence** is a bonus feature for model comparison\n",
    "\n",
    "üöÄ **Next steps:**\n",
    "- Try BlackJAX nested sampling on your own problems\n",
    "- Explore the [BlackJAX documentation](https://github.com/handley-lab/blackjax)\n",
    "- Use [Anesthetic](https://anesthetic.readthedocs.io) for publication-quality plots\n",
    "- Join the community discussion on GitHub\n",
    "\n",
    "üí° **Remember:** The future of scientific computing is GPU-native. BlackJAX positions you at the forefront of modern Bayesian inference!\n",
    "\n",
    "---\n",
    "\n",
    "**Questions? Suggestions? Let's discuss!** üó£Ô∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
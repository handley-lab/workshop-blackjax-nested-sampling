{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workshop_title"
   },
   "source": [
    "# BlackJAX Nested Sampling Workshop\n",
    "## GPU-Native Bayesian Inference for SBI\n",
    "\n",
    "**SBI Galev 2025 Workshop**  \n",
    "*Will Handley, University of Cambridge*\n",
    "\n",
    "---\n",
    "\n",
    "### Workshop Overview\n",
    "\n",
    "In this hands-on workshop, you'll learn to use **BlackJAX nested sampling** - a GPU-native implementation that leverages JAX's autodiff and JIT compilation for modern Bayesian inference.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand when nested sampling excels over MCMC methods\n",
    "2. Implement nested sampling with BlackJAX\n",
    "3. Visualize results with Anesthetic\n",
    "4. Compare performance: nested sampling vs. affine invariant ensemble sampling\n",
    "5. Leverage GPU acceleration for scientific inference\n",
    "\n",
    "**Why BlackJAX Nested Sampling?**\n",
    "- **GPU-native**: Full JAX integration (autodiff + JIT compilation)\n",
    "- **Open source**: Community-owned alternative to legacy Fortran tools\n",
    "- **Modern**: Designed for SBI workflows and scientific computing\n",
    "- **Efficient**: Handles multimodal posteriors and evidence computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "We'll install BlackJAX from the nested sampling branch and set up our environment for GPU-accelerated inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install BlackJAX nested sampling branch and visualization tools\n",
    "!pip install git+https://github.com/handley-lab/blackjax@nested_sampling\n",
    "!pip install anesthetic matplotlib corner tqdm\n",
    "\n",
    "# Check for GPU availability\n",
    "import jax\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX backend: {jax.lib.xla_bridge.get_backend().platform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# BlackJAX imports\n",
    "import blackjax\n",
    "import blackjax.ns.utils as ns_utils\n",
    "\n",
    "# Visualization\n",
    "from anesthetic import NestedSamples\n",
    "import corner\n",
    "\n",
    "# JAX configuration for precision and reproducibility\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update('jax_platform_name', 'cpu')  # Change to 'gpu' if available\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "rng_key = jax.random.key(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem_setup"
   },
   "source": [
    "## 2. Problem Setup: 2D Gaussian Parameter Inference\n",
    "\n",
    "We'll revisit the **2D Gaussian inference problem** from Viraj's JAX/SciML workshop, but this time tackle it with nested sampling.\n",
    "\n",
    "**Problem**: Infer parameters of a 2D Gaussian from noisy pixelated observations\n",
    "- **Parameters**: [μₓ, μᵧ, σₓ, σᵧ, ρₓᵧ] (5D parameter space)\n",
    "- **Data**: 50×50 pixel noisy images of 2D Gaussians\n",
    "- **Challenge**: Multimodal posterior due to parameter symmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simulator_function"
   },
   "outputs": [],
   "source": [
    "# Problem configuration\n",
    "image_size = 50\n",
    "x = jnp.linspace(-3, 3, image_size)\n",
    "y = jnp.linspace(-3, 3, image_size)\n",
    "X, Y = jnp.meshgrid(x, y)\n",
    "coords = jnp.stack([X.ravel(), Y.ravel()]).T  # (2500, 2)\n",
    "\n",
    "# True parameters for data generation\n",
    "true_params = jnp.array([0.5, -0.3, 1.2, 0.8, 0.4])  # [μₓ, μᵧ, σₓ, σᵧ, ρₓᵧ]\n",
    "\n",
    "@jax.jit\n",
    "def params_to_cov(params):\n",
    "    \"\"\"Convert parameters to mean vector and covariance matrix.\"\"\"\n",
    "    mu_x, mu_y, sigma_x, sigma_y, rho = params\n",
    "    \n",
    "    mean = jnp.array([mu_x, mu_y])\n",
    "    \n",
    "    # Covariance matrix\n",
    "    cov = jnp.array([\n",
    "        [sigma_x**2, rho * sigma_x * sigma_y],\n",
    "        [rho * sigma_x * sigma_y, sigma_y**2]\n",
    "    ])\n",
    "    \n",
    "    return mean, cov\n",
    "\n",
    "@jax.jit\n",
    "def simulator(params, rng_key, noise_sigma=0.1):\n",
    "    \"\"\"Simulate 2D Gaussian observation with noise.\"\"\"\n",
    "    mean, cov = params_to_cov(params)\n",
    "    \n",
    "    # Evaluate multivariate normal PDF on grid\n",
    "    logpdf = jax.scipy.stats.multivariate_normal.logpdf(coords, mean, cov)\n",
    "    image_clean = logpdf.reshape(image_size, image_size)\n",
    "    image_clean = jnp.exp(image_clean - jnp.max(image_clean))  # Normalize\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = jax.random.normal(rng_key, image_clean.shape) * noise_sigma\n",
    "    \n",
    "    return image_clean + noise\n",
    "\n",
    "# Generate observed data\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "observed_data = simulator(true_params, subkey)\n",
    "\n",
    "# Visualize the observed data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(observed_data, origin='lower', extent=[-3, 3, -3, 3], cmap='viridis')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title('Observed 2D Gaussian (with noise)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "\n",
    "print(f\"True parameters: μₓ={true_params[0]:.2f}, μᵧ={true_params[1]:.2f}, \"\n",
    "      f\"σₓ={true_params[2]:.2f}, σᵧ={true_params[3]:.2f}, ρ={true_params[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "likelihood_prior"
   },
   "source": [
    "## 3. Likelihood and Prior Setup\n",
    "\n",
    "Define the likelihood function and prior distributions for Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "likelihood_function"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loglikelihood_fn(params):\n",
    "    \"\"\"Log-likelihood function for parameter inference.\"\"\"\n",
    "    # Simulate clean image from parameters\n",
    "    mean, cov = params_to_cov(params)\n",
    "    \n",
    "    # Check if covariance matrix is positive definite\n",
    "    det_cov = jnp.linalg.det(cov)\n",
    "    \n",
    "    # Return -inf if covariance is not positive definite\n",
    "    def valid_cov():\n",
    "        logpdf = jax.scipy.stats.multivariate_normal.logpdf(coords, mean, cov)\n",
    "        image_pred = logpdf.reshape(image_size, image_size)\n",
    "        image_pred = jnp.exp(image_pred - jnp.max(image_pred))\n",
    "        \n",
    "        # Gaussian likelihood (MSE)\n",
    "        noise_sigma = 0.1\n",
    "        residuals = (observed_data - image_pred) / noise_sigma\n",
    "        return -0.5 * jnp.sum(residuals**2)\n",
    "    \n",
    "    def invalid_cov():\n",
    "        return -jnp.inf\n",
    "    \n",
    "    return jax.lax.cond(det_cov > 0, valid_cov, invalid_cov)\n",
    "\n",
    "# Prior bounds\n",
    "prior_bounds = {\n",
    "    \"mu_x\": (-2.0, 2.0),\n",
    "    \"mu_y\": (-2.0, 2.0), \n",
    "    \"sigma_x\": (0.5, 3.0),\n",
    "    \"sigma_y\": (0.5, 3.0),\n",
    "    \"rho\": (-0.99, 0.99)  # Correlation must be in (-1, 1)\n",
    "}\n",
    "\n",
    "# Test likelihood function\n",
    "test_loglik = loglikelihood_fn(true_params)\n",
    "print(f\"Log-likelihood at true parameters: {test_loglik:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nested_sampling_intro"
   },
   "source": [
    "## 4. BlackJAX Nested Sampling\n",
    "\n",
    "Now let's implement nested sampling with BlackJAX. Nested sampling is particularly powerful for:\n",
    "- **Multimodal posteriors** (common in parameter inference)\n",
    "- **Evidence computation** (model comparison)\n",
    "- **Efficient exploration** of complex parameter spaces\n",
    "\n",
    "### Why Nested Sampling for This Problem?\n",
    "The 2D Gaussian parameter inference has potential **symmetries** and **multimodality**:\n",
    "- Parameter correlations can create complex posterior geometry\n",
    "- Traditional MCMC (like HMC/NUTS) may struggle with mode mixing\n",
    "- Nested sampling naturally handles these challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nested_sampling_setup"
   },
   "outputs": [],
   "source": [
    "# Nested sampling configuration\n",
    "num_live = 1000  # Number of live points (controls precision)\n",
    "num_dims = 5     # Parameter dimensionality\n",
    "num_inner_steps = num_dims * 5  # MCMC steps per NS iteration (3-5 × ndim)\n",
    "num_delete = 50  # Parallelization parameter for GPU efficiency\n",
    "\n",
    "print(f\"Nested sampling configuration:\")\n",
    "print(f\"  Live points: {num_live}\")\n",
    "print(f\"  Inner MCMC steps: {num_inner_steps}\")\n",
    "print(f\"  Parallel deletion: {num_delete}\")\n",
    "\n",
    "# Initialize uniform prior and live points\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "particles, logprior_fn = ns_utils.uniform_prior(\n",
    "    subkey, num_live, prior_bounds\n",
    ")\n",
    "\n",
    "print(f\"\\nInitialized {particles.shape[0]} live points in {particles.shape[1]}D space\")\n",
    "print(f\"Parameter bounds: {prior_bounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nested_sampling_run"
   },
   "outputs": [],
   "source": [
    "# Create nested sampler\n",
    "nested_sampler = blackjax.nss(\n",
    "    logprior_fn=logprior_fn,\n",
    "    loglikelihood_fn=loglikelihood_fn,\n",
    "    num_delete=num_delete,\n",
    "    num_inner_steps=num_inner_steps\n",
    ")\n",
    "\n",
    "# Initialize nested sampling state\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "live_state = nested_sampler.init(particles)\n",
    "\n",
    "# JIT compile the sampling step for efficiency\n",
    "jit_step = jax.jit(nested_sampler.step)\n",
    "\n",
    "print(f\"Initial evidence estimate: {live_state.logZ:.3f}\")\n",
    "print(f\"Initial live evidence: {live_state.logZ_live:.3f}\")\n",
    "print(\"\\nStarting nested sampling...\")\n",
    "\n",
    "# Run nested sampling until convergence\n",
    "dead_points = []\n",
    "iteration = 0\n",
    "convergence_threshold = -3.0  # log(0.05) - stop when evidence contribution < 5%\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while (live_state.logZ_live - live_state.logZ) > convergence_threshold:\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    \n",
    "    # Take nested sampling step\n",
    "    live_state, dead_info = jit_step(subkey, live_state)\n",
    "    dead_points.append(dead_info)\n",
    "    \n",
    "    iteration += 1\n",
    "    \n",
    "    # Progress updates\n",
    "    if iteration % 50 == 0:\n",
    "        remaining_evidence = live_state.logZ_live - live_state.logZ\n",
    "        print(f\"Iteration {iteration:4d}: logZ = {live_state.logZ:.3f}, \"\n",
    "              f\"remaining = {remaining_evidence:.3f}\")\n",
    "\n",
    "sampling_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nNested sampling completed!\")\n",
    "print(f\"Total iterations: {iteration}\")\n",
    "print(f\"Sampling time: {sampling_time:.2f} seconds\")\n",
    "print(f\"Final evidence: logZ = {live_state.logZ:.3f} ± {jnp.sqrt(live_state.H):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_processing"
   },
   "source": [
    "## 5. Results Processing and Visualization\n",
    "\n",
    "Process the nested sampling results and create publication-quality visualizations with Anesthetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process_results"
   },
   "outputs": [],
   "source": [
    "# Combine dead and live points\n",
    "dead = ns_utils.finalise(live_state, dead_points)\n",
    "\n",
    "# Extract samples and metadata\n",
    "samples = dead.particles  # Parameter samples\n",
    "logL = dead.loglikelihood  # Log-likelihood values\n",
    "logL_birth = dead.logL_birth  # Birth likelihood thresholds\n",
    "\n",
    "print(f\"Total samples: {len(samples)}\")\n",
    "print(f\"Evidence: logZ = {live_state.logZ:.3f} ± {jnp.sqrt(live_state.H):.3f}\")\n",
    "print(f\"Information: H = {live_state.H:.3f} nats\")\n",
    "\n",
    "# Create NestedSamples object for anesthetic\n",
    "param_names = ['μₓ', 'μᵧ', 'σₓ', 'σᵧ', 'ρ']\n",
    "nested_samples = NestedSamples(\n",
    "    data=samples,\n",
    "    logL=logL,\n",
    "    logL_birth=logL_birth,\n",
    "    columns=param_names\n",
    ")\n",
    "\n",
    "# Posterior statistics\n",
    "print(\"\\nPosterior summary:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    mean = nested_samples[name].mean()\n",
    "    std = nested_samples[name].std()\n",
    "    true_val = true_params[i]\n",
    "    print(f\"  {name}: {mean:.3f} ± {std:.3f} (true: {true_val:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anesthetic_plots"
   },
   "outputs": [],
   "source": [
    "# Create corner plot with anesthetic\n",
    "fig, axes = plt.subplots(num_dims, num_dims, figsize=(12, 10))\n",
    "\n",
    "# Corner plot with true parameters\n",
    "nested_samples.plot_2d(\n",
    "    axes,\n",
    "    types={'diagonal': 'hist', 'lower': 'contour'},\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add true parameter values\n",
    "for i in range(num_dims):\n",
    "    # Diagonal (1D marginals)\n",
    "    axes[i, i].axvline(true_params[i], color='red', linestyle='--', alpha=0.8, label='True')\n",
    "    \n",
    "    # Off-diagonal (2D marginals)\n",
    "    for j in range(i):\n",
    "        axes[i, j].axvline(true_params[j], color='red', linestyle='--', alpha=0.8)\n",
    "        axes[i, j].axhline(true_params[i], color='red', linestyle='--', alpha=0.8)\n",
    "        axes[i, j].plot(true_params[j], true_params[i], 'r*', markersize=10)\n",
    "\n",
    "# Add legend\n",
    "axes[0, 0].legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('BlackJAX Nested Sampling: 2D Gaussian Parameter Inference', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nested_sampling_diagnostics"
   },
   "outputs": [],
   "source": [
    "# Nested sampling diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Evolution of evidence\n",
    "axes[0, 0].plot(nested_samples.logL, nested_samples.logZ_p)\n",
    "axes[0, 0].set_xlabel('Log-likelihood')\n",
    "axes[0, 0].set_ylabel('Evidence (logZ)')\n",
    "axes[0, 0].set_title('Evidence Evolution')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood sequence\n",
    "axes[0, 1].plot(logL)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Log-likelihood')\n",
    "axes[0, 1].set_title('Likelihood Sequence')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Posterior weights\n",
    "weights = nested_samples.get_weights()\n",
    "axes[1, 0].plot(weights)\n",
    "axes[1, 0].set_xlabel('Sample index')\n",
    "axes[1, 0].set_ylabel('Posterior weight')\n",
    "axes[1, 0].set_title('Posterior Weights')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Effective sample size evolution\n",
    "n_eff = 1.0 / jnp.sum(weights**2)\n",
    "axes[1, 1].text(0.5, 0.7, f'Effective samples: {n_eff:.0f}', \n",
    "               transform=axes[1, 1].transAxes, fontsize=12, ha='center')\n",
    "axes[1, 1].text(0.5, 0.5, f'Total samples: {len(weights)}', \n",
    "               transform=axes[1, 1].transAxes, fontsize=12, ha='center')\n",
    "axes[1, 1].text(0.5, 0.3, f'Efficiency: {n_eff/len(weights):.2%}', \n",
    "               transform=axes[1, 1].transAxes, fontsize=12, ha='center')\n",
    "axes[1, 1].set_title('Sampling Efficiency')\n",
    "axes[1, 1].set_xticks([])\n",
    "axes[1, 1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNested Sampling Diagnostics:\")\n",
    "print(f\"  Effective sample size: {n_eff:.0f} / {len(weights)} ({n_eff/len(weights):.1%})\")\n",
    "print(f\"  Evidence uncertainty: ±{jnp.sqrt(live_state.H):.3f}\")\n",
    "print(f\"  Information gain: {live_state.H:.3f} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison_setup"
   },
   "source": [
    "## 6. Performance Comparison: Nested Sampling vs. MCMC\n",
    "\n",
    "Let's compare BlackJAX nested sampling with traditional MCMC methods (NUTS and AIES) on the same problem.\n",
    "\n",
    "### Why This Comparison Matters:\n",
    "- **NUTS**: State-of-the-art Hamiltonian Monte Carlo (requires gradients)\n",
    "- **AIES**: Affine Invariant Ensemble Sampler (like `emcee`)\n",
    "- **Nested Sampling**: Designed for multimodal posteriors and evidence computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuts_comparison"
   },
   "outputs": [],
   "source": [
    "# NUTS (No-U-Turn Sampler) comparison\n",
    "from blackjax.mcmc.nuts import build_kernel\n",
    "from blackjax.mcmc.nuts import init as nuts_init\n",
    "\n",
    "# Convert to constrained space for NUTS (requires differentiable transforms)\n",
    "def transform_to_unconstrained(params):\n",
    "    \"\"\"Transform to unconstrained space for NUTS.\"\"\"\n",
    "    mu_x, mu_y, sigma_x, sigma_y, rho = params\n",
    "    \n",
    "    # Log transform for positive parameters\n",
    "    log_sigma_x = jnp.log(sigma_x)\n",
    "    log_sigma_y = jnp.log(sigma_y)\n",
    "    \n",
    "    # Logit transform for correlation [-1, 1] -> R\n",
    "    logit_rho = jnp.log((rho + 1) / (2 - rho))\n",
    "    \n",
    "    return jnp.array([mu_x, mu_y, log_sigma_x, log_sigma_y, logit_rho])\n",
    "\n",
    "def transform_to_constrained(unconstrained_params):\n",
    "    \"\"\"Transform back to constrained space.\"\"\"\n",
    "    mu_x, mu_y, log_sigma_x, log_sigma_y, logit_rho = unconstrained_params\n",
    "    \n",
    "    sigma_x = jnp.exp(log_sigma_x)\n",
    "    sigma_y = jnp.exp(log_sigma_y)\n",
    "    rho = 2 * jax.nn.sigmoid(logit_rho) - 1\n",
    "    \n",
    "    return jnp.array([mu_x, mu_y, sigma_x, sigma_y, rho])\n",
    "\n",
    "@jax.jit\n",
    "def unconstrained_logdensity(unconstrained_params):\n",
    "    \"\"\"Log-density in unconstrained space (for NUTS).\"\"\"\n",
    "    constrained_params = transform_to_constrained(unconstrained_params)\n",
    "    \n",
    "    # Check parameter bounds\n",
    "    mu_x, mu_y, sigma_x, sigma_y, rho = constrained_params\n",
    "    \n",
    "    # Prior constraints\n",
    "    if not (-2 <= mu_x <= 2 and -2 <= mu_y <= 2 and \n",
    "            0.5 <= sigma_x <= 3 and 0.5 <= sigma_y <= 3 and\n",
    "            -0.99 <= rho <= 0.99):\n",
    "        return -jnp.inf\n",
    "    \n",
    "    loglik = loglikelihood_fn(constrained_params)\n",
    "    \n",
    "    # Add Jacobian correction for transforms\n",
    "    log_sigma_x, log_sigma_y, logit_rho = unconstrained_params[2], unconstrained_params[3], unconstrained_params[4]\n",
    "    jacobian = log_sigma_x + log_sigma_y + logit_rho - 2 * jnp.log(jnp.cosh(logit_rho))\n",
    "    \n",
    "    return loglik + jacobian\n",
    "\n",
    "print(\"Setting up NUTS sampler...\")\n",
    "\n",
    "# NUTS configuration\n",
    "nuts_kernel = build_kernel(unconstrained_logdensity)\n",
    "\n",
    "# Initialize NUTS from a reasonable starting point\n",
    "initial_unconstrained = transform_to_unconstrained(true_params + 0.1 * jax.random.normal(rng_key, (5,)))\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nuts_state = nuts_init(initial_unconstrained, subkey)\n",
    "\n",
    "print(f\"NUTS initialized at: {transform_to_constrained(nuts_state.position)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_nuts"
   },
   "outputs": [],
   "source": [
    "# Run NUTS sampling\n",
    "print(\"Running NUTS sampling...\")\n",
    "\n",
    "num_nuts_samples = 2000\n",
    "num_warmup = 1000\n",
    "\n",
    "jit_nuts_step = jax.jit(nuts_kernel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Warmup phase\n",
    "nuts_samples = []\n",
    "current_state = nuts_state\n",
    "\n",
    "for i in range(num_warmup + num_nuts_samples):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    current_state, info = jit_nuts_step(subkey, current_state)\n",
    "    \n",
    "    # Keep samples after warmup\n",
    "    if i >= num_warmup:\n",
    "        constrained_sample = transform_to_constrained(current_state.position)\n",
    "        nuts_samples.append(constrained_sample)\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        status = \"warmup\" if i < num_warmup else \"sampling\"\n",
    "        print(f\"NUTS {status}: {i + 1}/{num_warmup + num_nuts_samples}\")\n",
    "\n",
    "nuts_time = time.time() - start_time\n",
    "nuts_samples = jnp.array(nuts_samples)\n",
    "\n",
    "print(f\"\\nNUTS completed in {nuts_time:.2f} seconds\")\n",
    "print(f\"Generated {len(nuts_samples)} samples\")\n",
    "\n",
    "# NUTS posterior statistics\n",
    "print(\"\\nNUTS posterior summary:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    mean = jnp.mean(nuts_samples[:, i])\n",
    "    std = jnp.std(nuts_samples[:, i])\n",
    "    true_val = true_params[i]\n",
    "    print(f\"  {name}: {mean:.3f} ± {std:.3f} (true: {true_val:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aies_comparison"
   },
   "outputs": [],
   "source": [
    "# Affine Invariant Ensemble Sampler (AIES) comparison\n",
    "print(\"Setting up AIES (emcee-like) sampler...\")\n",
    "\n",
    "# Use BlackJAX's implementation of ensemble sampling\n",
    "try:\n",
    "    from blackjax.mcmc.aies import init, build_kernel as aies_build_kernel\n",
    "    \n",
    "    # AIES configuration\n",
    "    num_walkers = 50\n",
    "    num_aies_steps = 2000\n",
    "    \n",
    "    @jax.jit\n",
    "    def aies_logdensity(params):\n",
    "        \"\"\"Log-density for AIES (gradient-free).\"\"\"\n",
    "        return loglikelihood_fn(params)  # Assuming flat prior within bounds\n",
    "    \n",
    "    # Initialize ensemble\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    \n",
    "    # Initialize walkers around true parameters with some spread\n",
    "    walker_init = []\n",
    "    for i in range(num_walkers):\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        # Sample from prior bounds with some concentration around true values\n",
    "        bounds_array = jnp.array([[v[0], v[1]] for v in prior_bounds.values()])\n",
    "        walker = jax.random.uniform(subkey, (5,), \n",
    "                                   minval=bounds_array[:, 0], \n",
    "                                   maxval=bounds_array[:, 1])\n",
    "        walker_init.append(walker)\n",
    "    \n",
    "    initial_ensemble = jnp.array(walker_init)\n",
    "    \n",
    "    aies_kernel = aies_build_kernel(aies_logdensity)\n",
    "    aies_state = init(initial_ensemble)\n",
    "    \n",
    "    print(f\"AIES initialized with {num_walkers} walkers\")\n",
    "    \n",
    "    # Run AIES\n",
    "    print(\"Running AIES sampling...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    jit_aies_step = jax.jit(aies_kernel)\n",
    "    \n",
    "    aies_samples = []\n",
    "    current_aies_state = aies_state\n",
    "    \n",
    "    for i in range(num_aies_steps):\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        current_aies_state, info = jit_aies_step(subkey, current_aies_state)\n",
    "        \n",
    "        # Store all walker positions\n",
    "        aies_samples.append(current_aies_state.position.copy())\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"AIES step: {i + 1}/{num_aies_steps}\")\n",
    "    \n",
    "    aies_time = time.time() - start_time\n",
    "    aies_samples = jnp.array(aies_samples)\n",
    "    \n",
    "    # Flatten walker dimension\n",
    "    aies_samples_flat = aies_samples.reshape(-1, 5)\n",
    "    \n",
    "    print(f\"\\nAIES completed in {aies_time:.2f} seconds\")\n",
    "    print(f\"Generated {len(aies_samples_flat)} samples\")\n",
    "    \n",
    "    # AIES posterior statistics\n",
    "    print(\"\\nAIES posterior summary:\")\n",
    "    for i, name in enumerate(param_names):\n",
    "        mean = jnp.mean(aies_samples_flat[:, i])\n",
    "        std = jnp.std(aies_samples_flat[:, i])\n",
    "        true_val = true_params[i]\n",
    "        print(f\"  {name}: {mean:.3f} ± {std:.3f} (true: {true_val:.3f})\")\n",
    "    \n",
    "    aies_available = True\n",
    "    \nexcept ImportError:\n",
    "    print(\"AIES not available in this BlackJAX version\")\n",
    "    print(\"Skipping AIES comparison...\")\n",
    "    aies_available = False\n",
    "    aies_time = 0\n",
    "    aies_samples_flat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison_visualization"
   },
   "outputs": [],
   "source": [
    "# Performance comparison visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Timing comparison\n",
    "methods = ['Nested Sampling', 'NUTS']\n",
    "times = [sampling_time, nuts_time]\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "if aies_available:\n",
    "    methods.append('AIES')\n",
    "    times.append(aies_time)\n",
    "    colors.append('green')\n",
    "\n",
    "axes[0, 0].bar(methods, times, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Sampling Time (seconds)')\n",
    "axes[0, 0].set_title('Computational Performance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample comparison for first two parameters\n",
    "param_idx = [0, 1]  # μₓ, μᵧ\n",
    "\n",
    "# Nested sampling\n",
    "axes[0, 1].scatter(samples[:, param_idx[0]], samples[:, param_idx[1]], \n",
    "                  alpha=0.3, s=10, label='Nested Sampling', c='blue')\n",
    "axes[0, 1].plot(true_params[param_idx[0]], true_params[param_idx[1]], \n",
    "               'r*', markersize=15, label='True')\n",
    "axes[0, 1].set_xlabel(param_names[param_idx[0]])\n",
    "axes[0, 1].set_ylabel(param_names[param_idx[1]])\n",
    "axes[0, 1].set_title('Nested Sampling Posterior')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# NUTS\n",
    "axes[0, 2].scatter(nuts_samples[:, param_idx[0]], nuts_samples[:, param_idx[1]], \n",
    "                  alpha=0.3, s=10, label='NUTS', c='orange')\n",
    "axes[0, 2].plot(true_params[param_idx[0]], true_params[param_idx[1]], \n",
    "               'r*', markersize=15, label='True')\n",
    "axes[0, 2].set_xlabel(param_names[param_idx[0]])\n",
    "axes[0, 2].set_ylabel(param_names[param_idx[1]])\n",
    "axes[0, 2].set_title('NUTS Posterior')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter accuracy comparison\n",
    "ns_errors = jnp.abs(jnp.array([nested_samples[name].mean() for name in param_names]) - true_params)\n",
    "nuts_errors = jnp.abs(jnp.mean(nuts_samples, axis=0) - true_params)\n",
    "\n",
    "x_pos = jnp.arange(len(param_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x_pos - width/2, ns_errors, width, label='Nested Sampling', color='blue', alpha=0.7)\n",
    "axes[1, 0].bar(x_pos + width/2, nuts_errors, width, label='NUTS', color='orange', alpha=0.7)\n",
    "\n",
    "if aies_available:\n",
    "    aies_errors = jnp.abs(jnp.mean(aies_samples_flat, axis=0) - true_params)\n",
    "    axes[1, 0].bar(x_pos + 1.5*width, aies_errors, width, label='AIES', color='green', alpha=0.7)\n",
    "\n",
    "axes[1, 0].set_xlabel('Parameters')\n",
    "axes[1, 0].set_ylabel('Absolute Error')\n",
    "axes[1, 0].set_title('Parameter Estimation Accuracy')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(param_names)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Marginal distributions comparison\n",
    "param_to_plot = 4  # ρ parameter (most constrained)\n",
    "\n",
    "axes[1, 1].hist(samples[:, param_to_plot], bins=30, alpha=0.7, \n",
    "               density=True, label='Nested Sampling', color='blue')\n",
    "axes[1, 1].hist(nuts_samples[:, param_to_plot], bins=30, alpha=0.7, \n",
    "               density=True, label='NUTS', color='orange')\n",
    "\n",
    "if aies_available:\n",
    "    axes[1, 1].hist(aies_samples_flat[:, param_to_plot], bins=30, alpha=0.7, \n",
    "                   density=True, label='AIES', color='green')\n",
    "\n",
    "axes[1, 1].axvline(true_params[param_to_plot], color='red', linestyle='--', \n",
    "                  linewidth=2, label='True')\n",
    "axes[1, 1].set_xlabel(f'{param_names[param_to_plot]}')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title(f'Marginal: {param_names[param_to_plot]}')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Summary statistics\n",
    "summary_text = f\"\"\"Method Comparison Summary:\n",
    "\n",
    "Nested Sampling:\n",
    "• Time: {sampling_time:.1f}s\n",
    "• Samples: {len(samples)}\n",
    "• Evidence: logZ = {live_state.logZ:.2f}\n",
    "• Handles multimodality\n",
    "\n",
    "NUTS:\n",
    "• Time: {nuts_time:.1f}s  \n",
    "• Samples: {len(nuts_samples)}\n",
    "• Requires gradients\n",
    "• May miss modes\n",
    "\"\"\"\n",
    "\n",
    "if aies_available:\n",
    "    summary_text += f\"\"\"\\nAIES:\n",
    "• Time: {aies_time:.1f}s\n",
    "• Samples: {len(aies_samples_flat)}\n",
    "• Gradient-free\n",
    "• Ensemble method\"\"\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, \n",
    "               fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "axes[1, 2].set_xlim(0, 1)\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].axis('off')\n",
    "axes[1, 2].set_title('Method Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_acceleration"
   },
   "source": [
    "## 7. GPU Acceleration and Performance\n",
    "\n",
    "BlackJAX's GPU-native implementation provides significant speedups, especially for:\n",
    "- **Large parameter spaces** (high-dimensional problems)\n",
    "- **Complex likelihood evaluations** (neural networks, PDEs)\n",
    "- **Parallel deletion** in nested sampling\n",
    "\n",
    "Let's explore the performance characteristics and demonstrate GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_scaling"
   },
   "outputs": [],
   "source": [
    "# Performance scaling experiment\n",
    "print(\"Testing performance scaling with different configurations...\")\n",
    "\n",
    "# Test different numbers of live points\n",
    "live_point_configs = [100, 500, 1000, 2000]\n",
    "performance_results = []\n",
    "\n",
    "for num_live_test in live_point_configs:\n",
    "    print(f\"\\nTesting with {num_live_test} live points...\")\n",
    "    \n",
    "    # Initialize nested sampling\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    particles_test, logprior_fn_test = ns_utils.uniform_prior(\n",
    "        subkey, num_live_test, prior_bounds\n",
    "    )\n",
    "    \n",
    "    nested_sampler_test = blackjax.nss(\n",
    "        logprior_fn=logprior_fn_test,\n",
    "        loglikelihood_fn=loglikelihood_fn,\n",
    "        num_delete=min(50, num_live_test // 10),  # Scale deletion parameter\n",
    "        num_inner_steps=25\n",
    "    )\n",
    "    \n",
    "    # Initialize and run a few steps for timing\n",
    "    live_state_test = nested_sampler_test.init(particles_test)\n",
    "    jit_step_test = jax.jit(nested_sampler_test.step)\n",
    "    \n",
    "    # Warm up JIT compilation\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    _, _ = jit_step_test(subkey, live_state_test)\n",
    "    \n",
    "    # Time multiple steps\n",
    "    num_test_steps = 10\n",
    "    start_time = time.time()\n",
    "    \n",
    "    current_state = live_state_test\n",
    "    for _ in range(num_test_steps):\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        current_state, _ = jit_step_test(subkey, current_state)\n",
    "    \n",
    "    step_time = (time.time() - start_time) / num_test_steps\n",
    "    \n",
    "    performance_results.append({\n",
    "        'num_live': num_live_test,\n",
    "        'step_time': step_time,\n",
    "        'throughput': num_live_test / step_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Step time: {step_time:.4f}s\")\n",
    "    print(f\"  Throughput: {num_live_test / step_time:.0f} points/second\")\n",
    "\n",
    "# Plot performance scaling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "num_live_points = [r['num_live'] for r in performance_results]\n",
    "step_times = [r['step_time'] for r in performance_results]\n",
    "throughputs = [r['throughput'] for r in performance_results]\n",
    "\n",
    "# Step time scaling\n",
    "ax1.plot(num_live_points, step_times, 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Live Points')\n",
    "ax1.set_ylabel('Step Time (seconds)')\n",
    "ax1.set_title('Nested Sampling Step Time Scaling')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Throughput scaling\n",
    "ax2.plot(num_live_points, throughputs, 's-', color='orange', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Live Points')\n",
    "ax2.set_ylabel('Throughput (points/second)')\n",
    "ax2.set_title('Sampling Throughput')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPerformance Summary:\")\n",
    "print(f\"  Best throughput: {max(throughputs):.0f} points/second\")\n",
    "print(f\"  Scaling: {'Sub-linear' if throughputs[-1] > throughputs[0] else 'Super-linear'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced_features"
   },
   "source": [
    "## 8. Advanced Features and Extensions\n",
    "\n",
    "BlackJAX nested sampling offers several advanced features for scientific applications:\n",
    "\n",
    "### Key Advantages:\n",
    "1. **Evidence computation**: Essential for model comparison in SBI\n",
    "2. **Multimodal handling**: Robust exploration of complex posteriors\n",
    "3. **GPU acceleration**: Leverages modern HPC infrastructure\n",
    "4. **JAX integration**: Seamless autodiff and JIT compilation\n",
    "5. **Open source**: Community-driven development\n",
    "\n",
    "### When to Use Nested Sampling:\n",
    "- **Model comparison** (Bayesian evidence needed)\n",
    "- **Multimodal posteriors** (phase transitions, symmetries)\n",
    "- **High-dimensional problems** (gradient-free exploration)\n",
    "- **SBI workflows** (NLE, NRE, NJE all need sampling)\n",
    "- **Scientific inference** (evidence quantification important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_comparison_demo"
   },
   "outputs": [],
   "source": [
    "# Model comparison demonstration\n",
    "print(\"Demonstrating Bayesian model comparison with nested sampling...\")\n",
    "\n",
    "# Define two competing models\n",
    "# Model 1: Full 2D Gaussian (5 parameters)\n",
    "# Model 2: Circular Gaussian (3 parameters: μₓ, μᵧ, σ)\n",
    "\n",
    "@jax.jit\n",
    "def circular_gaussian_loglikelihood(params_3d):\n",
    "    \"\"\"Likelihood for circular Gaussian model (σₓ = σᵧ, ρ = 0).\"\"\"\n",
    "    mu_x, mu_y, sigma = params_3d\n",
    "    \n",
    "    # Convert to 5D parameter space\n",
    "    params_5d = jnp.array([mu_x, mu_y, sigma, sigma, 0.0])\n",
    "    \n",
    "    return loglikelihood_fn(params_5d)\n",
    "\n",
    "# Model 2 prior bounds (3D)\n",
    "prior_bounds_3d = {\n",
    "    \"mu_x\": (-2.0, 2.0),\n",
    "    \"mu_y\": (-2.0, 2.0),\n",
    "    \"sigma\": (0.5, 3.0)\n",
    "}\n",
    "\n",
    "# Run nested sampling for Model 2 (circular Gaussian)\n",
    "print(\"\\nRunning nested sampling for Model 2 (circular Gaussian)...\")\n",
    "\n",
    "num_live_simple = 500\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "particles_3d, logprior_fn_3d = ns_utils.uniform_prior(\n",
    "    subkey, num_live_simple, prior_bounds_3d\n",
    ")\n",
    "\n",
    "nested_sampler_3d = blackjax.nss(\n",
    "    logprior_fn=logprior_fn_3d,\n",
    "    loglikelihood_fn=circular_gaussian_loglikelihood,\n",
    "    num_delete=25,\n",
    "    num_inner_steps=15\n",
    ")\n",
    "\n",
    "live_state_3d = nested_sampler_3d.init(particles_3d)\n",
    "jit_step_3d = jax.jit(nested_sampler_3d.step)\n",
    "\n",
    "# Run until convergence\n",
    "dead_points_3d = []\n",
    "iteration_3d = 0\n",
    "\n",
    "while (live_state_3d.logZ_live - live_state_3d.logZ) > -3.0:\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    live_state_3d, dead_info_3d = jit_step_3d(subkey, live_state_3d)\n",
    "    dead_points_3d.append(dead_info_3d)\n",
    "    iteration_3d += 1\n",
    "    \n",
    "    if iteration_3d % 25 == 0:\n",
    "        remaining = live_state_3d.logZ_live - live_state_3d.logZ\n",
    "        print(f\"  Iteration {iteration_3d}: logZ = {live_state_3d.logZ:.3f}, remaining = {remaining:.3f}\")\n",
    "\n",
    "print(f\"\\nModel Comparison Results:\")\n",
    "print(f\"Model 1 (Full 2D Gaussian):     logZ = {live_state.logZ:.3f} ± {jnp.sqrt(live_state.H):.3f}\")\n",
    "print(f\"Model 2 (Circular Gaussian):    logZ = {live_state_3d.logZ:.3f} ± {jnp.sqrt(live_state_3d.H):.3f}\")\n",
    "\n",
    "# Calculate Bayes factor\n",
    "log_bayes_factor = live_state.logZ - live_state_3d.logZ\n",
    "bayes_factor = jnp.exp(log_bayes_factor)\n",
    "\n",
    "print(f\"\\nBayes Factor (Model 1 / Model 2): {bayes_factor:.2f}\")\n",
    "print(f\"Log Bayes Factor: {log_bayes_factor:.3f}\")\n",
    "\n",
    "if log_bayes_factor > 1:\n",
    "    print(\"Evidence favors Model 1 (Full 2D Gaussian)\")\n",
    "elif log_bayes_factor < -1:\n",
    "    print(\"Evidence favors Model 2 (Circular Gaussian)\")\n",
    "else:\n",
    "    print(\"Evidence is inconclusive\")\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Evidence comparison\n",
    "models = ['Full 2D\\nGaussian', 'Circular\\nGaussian']\n",
    "evidences = [live_state.logZ, live_state_3d.logZ]\n",
    "errors = [jnp.sqrt(live_state.H), jnp.sqrt(live_state_3d.H)]\n",
    "\n",
    "ax1.bar(models, evidences, yerr=errors, capsize=5, alpha=0.7, color=['blue', 'orange'])\n",
    "ax1.set_ylabel('Log Evidence')\n",
    "ax1.set_title('Model Comparison: Bayesian Evidence')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter comparison for overlapping parameters\n",
    "dead_3d = ns_utils.finalise(live_state_3d, dead_points_3d)\n",
    "samples_3d = dead_3d.particles\n",
    "\n",
    "# Compare μₓ and μᵧ estimates\n",
    "ax2.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10, label='Model 1 (5D)', c='blue')\n",
    "ax2.scatter(samples_3d[:, 0], samples_3d[:, 1], alpha=0.3, s=10, label='Model 2 (3D)', c='orange')\n",
    "ax2.plot(true_params[0], true_params[1], 'r*', markersize=15, label='True')\n",
    "ax2.set_xlabel('μₓ')\n",
    "ax2.set_ylabel('μᵧ')\n",
    "ax2.set_title('Position Parameter Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "your_turn"
   },
   "source": [
    "## 9. Your Turn: Apply BlackJAX to Your Problems!\n",
    "\n",
    "Now it's time to experiment with BlackJAX nested sampling on your own research problems. Here are some suggestions:\n",
    "\n",
    "### Exercise Options:\n",
    "\n",
    "1. **Modify the current problem**:\n",
    "   - Change the true parameters and see how the inference performs\n",
    "   - Add more noise to make the problem harder\n",
    "   - Try different prior bounds\n",
    "\n",
    "2. **Use your own JAX likelihood**:\n",
    "   - Bring code from Viraj's workshop\n",
    "   - Implement your own scientific model\n",
    "   - Compare nested sampling vs. your usual inference method\n",
    "\n",
    "3. **Explore different configurations**:\n",
    "   - Increase the number of live points for higher precision\n",
    "   - Try different `num_inner_steps` values\n",
    "   - Experiment with the `num_delete` parameter for GPU optimization\n",
    "\n",
    "### Template for Your Own Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "your_experiment"
   },
   "outputs": [],
   "source": [
    "# Template for your own BlackJAX nested sampling experiment\n",
    "\n",
    "# Step 1: Define your likelihood function\n",
    "@jax.jit\n",
    "def your_loglikelihood_fn(params):\n",
    "    \"\"\"\n",
    "    Replace this with your own likelihood function.\n",
    "    \n",
    "    Args:\n",
    "        params: JAX array of parameters\n",
    "    \n",
    "    Returns:\n",
    "        Log-likelihood value\n",
    "    \"\"\"\n",
    "    # Example: simple 2D Gaussian\n",
    "    mu_x, mu_y = params[:2]\n",
    "    \n",
    "    # Your model/simulation here\n",
    "    model_prediction = your_model(params)\n",
    "    \n",
    "    # Your likelihood calculation here\n",
    "    loglik = your_likelihood_calculation(model_prediction, your_data)\n",
    "    \n",
    "    return loglik\n",
    "\n",
    "# Step 2: Define your prior bounds\n",
    "your_prior_bounds = {\n",
    "    \"param1\": (lower_bound, upper_bound),\n",
    "    \"param2\": (lower_bound, upper_bound),\n",
    "    # Add more parameters as needed\n",
    "}\n",
    "\n",
    "# Step 3: Configure and run nested sampling\n",
    "your_num_live = 500  # Adjust based on your problem complexity\n",
    "your_num_dims = len(your_prior_bounds)\n",
    "your_num_inner_steps = your_num_dims * 5\n",
    "\n",
    "# Initialize\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "your_particles, your_logprior_fn = ns_utils.uniform_prior(\n",
    "    subkey, your_num_live, your_prior_bounds\n",
    ")\n",
    "\n",
    "# Create sampler\n",
    "your_sampler = blackjax.nss(\n",
    "    logprior_fn=your_logprior_fn,\n",
    "    loglikelihood_fn=your_loglikelihood_fn,\n",
    "    num_delete=25,\n",
    "    num_inner_steps=your_num_inner_steps\n",
    ")\n",
    "\n",
    "# Run sampling (add your implementation here)\n",
    "print(\"Implement your nested sampling run here!\")\n",
    "print(\"Follow the pattern from the examples above.\")\n",
    "\n",
    "# Step 4: Analyze results with Anesthetic\n",
    "# your_nested_samples = NestedSamples(...)\n",
    "# Make corner plots, compute evidence, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resources_next_steps"
   },
   "source": [
    "## 10. Resources and Next Steps\n",
    "\n",
    "### Key Resources:\n",
    "\n",
    "**BlackJAX Nested Sampling:**\n",
    "- Repository: https://github.com/handley-lab/blackjax\n",
    "- Documentation: [BlackJAX docs](https://blackjax-devs.github.io/blackjax/)\n",
    "- Installation: `pip install git+https://github.com/handley-lab/blackjax@nested_sampling`\n",
    "\n",
    "**Visualization:**\n",
    "- Anesthetic: https://anesthetic.readthedocs.io/en/latest/plotting.html\n",
    "- Corner plots for nested sampling results\n",
    "- Evidence evolution diagnostics\n",
    "\n",
    "**JAX Ecosystem:**\n",
    "- JAX documentation: https://jax.readthedocs.io/\n",
    "- NumPyro (probabilistic programming): https://num.pyro.ai/\n",
    "- Optax (optimization): https://optax.readthedocs.io/\n",
    "\n",
    "### When to Use BlackJAX Nested Sampling:\n",
    "\n",
    "✅ **Good for:**\n",
    "- Multimodal posteriors\n",
    "- Model comparison (evidence computation)\n",
    "- High-dimensional problems\n",
    "- GPU-accelerated inference\n",
    "- SBI workflows (NLE, NRE, NJE)\n",
    "- Scientific applications requiring evidence quantification\n",
    "\n",
    "❌ **Consider alternatives for:**\n",
    "- Simple unimodal posteriors (HMC/NUTS may be faster)\n",
    "- Very low-dimensional problems (< 3D)\n",
    "- When you only need posterior samples (not evidence)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try on your research problems**: Apply to your own JAX-based models\n",
    "2. **Experiment with configurations**: Optimize for your specific use case\n",
    "3. **Compare methods**: Benchmark against your current inference approach\n",
    "4. **Contribute**: BlackJAX is community-driven - report issues, suggest features\n",
    "5. **Stay updated**: Follow BlackJAX development for new features\n",
    "\n",
    "### Questions?\n",
    "\n",
    "- GitHub Issues: https://github.com/handley-lab/blackjax/issues\n",
    "- Discussion: BlackJAX community channels\n",
    "- This workshop: Experiment with the provided templates!\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for participating in the BlackJAX Nested Sampling Workshop!**\n",
    "\n",
    "*The future of scientific inference is GPU-native, open-source, and community-driven.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "workshop_title"
   },
   "source": [
    "# BlackJAX Nested Sampling Workshop\n",
    "## GPU-Native Bayesian Inference for SBI\n",
    "\n",
    "**SBI Galev 2025 Workshop**  \n",
    "*Will Handley, University of Cambridge*\n",
    "\n",
    "---\n",
    "\n",
    "### Workshop Overview\n",
    "\n",
    "In this hands-on workshop, you'll learn to use **BlackJAX nested sampling** - a GPU-native implementation that leverages JAX's autodiff and JIT compilation for modern Bayesian inference.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand when nested sampling excels over MCMC methods\n",
    "2. Implement nested sampling with BlackJAX\n",
    "3. Visualize results with Anesthetic\n",
    "4. Compare performance: nested sampling vs. affine invariant ensemble sampling\n",
    "5. Leverage GPU acceleration for scientific inference\n",
    "\n",
    "**Why BlackJAX Nested Sampling?**\n",
    "- **GPU-native**: Full JAX integration (autodiff + JIT compilation)\n",
    "- **Open source**: Community-owned alternative to legacy Fortran tools\n",
    "- **Modern**: Designed for SBI workflows and scientific computing\n",
    "- **Efficient**: Handles multimodal posteriors and evidence computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "We'll install BlackJAX from the nested sampling branch and set up our environment for GPU-accelerated inference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": "# Install BlackJAX and visualization tools\n!pip install git+https://github.com/handley-lab/blackjax\n!pip install anesthetic matplotlib corner tqdm\n\n# Check for GPU availability\nimport jax\nprint(f\"JAX devices: {jax.devices()}\")\nprint(f\"JAX backend: {jax.lib.xla_bridge.get_backend().platform}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# BlackJAX imports\n",
    "import blackjax\n",
    "import blackjax.ns.utils as ns_utils\n",
    "\n",
    "# Visualization\n",
    "from anesthetic import NestedSamples\n",
    "import corner\n",
    "\n",
    "# JAX configuration for precision and reproducibility\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update('jax_platform_name', 'cpu')  # Change to 'gpu' if available\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "rng_key = jax.random.key(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Getting Started: Line Fitting with BlackJAX\n\nLet's start with a simple, accessible example: fitting a straight line to noisy data. This demonstrates the core BlackJAX nested sampling workflow before moving to more complex problems.\n\n**Problem**: Fit a line y = mx + c to noisy observations\n- **Parameters**: [m, c, σ] (slope, intercept, noise level)\n- **Data**: 20 noisy observations of a linear relationship\n- **Goal**: Posterior inference and evidence computation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate synthetic line data\nnp.random.seed(42)\nn_data = 20\nx_data = jnp.linspace(0, 10, n_data)\n\n# True parameters for line: y = mx + c + noise\ntrue_m = 2.5    # slope\ntrue_c = 1.0    # intercept  \ntrue_sigma = 0.5 # noise level\n\n# Generate noisy observations\ntrue_y_clean = true_m * x_data + true_c\nnoise = true_sigma * np.random.normal(size=n_data)\ny_data = true_y_clean + noise\n\n# Visualize the data\nplt.figure(figsize=(10, 6))\nplt.errorbar(x_data, y_data, yerr=true_sigma, fmt='o', capsize=3, alpha=0.7, label='Observed data')\nplt.plot(x_data, true_y_clean, 'r--', label=f'True line: y = {true_m}x + {true_c}', linewidth=2)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line Fitting Problem: Noisy Observations')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"True parameters: m = {true_m}, c = {true_c}, σ = {true_sigma}\")\nprint(f\"Number of data points: {n_data}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define likelihood and prior for line fitting\n@jax.jit\ndef line_loglikelihood(params_dict):\n    \"\"\"Log-likelihood for linear regression.\"\"\"\n    m = params_dict[\"m\"]\n    c = params_dict[\"c\"] \n    sigma = params_dict[\"sigma\"]\n    \n    # Predicted y values\n    y_pred = m * x_data + c\n    \n    # Gaussian likelihood\n    residuals = (y_data - y_pred) / sigma\n    loglik = -0.5 * jnp.sum(residuals**2) - n_data * jnp.log(sigma) - 0.5 * n_data * jnp.log(2 * jnp.pi)\n    \n    return loglik\n\n# Prior bounds for line fitting\nline_prior_bounds = {\n    \"m\": (0.0, 5.0),      # slope\n    \"c\": (-2.0, 4.0),     # intercept\n    \"sigma\": (0.1, 2.0)   # noise level (must be positive)\n}\n\n# Test the likelihood\ntest_params_line = {\"m\": true_m, \"c\": true_c, \"sigma\": true_sigma}\ntest_loglik_line = line_loglikelihood(test_params_line)\nprint(f\"Log-likelihood at true parameters: {test_loglik_line:.3f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run BlackJAX nested sampling for line fitting\nprint(\"Setting up nested sampling for line fitting...\")\n\n# Configuration for 3D problem\nline_num_live = 100\nline_num_dims = 3\nline_num_inner_steps = line_num_dims * 5\n\n# Initialize prior and particles\nrng_key, subkey = jax.random.split(rng_key)\nline_particles, line_logprior_fn = ns_utils.uniform_prior(\n    subkey, line_num_live, line_prior_bounds\n)\n\n# Create nested sampler\nline_sampler = blackjax.nss(\n    logprior_fn=line_logprior_fn,\n    loglikelihood_fn=line_loglikelihood,\n    num_delete=50,\n    num_inner_steps=line_num_inner_steps\n)\n\n# Initialize and run\nline_live_state = line_sampler.init(line_particles)\nline_jit_step = jax.jit(line_sampler.step)\n\nprint(f\"Initialized {line_num_live} live points for 3D line fitting problem\")\nprint(f\"Starting nested sampling...\")\n\n# Run nested sampling\nline_dead_points = []\nline_iteration = 0\nstart_time = time.time()\n\nwhile (line_live_state.logZ_live - line_live_state.logZ) > -3.0:\n    rng_key, subkey = jax.random.split(rng_key)\n    \n    line_live_state, line_dead_info = line_jit_step(subkey, line_live_state)\n    line_dead_points.append(line_dead_info)\n    \n    line_iteration += 1\n    \n    if line_iteration % 25 == 0:\n        remaining = line_live_state.logZ_live - line_live_state.logZ\n        print(f\"  Iteration {line_iteration:3d}: logZ = {line_live_state.logZ:.3f}, remaining = {remaining:.3f}\")\n\nline_sampling_time = time.time() - start_time\n\nprint(f\"\\nLine fitting completed!\")\nprint(f\"Total iterations: {line_iteration}\")\nprint(f\"Sampling time: {line_sampling_time:.2f} seconds\")\nprint(f\"Evidence: logZ = {line_live_state.logZ:.3f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Process and visualize line fitting results\nline_dead = ns_utils.finalise(line_live_state, line_dead_points)\n\n# Extract samples from dictionary format\nline_param_names = ['m', 'c', 'σ']\nparam_keys = ['m', 'c', 'sigma']  # Keys in the particles dict\n\n# Convert dictionary particles to array for anesthetic\nline_samples_dict = line_dead.particles\nline_data = jnp.vstack([line_samples_dict[key] for key in param_keys]).T\n\n# Create NestedSamples for anesthetic\nline_nested_samples = NestedSamples(\n    data=line_data,\n    logL=line_dead.loglikelihood,\n    logL_birth=line_dead.loglikelihood_birth,\n    columns=line_param_names,\n    labels=[r\"$m$\", r\"$c$\", r\"$\\sigma$\"]\n)\n\n# Print posterior summary\nprint(\"Line fitting posterior summary:\")\nline_true_params = jnp.array([true_m, true_c, true_sigma])\nfor i, name in enumerate(line_param_names):\n    mean = line_nested_samples[name].mean()\n    std = line_nested_samples[name].std()\n    true_val = line_true_params[i]\n    print(f\"  {name}: {mean:.3f} ± {std:.3f} (true: {true_val:.3f})\")\n\n# Estimate evidence with error\nevidence_mean = line_nested_samples.logZ()\nevidence_std = line_nested_samples.logZ(1000).std()\nprint(f\"\\nEvidence: logZ = {evidence_mean:.3f} ± {evidence_std:.3f}\")\n\n# Visualize results with anesthetic\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Corner plot using anesthetic\nkinds = {'lower': 'kde_2d', 'diagonal': 'hist_1d', 'upper': 'scatter_2d'}\ncorner_axes = line_nested_samples.plot_2d(line_param_names, kinds=kinds, label='posterior')\n\n# Posterior predictions\naxes[0, 0].errorbar(x_data, y_data, yerr=true_sigma, fmt='o', capsize=3, alpha=0.7, label='Data')\n\n# Sample posterior lines\nn_posterior_samples = 100\nx_pred = jnp.linspace(0, 10, 100)\n\n# Get posterior samples for prediction\npost_samples = line_nested_samples.sample(n_posterior_samples)\nfor i in range(min(20, len(post_samples))):  # Show first 20 samples\n    m_sample = post_samples.iloc[i]['m']\n    c_sample = post_samples.iloc[i]['c']\n    y_pred_sample = m_sample * x_pred + c_sample\n    axes[0, 0].plot(x_pred, y_pred_sample, 'b-', alpha=0.1)\n\n# True line and posterior mean\naxes[0, 0].plot(x_pred, true_m * x_pred + true_c, 'r--', linewidth=2, label='True line')\nmean_m = line_nested_samples['m'].mean()\nmean_c = line_nested_samples['c'].mean()\naxes[0, 0].plot(x_pred, mean_m * x_pred + mean_c, 'g-', linewidth=2, label='Posterior mean')\n\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('y')\naxes[0, 0].set_title('Line Fitting: Posterior Predictions')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Evidence evolution\naxes[0, 1].plot(line_nested_samples.logL, line_nested_samples.logZ_birth)\naxes[0, 1].set_xlabel('Log-likelihood')\naxes[0, 1].set_ylabel('Log Evidence at Birth')\naxes[0, 1].set_title('Evidence Evolution')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Log-likelihood sequence\naxes[1, 0].plot(line_nested_samples.logL)\naxes[1, 0].set_xlabel('Sample index')\naxes[1, 0].set_ylabel('Log-likelihood')\naxes[1, 0].set_title('Likelihood Sequence')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Summary text\nsummary_text = f\"\"\"Line Fitting Results:\n\nPosterior means:\n• Slope (m): {mean_m:.3f} ± {line_nested_samples['m'].std():.3f}\n• Intercept (c): {mean_c:.3f} ± {line_nested_samples['c'].std():.3f}\n• Noise (σ): {line_nested_samples['σ'].mean():.3f} ± {line_nested_samples['σ'].std():.3f}\n\nEvidence: {evidence_mean:.3f} ± {evidence_std:.3f}\nSampling time: {line_sampling_time:.1f}s\nIterations: {line_iteration}\"\"\"\n\naxes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,\n               fontsize=10, verticalalignment='top', fontfamily='monospace')\naxes[1, 1].set_xlim(0, 1)\naxes[1, 1].set_ylim(0, 1)\naxes[1, 1].axis('off')\naxes[1, 1].set_title('Summary')\n\nplt.suptitle('BlackJAX Nested Sampling: Line Fitting Example', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem_setup"
   },
   "source": "## 3. Advanced Example: 2D Gaussian Parameter Inference\n\nNow let's move to a more complex problem that builds on Viraj's JAX/SciML workshop. This demonstrates nested sampling's power for multimodal posteriors.\n\n**Problem**: Infer parameters of a 2D Gaussian from noisy pixelated observations\n- **Parameters**: [μₓ, μᵧ, σₓ, σᵧ, ρₓᵧ] (5D parameter space)\n- **Data**: 50×50 pixel noisy images of 2D Gaussians\n- **Challenge**: Multimodal posterior due to parameter symmetries",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simulator_function"
   },
   "outputs": [],
   "source": [
    "# Problem configuration\n",
    "image_size = 50\n",
    "x = jnp.linspace(-3, 3, image_size)\n",
    "y = jnp.linspace(-3, 3, image_size)\n",
    "X, Y = jnp.meshgrid(x, y)\n",
    "coords = jnp.stack([X.ravel(), Y.ravel()]).T  # (2500, 2)\n",
    "\n",
    "# True parameters for data generation\n",
    "true_params = jnp.array([0.5, -0.3, 1.2, 0.8, 0.4])  # [μₓ, μᵧ, σₓ, σᵧ, ρₓᵧ]\n",
    "\n",
    "@jax.jit\n",
    "def params_to_cov(params):\n",
    "    \"\"\"Convert parameters to mean vector and covariance matrix.\"\"\"\n",
    "    mu_x, mu_y, sigma_x, sigma_y, rho = params\n",
    "    \n",
    "    mean = jnp.array([mu_x, mu_y])\n",
    "    \n",
    "    # Covariance matrix\n",
    "    cov = jnp.array([\n",
    "        [sigma_x**2, rho * sigma_x * sigma_y],\n",
    "        [rho * sigma_x * sigma_y, sigma_y**2]\n",
    "    ])\n",
    "    \n",
    "    return mean, cov\n",
    "\n",
    "@jax.jit\n",
    "def simulator(params, rng_key, noise_sigma=0.1):\n",
    "    \"\"\"Simulate 2D Gaussian observation with noise.\"\"\"\n",
    "    mean, cov = params_to_cov(params)\n",
    "    \n",
    "    # Evaluate multivariate normal PDF on grid\n",
    "    logpdf = jax.scipy.stats.multivariate_normal.logpdf(coords, mean, cov)\n",
    "    image_clean = logpdf.reshape(image_size, image_size)\n",
    "    image_clean = jnp.exp(image_clean - jnp.max(image_clean))  # Normalize\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = jax.random.normal(rng_key, image_clean.shape) * noise_sigma\n",
    "    \n",
    "    return image_clean + noise\n",
    "\n",
    "# Generate observed data\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "observed_data = simulator(true_params, subkey)\n",
    "\n",
    "# Visualize the observed data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(observed_data, origin='lower', extent=[-3, 3, -3, 3], cmap='viridis')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title('Observed 2D Gaussian (with noise)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "\n",
    "print(f\"True parameters: μₓ={true_params[0]:.2f}, μᵧ={true_params[1]:.2f}, \"\n",
    "      f\"σₓ={true_params[2]:.2f}, σᵧ={true_params[3]:.2f}, ρ={true_params[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "likelihood_function"
   },
   "outputs": [],
   "source": "@jax.jit\ndef loglikelihood_fn(params_dict):\n    \"\"\"Log-likelihood function for parameter inference.\"\"\"\n    # Extract parameters from dictionary\n    mu_x = params_dict[\"mu_x\"]\n    mu_y = params_dict[\"mu_y\"]\n    sigma_x = params_dict[\"sigma_x\"]\n    sigma_y = params_dict[\"sigma_y\"]\n    rho = params_dict[\"rho\"]\n    \n    # Convert to mean and covariance\n    mean = jnp.array([mu_x, mu_y])\n    cov = jnp.array([\n        [sigma_x**2, rho * sigma_x * sigma_y],\n        [rho * sigma_x * sigma_y, sigma_y**2]\n    ])\n    \n    # Check if covariance matrix is positive definite\n    det_cov = jnp.linalg.det(cov)\n    \n    # Return -inf if covariance is not positive definite\n    def valid_cov():\n        logpdf = jax.scipy.stats.multivariate_normal.logpdf(coords, mean, cov)\n        image_pred = logpdf.reshape(image_size, image_size)\n        image_pred = jnp.exp(image_pred - jnp.max(image_pred))\n        \n        # Gaussian likelihood (MSE)\n        noise_sigma = 0.1\n        residuals = (observed_data - image_pred) / noise_sigma\n        return -0.5 * jnp.sum(residuals**2)\n    \n    def invalid_cov():\n        return -jnp.inf\n    \n    return jax.lax.cond(det_cov > 0, valid_cov, invalid_cov)\n\n# Prior bounds\nprior_bounds = {\n    \"mu_x\": (-2.0, 2.0),\n    \"mu_y\": (-2.0, 2.0), \n    \"sigma_x\": (0.5, 3.0),\n    \"sigma_y\": (0.5, 3.0),\n    \"rho\": (-0.99, 0.99)  # Correlation must be in (-1, 1)\n}\n\n# Test likelihood function\ntest_params_dict = {\n    \"mu_x\": true_params[0],\n    \"mu_y\": true_params[1], \n    \"sigma_x\": true_params[2],\n    \"sigma_y\": true_params[3],\n    \"rho\": true_params[4]\n}\ntest_loglik = loglikelihood_fn(test_params_dict)\nprint(f\"Log-likelihood at true parameters: {test_loglik:.3f}\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nested_sampling_setup"
   },
   "outputs": [],
   "source": "# Nested sampling configuration\nnum_live = 100  # Number of live points (controls precision)\nnum_dims = 5     # Parameter dimensionality\nnum_inner_steps = num_dims * 5  # MCMC steps per NS iteration (3-5 × ndim)\nnum_delete = 50  # Parallelization parameter for GPU efficiency\n\nprint(f\"Nested sampling configuration:\")\nprint(f\"  Live points: {num_live}\")\nprint(f\"  Inner MCMC steps: {num_inner_steps}\")\nprint(f\"  Parallel deletion: {num_delete}\")\n\n# Initialize uniform prior and live points\nrng_key, subkey = jax.random.split(rng_key)\nparticles, logprior_fn = ns_utils.uniform_prior(\n    subkey, num_live, prior_bounds\n)\n\nprint(f\"\\nInitialized {num_live} live points in {num_dims}D space\")\nprint(f\"Parameter bounds: {prior_bounds}\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nested_sampling_run"
   },
   "outputs": [],
   "source": "# Create nested sampler\nnested_sampler = blackjax.nss(\n    logprior_fn=logprior_fn,\n    loglikelihood_fn=loglikelihood_fn,\n    num_delete=num_delete,\n    num_inner_steps=num_inner_steps\n)\n\n# Initialize nested sampling state\nrng_key, subkey = jax.random.split(rng_key)\nlive_state = nested_sampler.init(particles)\n\n# JIT compile the sampling step for efficiency\njit_step = jax.jit(nested_sampler.step)\n\nprint(f\"Initial evidence estimate: {live_state.logZ:.3f}\")\nprint(f\"Initial live evidence: {live_state.logZ_live:.3f}\")\nprint(\"\\nStarting nested sampling...\")\n\n# Run nested sampling until convergence\ndead_points = []\niteration = 0\nconvergence_threshold = -3.0  # log(0.05) - stop when evidence contribution < 5%\n\nstart_time = time.time()\n\nwhile (live_state.logZ_live - live_state.logZ) > convergence_threshold:\n    rng_key, subkey = jax.random.split(rng_key)\n    \n    # Take nested sampling step\n    live_state, dead_info = jit_step(subkey, live_state)\n    dead_points.append(dead_info)\n    \n    iteration += 1\n    \n    # Progress updates\n    if iteration % 50 == 0:\n        remaining_evidence = live_state.logZ_live - live_state.logZ\n        print(f\"Iteration {iteration:4d}: logZ = {live_state.logZ:.3f}, \"\n              f\"remaining = {remaining_evidence:.3f}\")\n\nsampling_time = time.time() - start_time\n\nprint(f\"\\nNested sampling completed!\")\nprint(f\"Total iterations: {iteration}\")\nprint(f\"Sampling time: {sampling_time:.2f} seconds\")\nprint(f\"Final evidence: logZ = {live_state.logZ:.3f}\")"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "results_processing"
   },
   "source": "# Process results and create anesthetic NestedSamples\ndead = ns_utils.finalise(live_state, dead_points)\n\n# Extract samples from dictionary format for anesthetic\nparam_names = ['μₓ', 'μᵧ', 'σₓ', 'σᵧ', 'ρ']\nparam_keys = ['mu_x', 'mu_y', 'sigma_x', 'sigma_y', 'rho']\n\n# Convert dictionary particles to array\nsamples_dict = dead.particles\nsamples = jnp.vstack([samples_dict[key] for key in param_keys]).T\n\n# Create NestedSamples for anesthetic\nnested_samples = NestedSamples(\n    data=samples,\n    logL=dead.loglikelihood,\n    logL_birth=dead.loglikelihood_birth,\n    columns=param_names,\n    labels=[r\"$\\mu_x$\", r\"$\\mu_y$\", r\"$\\sigma_x$\", r\"$\\sigma_y$\", r\"$\\rho$\"]\n)\n\n# Print posterior summary\nprint(\"2D Gaussian parameter inference results:\")\nfor i, name in enumerate(param_names):\n    mean = nested_samples[name].mean()\n    std = nested_samples[name].std()\n    true_val = true_params[i]\n    print(f\"  {name}: {mean:.3f} ± {std:.3f} (true: {true_val:.3f})\")\n\n# Evidence calculation with error estimate\nevidence_mean = nested_samples.logZ()\nevidence_std = nested_samples.logZ(1000).std()\nprint(f\"\\nEvidence: logZ = {evidence_mean:.3f} ± {evidence_std:.3f}\")\nprint(f\"Total iterations: {iteration}\")\nprint(f\"Sampling time: {sampling_time:.2f} seconds\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nested_sampling_diagnostics"
   },
   "outputs": [],
   "source": "# Nested sampling diagnostic plots and visualization\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Corner plot using anesthetic\nkinds = {'lower': 'kde_2d', 'diagonal': 'hist_1d', 'upper': 'scatter_2d'}\ncorner_fig, corner_axes = nested_samples.plot_2d(param_names[:3], kinds=kinds)\ncorner_fig.suptitle('Posterior Corner Plot (First 3 Parameters)')\n\n# Evidence evolution\naxes[0, 0].plot(nested_samples.logL, nested_samples.logZ_birth)\naxes[0, 0].set_xlabel('Log-likelihood')\naxes[0, 0].set_ylabel('Log Evidence at Birth')\naxes[0, 0].set_title('Evidence Evolution')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Log-likelihood sequence\naxes[0, 1].plot(nested_samples.logL)\naxes[0, 1].set_xlabel('Sample index')\naxes[0, 1].set_ylabel('Log-likelihood')\naxes[0, 1].set_title('Likelihood Sequence')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Posterior weights\nweights = nested_samples.get_weights()\naxes[0, 2].plot(weights)\naxes[0, 2].set_xlabel('Sample index')\naxes[0, 2].set_ylabel('Posterior weight')\naxes[0, 2].set_title('Posterior Weights')\naxes[0, 2].grid(True, alpha=0.3)\n\n# Parameter scatter plot (2D projection)\naxes[1, 0].scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=10, c='blue')\naxes[1, 0].plot(true_params[0], true_params[1], 'r*', markersize=15, label='True')\naxes[1, 0].set_xlabel(param_names[0])\naxes[1, 0].set_ylabel(param_names[1])\naxes[1, 0].set_title('Position Parameters')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Marginal distribution for correlation parameter\naxes[1, 1].hist(samples[:, 4], bins=30, alpha=0.7, density=True, color='blue')\naxes[1, 1].axvline(true_params[4], color='red', linestyle='--', linewidth=2, label='True')\naxes[1, 1].set_xlabel(param_names[4])\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].set_title('Correlation Parameter')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Effective sample size and summary\nn_eff = nested_samples.neff()\nefficiency = n_eff / len(samples)\nsummary_text = f\"\"\"Nested Sampling Results:\n\nEvidence: {evidence_mean:.3f} ± {evidence_std:.3f}\nEffective samples: {n_eff:.0f} / {len(samples)}\nEfficiency: {efficiency:.1%}\nTotal iterations: {iteration}\nSampling time: {sampling_time:.2f}s\n\nTrue parameters:\nμₓ = {true_params[0]:.2f}\nμᵧ = {true_params[1]:.2f}  \nσₓ = {true_params[2]:.2f}\nσᵧ = {true_params[3]:.2f}\nρ = {true_params[4]:.2f}\"\"\"\n\naxes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes,\n               fontsize=10, verticalalignment='top', fontfamily='monospace')\naxes[1, 2].set_xlim(0, 1)\naxes[1, 2].set_ylim(0, 1)\naxes[1, 2].axis('off')\naxes[1, 2].set_title('Summary')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nNested Sampling Diagnostics:\")\nprint(f\"  Effective sample size: {n_eff:.0f} / {len(samples)} ({efficiency:.1%})\")\nprint(f\"  Evidence: logZ = {evidence_mean:.3f} ± {evidence_std:.3f}\")\nprint(f\"  Total iterations: {iteration}\")\nprint(f\"  Performance: {len(samples)/sampling_time:.0f} samples/second\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison_setup"
   },
   "source": [
    "## 6. Performance Comparison: Nested Sampling vs. MCMC\n",
    "\n",
    "Let's compare BlackJAX nested sampling with traditional MCMC methods (NUTS and AIES) on the same problem.\n",
    "\n",
    "### Why This Comparison Matters:\n",
    "- **NUTS**: State-of-the-art Hamiltonian Monte Carlo (requires gradients)\n",
    "- **AIES**: Affine Invariant Ensemble Sampler (like `emcee`)\n",
    "- **Nested Sampling**: Designed for multimodal posteriors and evidence computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuts_comparison"
   },
   "outputs": [],
   "source": [
    "# NUTS (No-U-Turn Sampler) comparison\n",
    "from blackjax.mcmc.nuts import build_kernel\n",
    "from blackjax.mcmc.nuts import init as nuts_init\n",
    "\n",
    "# Convert to constrained space for NUTS (requires differentiable transforms)\n",
    "def transform_to_unconstrained(params):\n",
    "    \"\"\"Transform to unconstrained space for NUTS.\"\"\"\n",
    "    mu_x, mu_y, sigma_x, sigma_y, rho = params\n",
    "    \n",
    "    # Log transform for positive parameters\n",
    "    log_sigma_x = jnp.log(sigma_x)\n",
    "    log_sigma_y = jnp.log(sigma_y)\n",
    "    \n",
    "    # Logit transform for correlation [-1, 1] -> R\n",
    "    logit_rho = jnp.log((rho + 1) / (2 - rho))\n",
    "    \n",
    "    return jnp.array([mu_x, mu_y, log_sigma_x, log_sigma_y, logit_rho])\n",
    "\n",
    "def transform_to_constrained(unconstrained_params):\n",
    "    \"\"\"Transform back to constrained space.\"\"\"\n",
    "    mu_x, mu_y, log_sigma_x, log_sigma_y, logit_rho = unconstrained_params\n",
    "    \n",
    "    sigma_x = jnp.exp(log_sigma_x)\n",
    "    sigma_y = jnp.exp(log_sigma_y)\n",
    "    rho = 2 * jax.nn.sigmoid(logit_rho) - 1\n",
    "    \n",
    "    return jnp.array([mu_x, mu_y, sigma_x, sigma_y, rho])\n",
    "\n",
    "@jax.jit\n",
    "def unconstrained_logdensity(unconstrained_params):\n",
    "    \"\"\"Log-density in unconstrained space (for NUTS).\"\"\"\n",
    "    constrained_params = transform_to_constrained(unconstrained_params)\n",
    "    \n",
    "    # Check parameter bounds\n",
    "    mu_x, mu_y, sigma_x, sigma_y, rho = constrained_params\n",
    "    \n",
    "    # Prior constraints\n",
    "    if not (-2 <= mu_x <= 2 and -2 <= mu_y <= 2 and \n",
    "            0.5 <= sigma_x <= 3 and 0.5 <= sigma_y <= 3 and\n",
    "            -0.99 <= rho <= 0.99):\n",
    "        return -jnp.inf\n",
    "    \n",
    "    loglik = loglikelihood_fn(constrained_params)\n",
    "    \n",
    "    # Add Jacobian correction for transforms\n",
    "    log_sigma_x, log_sigma_y, logit_rho = unconstrained_params[2], unconstrained_params[3], unconstrained_params[4]\n",
    "    jacobian = log_sigma_x + log_sigma_y + logit_rho - 2 * jnp.log(jnp.cosh(logit_rho))\n",
    "    \n",
    "    return loglik + jacobian\n",
    "\n",
    "print(\"Setting up NUTS sampler...\")\n",
    "\n",
    "# NUTS configuration\n",
    "nuts_kernel = build_kernel(unconstrained_logdensity)\n",
    "\n",
    "# Initialize NUTS from a reasonable starting point\n",
    "initial_unconstrained = transform_to_unconstrained(true_params + 0.1 * jax.random.normal(rng_key, (5,)))\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nuts_state = nuts_init(initial_unconstrained, subkey)\n",
    "\n",
    "print(f\"NUTS initialized at: {transform_to_constrained(nuts_state.position)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_nuts"
   },
   "outputs": [],
   "source": [
    "# Run NUTS sampling\n",
    "print(\"Running NUTS sampling...\")\n",
    "\n",
    "num_nuts_samples = 2000\n",
    "num_warmup = 1000\n",
    "\n",
    "jit_nuts_step = jax.jit(nuts_kernel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Warmup phase\n",
    "nuts_samples = []\n",
    "current_state = nuts_state\n",
    "\n",
    "for i in range(num_warmup + num_nuts_samples):\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    current_state, info = jit_nuts_step(subkey, current_state)\n",
    "    \n",
    "    # Keep samples after warmup\n",
    "    if i >= num_warmup:\n",
    "        constrained_sample = transform_to_constrained(current_state.position)\n",
    "        nuts_samples.append(constrained_sample)\n",
    "    \n",
    "    if (i + 1) % 500 == 0:\n",
    "        status = \"warmup\" if i < num_warmup else \"sampling\"\n",
    "        print(f\"NUTS {status}: {i + 1}/{num_warmup + num_nuts_samples}\")\n",
    "\n",
    "nuts_time = time.time() - start_time\n",
    "nuts_samples = jnp.array(nuts_samples)\n",
    "\n",
    "print(f\"\\nNUTS completed in {nuts_time:.2f} seconds\")\n",
    "print(f\"Generated {len(nuts_samples)} samples\")\n",
    "\n",
    "# NUTS posterior statistics\n",
    "print(\"\\nNUTS posterior summary:\")\n",
    "for i, name in enumerate(param_names):\n",
    "    mean = jnp.mean(nuts_samples[:, i])\n",
    "    std = jnp.std(nuts_samples[:, i])\n",
    "    true_val = true_params[i]\n",
    "    print(f\"  {name}: {mean:.3f} ± {std:.3f} (true: {true_val:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aies_comparison"
   },
   "outputs": [],
   "source": [
    "# Affine Invariant Ensemble Sampler (AIES) comparison\n",
    "print(\"Setting up AIES (emcee-like) sampler...\")\n",
    "\n",
    "# Use BlackJAX's implementation of ensemble sampling\n",
    "try:\n",
    "    from blackjax.mcmc.aies import init, build_kernel as aies_build_kernel\n",
    "    \n",
    "    # AIES configuration\n",
    "    num_walkers = 50\n",
    "    num_aies_steps = 2000\n",
    "    \n",
    "    @jax.jit\n",
    "    def aies_logdensity(params):\n",
    "        \"\"\"Log-density for AIES (gradient-free).\"\"\"\n",
    "        return loglikelihood_fn(params)  # Assuming flat prior within bounds\n",
    "    \n",
    "    # Initialize ensemble\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    \n",
    "    # Initialize walkers around true parameters with some spread\n",
    "    walker_init = []\n",
    "    for i in range(num_walkers):\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        # Sample from prior bounds with some concentration around true values\n",
    "        bounds_array = jnp.array([[v[0], v[1]] for v in prior_bounds.values()])\n",
    "        walker = jax.random.uniform(subkey, (5,), \n",
    "                                   minval=bounds_array[:, 0], \n",
    "                                   maxval=bounds_array[:, 1])\n",
    "        walker_init.append(walker)\n",
    "    \n",
    "    initial_ensemble = jnp.array(walker_init)\n",
    "    \n",
    "    aies_kernel = aies_build_kernel(aies_logdensity)\n",
    "    aies_state = init(initial_ensemble)\n",
    "    \n",
    "    print(f\"AIES initialized with {num_walkers} walkers\")\n",
    "    \n",
    "    # Run AIES\n",
    "    print(\"Running AIES sampling...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    jit_aies_step = jax.jit(aies_kernel)\n",
    "    \n",
    "    aies_samples = []\n",
    "    current_aies_state = aies_state\n",
    "    \n",
    "    for i in range(num_aies_steps):\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        current_aies_state, info = jit_aies_step(subkey, current_aies_state)\n",
    "        \n",
    "        # Store all walker positions\n",
    "        aies_samples.append(current_aies_state.position.copy())\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"AIES step: {i + 1}/{num_aies_steps}\")\n",
    "    \n",
    "    aies_time = time.time() - start_time\n",
    "    aies_samples = jnp.array(aies_samples)\n",
    "    \n",
    "    # Flatten walker dimension\n",
    "    aies_samples_flat = aies_samples.reshape(-1, 5)\n",
    "    \n",
    "    print(f\"\\nAIES completed in {aies_time:.2f} seconds\")\n",
    "    print(f\"Generated {len(aies_samples_flat)} samples\")\n",
    "    \n",
    "    # AIES posterior statistics\n",
    "    print(\"\\nAIES posterior summary:\")\n",
    "    for i, name in enumerate(param_names):\n",
    "        mean = jnp.mean(aies_samples_flat[:, i])\n",
    "        std = jnp.std(aies_samples_flat[:, i])\n",
    "        true_val = true_params[i]\n",
    "        print(f\"  {name}: {mean:.3f} ± {std:.3f} (true: {true_val:.3f})\")\n",
    "    \n",
    "    aies_available = True\n",
    "    \nexcept ImportError:\n",
    "    print(\"AIES not available in this BlackJAX version\")\n",
    "    print(\"Skipping AIES comparison...\")\n",
    "    aies_available = False\n",
    "    aies_time = 0\n",
    "    aies_samples_flat = None"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "comparison_visualization"
   },
   "outputs": [],
   "source": "# Performance comparison visualization\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Timing comparison\nmethods = ['Nested Sampling', 'NUTS']\ntimes = [sampling_time, nuts_time]\ncolors = ['blue', 'orange']\n\nif aies_available:\n    methods.append('AIES')\n    times.append(aies_time)\n    colors.append('green')\n\naxes[0, 0].bar(methods, times, color=colors, alpha=0.7)\naxes[0, 0].set_ylabel('Sampling Time (seconds)')\naxes[0, 0].set_title('Computational Performance')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Sample comparison for first two parameters\nparam_idx = [0, 1]  # μₓ, μᵧ\n\n# Nested sampling\naxes[0, 1].scatter(samples[:, param_idx[0]], samples[:, param_idx[1]], \n                  alpha=0.3, s=10, label='Nested Sampling', c='blue')\naxes[0, 1].plot(true_params[param_idx[0]], true_params[param_idx[1]], \n               'r*', markersize=15, label='True')\naxes[0, 1].set_xlabel(param_names[param_idx[0]])\naxes[0, 1].set_ylabel(param_names[param_idx[1]])\naxes[0, 1].set_title('Nested Sampling Posterior')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# NUTS\naxes[0, 2].scatter(nuts_samples[:, param_idx[0]], nuts_samples[:, param_idx[1]], \n                  alpha=0.3, s=10, label='NUTS', c='orange')\naxes[0, 2].plot(true_params[param_idx[0]], true_params[param_idx[1]], \n               'r*', markersize=15, label='True')\naxes[0, 2].set_xlabel(param_names[param_idx[0]])\naxes[0, 2].set_ylabel(param_names[param_idx[1]])\naxes[0, 2].set_title('NUTS Posterior')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# Parameter accuracy comparison\nns_errors = jnp.abs(jnp.array([nested_samples[name].mean() for name in param_names]) - true_params)\nnuts_errors = jnp.abs(jnp.mean(nuts_samples, axis=0) - true_params)\n\nx_pos = jnp.arange(len(param_names))\nwidth = 0.35\n\naxes[1, 0].bar(x_pos - width/2, ns_errors, width, label='Nested Sampling', color='blue', alpha=0.7)\naxes[1, 0].bar(x_pos + width/2, nuts_errors, width, label='NUTS', color='orange', alpha=0.7)\n\nif aies_available:\n    aies_errors = jnp.abs(jnp.mean(aies_samples_flat, axis=0) - true_params)\n    axes[1, 0].bar(x_pos + 1.5*width, aies_errors, width, label='AIES', color='green', alpha=0.7)\n\naxes[1, 0].set_xlabel('Parameters')\naxes[1, 0].set_ylabel('Absolute Error')\naxes[1, 0].set_title('Parameter Estimation Accuracy')\naxes[1, 0].set_xticks(x_pos)\naxes[1, 0].set_xticklabels(param_names)\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Marginal distributions comparison\nparam_to_plot = 4  # ρ parameter (most constrained)\n\naxes[1, 1].hist(samples[:, param_to_plot], bins=30, alpha=0.7, \n               density=True, label='Nested Sampling', color='blue')\naxes[1, 1].hist(nuts_samples[:, param_to_plot], bins=30, alpha=0.7, \n               density=True, label='NUTS', color='orange')\n\nif aies_available:\n    axes[1, 1].hist(aies_samples_flat[:, param_to_plot], bins=30, alpha=0.7, \n                   density=True, label='AIES', color='green')\n\naxes[1, 1].axvline(true_params[param_to_plot], color='red', linestyle='--', \n                  linewidth=2, label='True')\naxes[1, 1].set_xlabel(f'{param_names[param_to_plot]}')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].set_title(f'Marginal: {param_names[param_to_plot]}')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Summary statistics\nsummary_text = f\"\"\"Method Comparison Summary:\n\nNested Sampling:\n• Time: {sampling_time:.1f}s\n• Samples: {len(samples)}\n• Evidence: logZ = {evidence_mean:.2f} ± {evidence_std:.2f}\n• Handles multimodality\n\nNUTS:\n• Time: {nuts_time:.1f}s  \n• Samples: {len(nuts_samples)}\n• Requires gradients\n• May miss modes\"\"\"\n\nif aies_available:\n    summary_text += f\"\"\"\n\nAIES:\n• Time: {aies_time:.1f}s\n• Samples: {len(aies_samples_flat)}\n• Gradient-free\n• Ensemble method\"\"\"\n\naxes[1, 2].text(0.05, 0.95, summary_text, transform=axes[1, 2].transAxes, \n               fontsize=10, verticalalignment='top', fontfamily='monospace')\naxes[1, 2].set_xlim(0, 1)\naxes[1, 2].set_ylim(0, 1)\naxes[1, 2].axis('off')\naxes[1, 2].set_title('Method Comparison')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_acceleration"
   },
   "source": [
    "## 7. GPU Acceleration and Performance\n",
    "\n",
    "BlackJAX's GPU-native implementation provides significant speedups, especially for:\n",
    "- **Large parameter spaces** (high-dimensional problems)\n",
    "- **Complex likelihood evaluations** (neural networks, PDEs)\n",
    "- **Parallel deletion** in nested sampling\n",
    "\n",
    "Let's explore the performance characteristics and demonstrate GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_scaling"
   },
   "outputs": [],
   "source": [
    "# Performance scaling experiment\n",
    "print(\"Testing performance scaling with different configurations...\")\n",
    "\n",
    "# Test different numbers of live points\n",
    "live_point_configs = [100, 500, 1000, 2000]\n",
    "performance_results = []\n",
    "\n",
    "for num_live_test in live_point_configs:\n",
    "    print(f\"\\nTesting with {num_live_test} live points...\")\n",
    "    \n",
    "    # Initialize nested sampling\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    particles_test, logprior_fn_test = ns_utils.uniform_prior(\n",
    "        subkey, num_live_test, prior_bounds\n",
    "    )\n",
    "    \n",
    "    nested_sampler_test = blackjax.nss(\n",
    "        logprior_fn=logprior_fn_test,\n",
    "        loglikelihood_fn=loglikelihood_fn,\n",
    "        num_delete=min(50, num_live_test // 10),  # Scale deletion parameter\n",
    "        num_inner_steps=25\n",
    "    )\n",
    "    \n",
    "    # Initialize and run a few steps for timing\n",
    "    live_state_test = nested_sampler_test.init(particles_test)\n",
    "    jit_step_test = jax.jit(nested_sampler_test.step)\n",
    "    \n",
    "    # Warm up JIT compilation\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    _, _ = jit_step_test(subkey, live_state_test)\n",
    "    \n",
    "    # Time multiple steps\n",
    "    num_test_steps = 10\n",
    "    start_time = time.time()\n",
    "    \n",
    "    current_state = live_state_test\n",
    "    for _ in range(num_test_steps):\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        current_state, _ = jit_step_test(subkey, current_state)\n",
    "    \n",
    "    step_time = (time.time() - start_time) / num_test_steps\n",
    "    \n",
    "    performance_results.append({\n",
    "        'num_live': num_live_test,\n",
    "        'step_time': step_time,\n",
    "        'throughput': num_live_test / step_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  Step time: {step_time:.4f}s\")\n",
    "    print(f\"  Throughput: {num_live_test / step_time:.0f} points/second\")\n",
    "\n",
    "# Plot performance scaling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "num_live_points = [r['num_live'] for r in performance_results]\n",
    "step_times = [r['step_time'] for r in performance_results]\n",
    "throughputs = [r['throughput'] for r in performance_results]\n",
    "\n",
    "# Step time scaling\n",
    "ax1.plot(num_live_points, step_times, 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Live Points')\n",
    "ax1.set_ylabel('Step Time (seconds)')\n",
    "ax1.set_title('Nested Sampling Step Time Scaling')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Throughput scaling\n",
    "ax2.plot(num_live_points, throughputs, 's-', color='orange', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Live Points')\n",
    "ax2.set_ylabel('Throughput (points/second)')\n",
    "ax2.set_title('Sampling Throughput')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPerformance Summary:\")\n",
    "print(f\"  Best throughput: {max(throughputs):.0f} points/second\")\n",
    "print(f\"  Scaling: {'Sub-linear' if throughputs[-1] > throughputs[0] else 'Super-linear'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced_features"
   },
   "source": [
    "## 8. Advanced Features and Extensions\n",
    "\n",
    "BlackJAX nested sampling offers several advanced features for scientific applications:\n",
    "\n",
    "### Key Advantages:\n",
    "1. **Evidence computation**: Essential for model comparison in SBI\n",
    "2. **Multimodal handling**: Robust exploration of complex posteriors\n",
    "3. **GPU acceleration**: Leverages modern HPC infrastructure\n",
    "4. **JAX integration**: Seamless autodiff and JIT compilation\n",
    "5. **Open source**: Community-driven development\n",
    "\n",
    "### When to Use Nested Sampling:\n",
    "- **Model comparison** (Bayesian evidence needed)\n",
    "- **Multimodal posteriors** (phase transitions, symmetries)\n",
    "- **High-dimensional problems** (gradient-free exploration)\n",
    "- **SBI workflows** (NLE, NRE, NJE all need sampling)\n",
    "- **Scientific inference** (evidence quantification important)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "model_comparison_demo"
   },
   "outputs": [],
   "source": "# Model comparison demonstration\nprint(\"Demonstrating Bayesian model comparison with nested sampling...\")\n\n# Define two competing models\n# Model 1: Full 2D Gaussian (5 parameters)\n# Model 2: Circular Gaussian (3 parameters: μₓ, μᵧ, σ)\n\n@jax.jit\ndef circular_gaussian_loglikelihood(params_3d_dict):\n    \"\"\"Likelihood for circular Gaussian model (σₓ = σᵧ, ρ = 0).\"\"\"\n    mu_x = params_3d_dict[\"mu_x\"]\n    mu_y = params_3d_dict[\"mu_y\"] \n    sigma = params_3d_dict[\"sigma\"]\n    \n    # Convert to 5D parameter dict\n    params_5d_dict = {\n        \"mu_x\": mu_x,\n        \"mu_y\": mu_y,\n        \"sigma_x\": sigma,\n        \"sigma_y\": sigma,\n        \"rho\": 0.0\n    }\n    \n    return loglikelihood_fn(params_5d_dict)\n\n# Model 2 prior bounds (3D)\nprior_bounds_3d = {\n    \"mu_x\": (-2.0, 2.0),\n    \"mu_y\": (-2.0, 2.0),\n    \"sigma\": (0.5, 3.0)\n}\n\n# Run nested sampling for Model 2 (circular Gaussian)\nprint(\"\\nRunning nested sampling for Model 2 (circular Gaussian)...\")\n\nnum_live_simple = 100\nrng_key, subkey = jax.random.split(rng_key)\nparticles_3d, logprior_fn_3d = ns_utils.uniform_prior(\n    subkey, num_live_simple, prior_bounds_3d\n)\n\nnested_sampler_3d = blackjax.nss(\n    logprior_fn=logprior_fn_3d,\n    loglikelihood_fn=circular_gaussian_loglikelihood,\n    num_delete=50,\n    num_inner_steps=15\n)\n\nlive_state_3d = nested_sampler_3d.init(particles_3d)\njit_step_3d = jax.jit(nested_sampler_3d.step)\n\n# Run until convergence\ndead_points_3d = []\niteration_3d = 0\n\nwhile (live_state_3d.logZ_live - live_state_3d.logZ) > -3.0:\n    rng_key, subkey = jax.random.split(rng_key)\n    live_state_3d, dead_info_3d = jit_step_3d(subkey, live_state_3d)\n    dead_points_3d.append(dead_info_3d)\n    iteration_3d += 1\n    \n    if iteration_3d % 25 == 0:\n        remaining = live_state_3d.logZ_live - live_state_3d.logZ\n        print(f\"  Iteration {iteration_3d}: logZ = {live_state_3d.logZ:.3f}, remaining = {remaining:.3f}\")\n\nprint(f\"\\nModel Comparison Results:\")\nprint(f\"Model 1 (Full 2D Gaussian):     logZ = {evidence_mean:.3f}\")\nprint(f\"Model 2 (Circular Gaussian):    logZ = {live_state_3d.logZ:.3f}\")\n\n# Calculate Bayes factor\nlog_bayes_factor = evidence_mean - live_state_3d.logZ\nbayes_factor = jnp.exp(log_bayes_factor)\n\nprint(f\"\\nBayes Factor (Model 1 / Model 2): {bayes_factor:.2f}\")\nprint(f\"Log Bayes Factor: {log_bayes_factor:.3f}\")\n\nif log_bayes_factor > 1:\n    print(\"Evidence favors Model 1 (Full 2D Gaussian)\")\nelif log_bayes_factor < -1:\n    print(\"Evidence favors Model 2 (Circular Gaussian)\")\nelse:\n    print(\"Evidence is inconclusive\")\n\n# Visualize model comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Evidence comparison\nmodels = ['Full 2D\\nGaussian', 'Circular\\nGaussian']\nevidences = [evidence_mean, live_state_3d.logZ]\n\nax1.bar(models, evidences, alpha=0.7, color=['blue', 'orange'])\nax1.set_ylabel('Log Evidence')\nax1.set_title('Model Comparison: Bayesian Evidence')\nax1.grid(True, alpha=0.3)\n\n# Parameter comparison for overlapping parameters\ndead_3d = ns_utils.finalise(live_state_3d, dead_points_3d)\nsamples_3d_dict = dead_3d.particles\nsamples_3d = jnp.stack([samples_3d_dict[\"mu_x\"], samples_3d_dict[\"mu_y\"]], axis=1)\n\n# Compare μₓ and μᵧ estimates\nax2.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10, label='Model 1 (5D)', c='blue')\nax2.scatter(samples_3d[:, 0], samples_3d[:, 1], alpha=0.3, s=10, label='Model 2 (3D)', c='orange')\nax2.plot(true_params[0], true_params[1], 'r*', markersize=15, label='True')\nax2.set_xlabel('μₓ')\nax2.set_ylabel('μᵧ')\nax2.set_title('Position Parameter Comparison')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "your_turn"
   },
   "source": [
    "## 9. Your Turn: Apply BlackJAX to Your Problems!\n",
    "\n",
    "Now it's time to experiment with BlackJAX nested sampling on your own research problems. Here are some suggestions:\n",
    "\n",
    "### Exercise Options:\n",
    "\n",
    "1. **Modify the current problem**:\n",
    "   - Change the true parameters and see how the inference performs\n",
    "   - Add more noise to make the problem harder\n",
    "   - Try different prior bounds\n",
    "\n",
    "2. **Use your own JAX likelihood**:\n",
    "   - Bring code from Viraj's workshop\n",
    "   - Implement your own scientific model\n",
    "   - Compare nested sampling vs. your usual inference method\n",
    "\n",
    "3. **Explore different configurations**:\n",
    "   - Increase the number of live points for higher precision\n",
    "   - Try different `num_inner_steps` values\n",
    "   - Experiment with the `num_delete` parameter for GPU optimization\n",
    "\n",
    "### Template for Your Own Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "your_experiment"
   },
   "outputs": [],
   "source": [
    "# Template for your own BlackJAX nested sampling experiment\n",
    "\n",
    "# Step 1: Define your likelihood function\n",
    "@jax.jit\n",
    "def your_loglikelihood_fn(params):\n",
    "    \"\"\"\n",
    "    Replace this with your own likelihood function.\n",
    "    \n",
    "    Args:\n",
    "        params: JAX array of parameters\n",
    "    \n",
    "    Returns:\n",
    "        Log-likelihood value\n",
    "    \"\"\"\n",
    "    # Example: simple 2D Gaussian\n",
    "    mu_x, mu_y = params[:2]\n",
    "    \n",
    "    # Your model/simulation here\n",
    "    model_prediction = your_model(params)\n",
    "    \n",
    "    # Your likelihood calculation here\n",
    "    loglik = your_likelihood_calculation(model_prediction, your_data)\n",
    "    \n",
    "    return loglik\n",
    "\n",
    "# Step 2: Define your prior bounds\n",
    "your_prior_bounds = {\n",
    "    \"param1\": (lower_bound, upper_bound),\n",
    "    \"param2\": (lower_bound, upper_bound),\n",
    "    # Add more parameters as needed\n",
    "}\n",
    "\n",
    "# Step 3: Configure and run nested sampling\n",
    "your_num_live = 500  # Adjust based on your problem complexity\n",
    "your_num_dims = len(your_prior_bounds)\n",
    "your_num_inner_steps = your_num_dims * 5\n",
    "\n",
    "# Initialize\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "your_particles, your_logprior_fn = ns_utils.uniform_prior(\n",
    "    subkey, your_num_live, your_prior_bounds\n",
    ")\n",
    "\n",
    "# Create sampler\n",
    "your_sampler = blackjax.nss(\n",
    "    logprior_fn=your_logprior_fn,\n",
    "    loglikelihood_fn=your_loglikelihood_fn,\n",
    "    num_delete=25,\n",
    "    num_inner_steps=your_num_inner_steps\n",
    ")\n",
    "\n",
    "# Run sampling (add your implementation here)\n",
    "print(\"Implement your nested sampling run here!\")\n",
    "print(\"Follow the pattern from the examples above.\")\n",
    "\n",
    "# Step 4: Analyze results with Anesthetic\n",
    "# your_nested_samples = NestedSamples(...)\n",
    "# Make corner plots, compute evidence, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resources_next_steps"
   },
   "source": [
    "## 10. Resources and Next Steps\n",
    "\n",
    "### Key Resources:\n",
    "\n",
    "**BlackJAX Nested Sampling:**\n",
    "- Repository: https://github.com/handley-lab/blackjax\n",
    "- Documentation: [BlackJAX docs](https://blackjax-devs.github.io/blackjax/)\n",
    "- Installation: `pip install git+https://github.com/handley-lab/blackjax@nested_sampling`\n",
    "\n",
    "**Visualization:**\n",
    "- Anesthetic: https://anesthetic.readthedocs.io/en/latest/plotting.html\n",
    "- Corner plots for nested sampling results\n",
    "- Evidence evolution diagnostics\n",
    "\n",
    "**JAX Ecosystem:**\n",
    "- JAX documentation: https://jax.readthedocs.io/\n",
    "- NumPyro (probabilistic programming): https://num.pyro.ai/\n",
    "- Optax (optimization): https://optax.readthedocs.io/\n",
    "\n",
    "### When to Use BlackJAX Nested Sampling:\n",
    "\n",
    "✅ **Good for:**\n",
    "- Multimodal posteriors\n",
    "- Model comparison (evidence computation)\n",
    "- High-dimensional problems\n",
    "- GPU-accelerated inference\n",
    "- SBI workflows (NLE, NRE, NJE)\n",
    "- Scientific applications requiring evidence quantification\n",
    "\n",
    "❌ **Consider alternatives for:**\n",
    "- Simple unimodal posteriors (HMC/NUTS may be faster)\n",
    "- Very low-dimensional problems (< 3D)\n",
    "- When you only need posterior samples (not evidence)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try on your research problems**: Apply to your own JAX-based models\n",
    "2. **Experiment with configurations**: Optimize for your specific use case\n",
    "3. **Compare methods**: Benchmark against your current inference approach\n",
    "4. **Contribute**: BlackJAX is community-driven - report issues, suggest features\n",
    "5. **Stay updated**: Follow BlackJAX development for new features\n",
    "\n",
    "### Questions?\n",
    "\n",
    "- GitHub Issues: https://github.com/handley-lab/blackjax/issues\n",
    "- Discussion: BlackJAX community channels\n",
    "- This workshop: Experiment with the provided templates!\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for participating in the BlackJAX Nested Sampling Workshop!**\n",
    "\n",
    "*The future of scientific inference is GPU-native, open-source, and community-driven.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}